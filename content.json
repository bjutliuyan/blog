{"pages":[{"title":"tags","text":"","link":"/blog/tags/index.html"}],"posts":[{"title":"C+中STL用法详解一_内容介绍","text":"STL即标准模板库，其内封装了诸多的基本数据结构和算法。 1. 什么是STL STL是C++标准程序库的核心，其中包含了很多常用的基本数据结构和算法，提供了一个可扩展的应用框架； STL的一个重要特点是数据结构和算法的分离这使得STL变得很通用，比如sort()方法是完全通用的，可以用它来操作任何数据集合，包括链表、容器、数组 STL是基于template模板实现的，而不是面向对象 2. STL内容介绍STL中主要有六大组件：1. 容器(Container)用来存储并管理某类对象的集合，以模板类的方法提供。每一种容器都有其优缺点，为了访问容器中的数据，可以使用由容器类输出的迭代器。2. 迭代器(Iterator)提供访问容器中对象的方法。例如可以使用一对迭代器指定list或vector中一定范围的对象。迭代器可划分为 5 种类属，这 5 种类属归属两种类型：双向迭代器和随机存取迭代器3. 算法(Algorithm)用来操作容器中数据结构的模板函数。例如STL用sort()来对容器中数据排序，用find()在容器中查找元素。函数本身与它们操作的数据结构和类型无关，因此可以在简单数组和任何高复杂容器中使用4. 仿函数(Functor)5. 适配器(Adaptor)6. 分配器(Allocator) 2.1 容器STL中容器有序列式容器和关联容器，容器适配器(stack、queue、priority_queue)，位集(bit_set)，串包(string_package)等。（1）序列式容器每个元素都有固定位置——取决于插入时机和下标，和元素值无关（vector、deque、list）（2）关联式容器，元素位置取决于特定的排序准则，和插入顺序无关（set/multiset、map/mutlimap）目前STL中已经提供的主要容器如下： vector &lt;T&gt;：一种向量 list &lt;T&gt;：一个双向链表容器，完成了标准C++数据结构中链表的所有功能 deque &lt;T&gt;：双端队列容器 set &lt;T&gt;：一种集合容器，元素不能重复 multiset &lt;T&gt;：一种允许出现重复元素的集合容器 map &lt;key,value&gt;：关键值对的容器，key唯一 multimap &lt;key,value&gt;：一种允许出现重复key值的关联式容器 stack &lt;T&gt;：一种栈容器 queue &lt;T&gt; ：一种队列容器 prority_queue &lt;T&gt;：优先队列容器 2.2 STL迭代器 作用: Iterator(迭代器)模式又称Cursor模式，可以使得我们在不知道对象内部表示情况下，按一定顺序访问聚合对象中的元素； 能让容器与算法不干扰的相互发展，最后又能无间隙的粘合起来，容器提供迭代器，算法使用迭代器访问容器中的数据； 重载了*、++、==、!=、=运算符。 2.3 算法STL提供了大约100个实现算法的模板函数，只需要调用一下就可以很大程度上简化代码。算法部分主要由头文件&lt;algorithm&gt;,&lt;numeric&gt;,&lt;fuctional&gt;组成，STL中的算法大致分为四类： 非可变序列算法：不直接修改所操作容器内容的算法 可变序列算法：可以修改所操作容器内容的算法 排序算法：对序列进行排序、合并、搜索、集合操作等 数值算法：对容器内容进行数值计算常见的部分算法如下： for_each()：用指定函数依次对指定范围内所有元素进行迭代访问，返回所指定的函数类型。 find()： 利用底层元素的等于操作符，对指定范围内的元素与输入值进行比较。当匹配时，结束搜索，返回该元素的一个InputIterator。 find_if()： 使用输入的函数代替等于操作符执行find。 count()：利用等于操作符，把标志范围内的元素与输入值比较，返回相等元素个数。 count_if()： 利用输入的操作符，对标志范围内的元素进行操作，返回结果为true的个数。 replace()： 将指定范围内所有等于vold的元素都用vnew代替。 replace_if()：将指定范围内所有操作结果为true的元素用新值代替。 sort()：以升序重新排列指定范围内的元素。重载版本使用自定义的比较操作。 merge()：合并两个有序序列，存放到另一个序列。重载版本使用自定义的比较。 reverse()：将指定范围内元素重新反序排序。 unique()：清除序列中重复元素。 remove()：删除指定范围内所有等于指定元素的元素。 equal()：如果两个序列在标志范围内元素都相等，返回true。 set_union()：返回两个序列的并集。 set_intersection()：返回两个序列的交集。 set_difference()：返回两个序列的差集。 set_symmetric_difference()：返回两个序列的对称差集。 2.4 仿函数C++中通过一个类或结构体中重载括号运算符的方法使用一个函数对象而不是一个普通函数，要使使用仿函数比一般函数优点参考以下链接link 1234567struct cmp{ bool operator()(ListNode* node1, ListNode* node2){ return node1-&gt;val &gt; node2-&gt;val; }};sort(a, a+n, cmp); //按照cmp的规则迭代排序 用STL内建的仿函数，必须包含头文件。该头文件包含的仿函数分类包括： 算术类仿函数加： plus 减： minus 乘： multiplies 除： dividex 模取： modulus 否定： negate 关系运算类仿函数等于： equal_to 不等于： not_equal_to 大于： greater 大于等于：greater_equal 小于： less 小于等于：less_equal 逻辑运算仿函数逻辑与： logical_and 逻辑或： logical_or 逻辑否： logical_no 除了使用STL内建的仿函数，还可以使用自定义的仿函数。 2.5 容器适配器容器适配器(配接器)对容器进行包装使其表现出另外一种行为。例如stack","link":"/blog/2019/10/12/C++/C++中STL用法详解一-内容介绍/"},{"title":"(一)TensorFlow相关介绍及安装步骤详解","text":"TensorFlow即是实现机器学习算法的接口，也是执行机器学习算法的框架 TensorFlow介绍 TensorFlow基础架构TensorFlow是谷歌2015年发布的第二代分布式机器学习系统。前端支持Python、C++、Go、Java等多种开发语言，后端使用C++、CUDA等写成，可以方便的部署到各种平台。 构件图、执行图TensorFlow使用数据流式图来规划计算流程，图中的节点被称之为 _op_ (operation 的缩写)。 一个 op 获得 0 个或多个 Tensor, 执行计算，产生 0 个或多个 Tensor。一个 TensorFlow 图描述了计算的过程，不过图是静态的，为了进行计算，图必须在 会话 里被启动。 会话 将图的 op 分发到诸如 CPU 或 GPU 之类的 设备 上， 同时提供执行 op 的方法。执行后再将产生的 tensor 返回。在 Python 语言中, 返回的 tensor 是 numpy ndarray 对象; 在 C 和 C++ 语言中, 返回的 tensor 是tensorflow::Tensor 实例. 在一个会话中启动图 构建图完成后，执行图过程需要通过session会话完成： 1234567891011import tensorflow as tf#构建图matrix1 = tf.constant([[3., 3.]])matrix2 = tf.constant([[2.],[2.]])product = tf.matmul(matrix1, matrix2)#执行图#函数调用 &apos;run(product)&apos; 触发了图中三个 op (两个常量 op 和一个矩阵乘法 op) 的执行，返回resultsess = tf.Session()result = sess.run(product)print (result)sess.close() session使用完后需要关闭释放资源，除了显示调用close()，也可以用with自动完成： 123with tf.session() as sess: result = sess.run(product) print(result) 分配设备：一般不需要显示指定使用GPU还是CPU，TensorFlow能自动检测；如果检测到GPU会尽可能地利用找到的第一个GPU来执行操作。如果机器上有多个可用GPU，除了第一个其余默认是不参与计算的，需要明确指派。 123with tf.Session() as sess: with tf.device(&quot;/gpu:1&quot;): ... 交互式使用为了便于使用类与IPython的Python交互环境，可用InteractiveSession代替Session类，使用Tensor.eval()和Operation.run()代替Session.run()： 123456789import tensorflow as tfsess = tf.InteractiveSession()x = tf.Variable([1.0, 2.0])a = tf.constant([3.0, 3.0])# 使用初始化器 initializer op 的 run() 方法初始化 &apos;x&apos; x.initializer.run()# 增加一个减法 sub op, 从 &apos;x&apos; 减去 &apos;a&apos;. 运行减法 op, 输出结果 sub = tf.sub(x, a)print sub.eval() Fetch、Feed Fetch需要在op的一次运行中获取多个tensor值： 1result = sess.run([mul, intermed]) FeedFeed机制相当于提供数据作为run()调用的参数，只在调用它的方法内有效，方法结束feed就会消失。 12345input1 = tf.placeholder(tf.types.float32)input2 = tf.placeholder(tf.types.float32)output = tf.mul(input1, input2)with tf.Session() as sess: print sess.run([output], feed_dict={input1:[7.], input2:[2.]}) Liunx下TensorFlow_GPU版本安装步骤TensorFlow并不是全部由Python写成的库，底层有很多C++乃至CUDA的代码。 先在本地安装Python3.5 安装AnacondaAnaconda是Python的一个科学计算发行版，内置了数百个Python经常会使用的库，单独安装这些库时很容易出现兼容性问题，建议安装Anaconda。 到Anaconda官网下载Anaconda3 4.2.0版 到Anaconda下载目录执行bash Anaconda3-4.2.0-Linux-x86_64.sh 接下来会看到安装提示，直接按回车确认进入下一步 安装完成后，程序提示是否把anaconda3的binary路径加入.bashrc，建议添加（这样以后python和ipython命令会自动使用Anaconda Python3.5的环境） 安装正确的CUDA版本 到CUDA官网下载CUDA安装包，一般里面集成了显卡驱动 安装前需要暂停当前NVIDIA驱动的X server，如果是远程连接的Linux机器，可以运行该命令：sudo init 3 将CUDA安装包权限设置成可执行的，并执行安装程序： 12chmod u+x cuda_8.0.44_linux.runsudo ./cuda_8.0.44_linux.run 接下来是CUDA安装的一些确认，除了确认是否安装CUDA 8.0 Samples选择n之外，其余的选择y；等待CUDA安装完成 到系统环境设置CUDA路径1234567vim ~/.bashrc#cuda的绝对路径export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:/usr/local/cuda-8.0/extras/CUPTI/lib64:$LD_LIBRARY_PATHexport CUDA_HOME=/usr/local/cuda-8.0export PATH=/usr/local/cuda-8.0/bin:$PATHsource ~/.bashrc 安装正确的CUDNN版本cuDNN是NVIDIA推出的深度学习中CNN和RNN高度优化的实现，目前绝大多数的深度学习框架都使用cuDNN来驱动GPU计算。 从CUDNN官网下载，并解压 安装TensorFlow 下载安装 1pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.0rc0-cp35-cp35m-linux_x86_64.whl 选择是否确认信息到是否支持CUDA这一步，选择支持 选择要支持使用的CUDA、CUDNN版本及安装路径CUDA选择8.0版本，路径选择/usr/local/cuda-8.0；CUDNN选择5.1版本，路径也设置为/usr/local/cuda-8.012345Please specify the location where CUDA 8.0 toolkit is installed. Refer toREADME.md for more details. [default is: /usr/local/cuda]: /usr/local/cuda-8.0Please specify the location where CUDNN 5.1 V2 library is installed. Refer toREADME.md for more details. [default is: /usr/local/cuda]: /usr/local/cuda-8.0","link":"/blog/2019/09/16/TensorFlow/一-TensorFlow相关介绍及安装步骤详解/"},{"title":"(三)TensorBoard可视化及模型数据的保存加载","text":"模型的可视化与保存在深度学习训练中起到很重要的作用。 一、模型的保存与恢复在通过TensorFlow进行深度学习训练时，会产生大量的参数，那么下一次进行训练的时候又得重新进行训练。所以可以将已经训练好的模型保存到本地。等需要的时候直接将模型读入到内存中来。 模型的保存 1234567891011121314import tensorflow as tfv1 = tf.Variable(1, name=\"v1\")v2 = tf.Variable(2, name=\"v2\")#定义初始化所有变量opinit_op = tf.initialize_all_variables()#增加保存模型opsaver = tf.train.Saver()#开启一个会话with tf.Session() as sess: sess.run(init_op) save_path = saver.save(sess=sess, save_path=\"/home/liuyan/Desktop/OpenCV/checkpoints/model.ckpt\") 运行之后就会生成保存的模型： 模型的恢复模型的恢复需要两步： 恢复模型的结构（如果自己又重新写了一份静态图，则不需要该步） 恢复模型的参数123456789101112import tensorflow as tf#恢复模型结构(省略的话如下所示需要自己再重新构建图)#saver = tf.train.import_meta_graph('/home/liuyan/Desktop/OpenCV/checkpoints/model.ckpt.meta')v1 = tf.Variable(0, name=\"v1\")v2 = tf.Variable(0, name=\"v2\")saver = tf.train.Saver()#开启会话 with tf.Session() as sess: saver.restore(sess, '/home/liuyan/Desktop/OpenCV/checkpoints/model.ckpt') print(sess.run(v1)) 选择存储和恢复哪些变量如果不给tf.train.Saver()传入参数，那么会保存所有的变量，其中每一个变量都以被创建时的名称被保存。 12#Add ops to save and restore only 'v2' using the name \"my_v2\"saver = tf.train.Saver({\"my_v2\": v2}) 二、TensorBoard可视化学习为了方便对TensorFlow程序的理解、调试与优化，可以使用TensorBoard来展现TensorFlow的静态图、绘制图像生成的定量指标图以及附加数据。相关操作十分简单，具体步骤如下： 将当前的图写入硬盘 123456789101112import tensorflow as tfv1 = tf.placeholder(dtype=tf.int32)v2 = tf.placeholder(dtype=tf.int32)v3 = tf.add(v1, v2, name=\"v3\")#开启一个会话with tf.Session() as sess: print(sess.run(v3, feed_dict={v1: 1, v2: 2})) #定义一个生成tensorboard图操作，第一个参数是图的存储路径 graph_writer_op = tf.summary.FileWriter('.', sess.graph) sess.run(graph_writer_op) 启动TensorBoard显示输入下面的指令来启动TensorBoard： 1python tensorflow/tensorboard/tensorboard.py --logdir=path/to/log-directory 这里的参数 logdir 指向 SummaryWriter 序列化数据的存储路径。如果logdir目录的子目录中包含另一次运行时的数据，那么 TensorBoard 会展示所有运行的数据。TensorBoard 开始运行后，可以通过在浏览器中输入 localhost:6006 来查看 TensorBoard。 如果已经通过pip安装了TensorBoard，也可以通过执行以下命令来访问： 1tensorboard --logdir=/path/to/log-directory TensorBoard图表可视化TensorBoard的图表计算强大而又复杂，在可视化和理解网络结构时非常有帮助，，如下所示：典型的网络结构都有数以千计的节点，那么多的节点难以一下子全部看到，简单起见我们可以为变量名划定范围。下面这个例子使用tf.name_scope在hidden命名域下定义了三个操作： 123456import tensorflow as tfwith tf.name_scope('hidden') as scope: a = tf.constant(5, name='alpha') W = tf.Variable(tf.random_uniform([1, 2], -1.0, 1.0), name='weights') b = tf.Variable(tf.zeros([1]), name='biases') 通过静态图获取具体tensorTensorFlow是通过定义静态图来运行的，我们也可以在程序最后获得这张图： 1g = tf.get_default_graph 然后可以进一步从图中获取变量： 12var_a = g.get_tensor_by_name(“v1”)print(sess.run(var_a))","link":"/blog/2019/09/17/TensorFlow/三-TensorBoard可视化及模型数据的保存与加载/"},{"title":"C+中STL用法详解二_常用容器用法介绍","text":"熟悉容器的使用是掌握STL的基本步骤。 1. 序列式容器序列容器以线性序列的方式存储元素。元素在序列中的顺序与存储的顺序相同，没有对元素进行排序。 vector的基本用法长度可变序列，但只能在序列的末尾高效地增加或删除元素。 12345678910111213141516vector&lt;类型&gt;T; //构建T.push_back(x); //尾部新增一个元素xT.insert(iterator it, int x); //在it指向的元素前新增一个xT.insert(iterator it, int n, int x); //在it指向元素前插入n个xT.erase(iterator it); //删除it指向的元素T.erase(iterator first, iterator last); //删除[last,first)中元素T.pop_back(); //删除最后一个元素T.empty(); //判断是否为空T.size(); //返回元素个数T.at(index); //返回index编号元素reverse(T); //反转T中元素T.begin(); //返回首元素指针T.end(); //返回最后一个元素+1的指针T.front(); //返回首元素引用T.back(); //返回尾元素引用sort(T.begin(), T.end()); //排序 deque的基本用法以双端队列形式组织元素，在尾部和头部插入删除迅速，在中间插入删除元素则比较费时，因为必须移动中间其它元素。 12345678910111213deque&lt;类型&gt;T； //声明一个双端队列deque&lt;类型&gt;T(size,value); //声明具有size个value默认值的双端队列T.push_front(x); //在头部插入xT.push_back(x); //在尾部插入xT.pop_front(); //弹出头部的一个元素T.pop_back(); //弹出尾部的一个元素T.front(); //返回第一个元素的引用T.back(); //返回最后一个元素的引用T.begin();T.end();T.size();T.at(index);T.insert(it, x); list的基本用法list实现的是双向链表，与vector比它允许快速的插入和删除，但随机访问比较慢。 12345678910111213list&lt;类型&gt;T; // 构建T.push_back(x);T.push_front(x);T.pop_back();T.pop_front();T.front();T.back();T.begin();T.reverse();T.sort();T.end();T.insert(x);T.remove(x); 2. 关联式容器 map的基本用法map是关联容器的一种，通过键值对来存储。map内部自建一颗红黑树(平衡二叉树)，这棵树具有对数据的自动排序功能（按照键值排序，当键值是不可比较元素时需指定比较仿函数）。map容器有4中，每一种都由类模板定义，因为修改键会扰乱容器中元素的顺序，每种map容器模板都有不同的特性： map：唯一&lt;key,value&gt;对应，默认按键值从小到大排序。 mutlimap：允许使用重复的键。 unordered_map： unordered_multimap： 123456789map&lt; int,string &gt; T; //构建T.insert(make_pair&lt;int,string&gt;(1, \"student_one\")); //插入T.empty();T.size();T.erase(key); //根据key删除，返回删除元素的数量T.erase(iterator it); //删除迭代器指向位置的键值对，并返回指向下一个元素的迭代器T.at(key); // 根据key获取对应value值T.find(key); //返回指向键值key的迭代器指针T.count(key); mutlimap的基本用法mutlimap容器保存的是有序的键值对，键值可以重复。 1//操作与map类似 set的基本用法 mutliset的基本用法 3. 容器适配器 stack的基本用法 queue的基本用法 prority_queue的基本用法","link":"/blog/2019/10/13/C++/C++中STL用法详解二-常用容器用法介绍/"},{"title":"(二)TensorFlow常量、变量、占位符及op详解","text":"TensorFLow提供了一个库来定义和执行对张量的各种数学运算。张量可理解为一个n维矩阵，所有类型的数据，包括标量、矢量、和矩阵等都是特殊类型的张量。 数据的类型 张量 形状 标量 0维张量 [] 向量 1维张量 [D0] 矩阵 2维张量 [D0,D1] 张量 N维张量 [D0,D1…Dn-1] TensorFlow支持以下三种类型的张量： 一、TensorFlow常量常量是值不能改变的张量。 声明标量、向量、矩阵的常量 1234567891011121314151617#1.声明一个标量常量：t_1 = tf.constant(4)#2.声明一个向量常量：t_2 = tf.constant([4,3,2])#3.声明一个2*3的零矩阵常量、全1矩阵常量（第二个参数表示类型）：zero_t = tf.zeros([2,3],tf.int32)ones_t = tf.ones([2,3],tf.int32)#4.创建一个与现有Numpy数组或张量常量具有相同形状的张量常量t_3 = tf.zeros_like(t_2)t_4 = tf.ones_like(t_2)#5.声明一个在一定范围内等差排列的序列t_5 = tf.linspace(start,stop,num)t_6 = tf.range(start,limit,delta) 创建随机张量TensorFlow允许创建具有不同分布的随机张量： 12345678#1.创建标准正态随机分布(mean表示均值，stddev表示标准差)t_random = tf.random_normal([2,3],mean=2.0,stddev=4,seed=None)#2.创建正态随机分布t_random = tf.truncated_normal([2,3],mean=2.0,stddev=2,seed=None)#3.创建均匀随机分布t_random = tf.random_uniform([2,3],minval=0..0,maxval=1.0,seed=None) 如果t_random是要将给定的[3,6]张量随机剪裁为[2,5]大小： 1tf.random_crop(t_random, [2,5], seed=None) 二、TensorFlow变量Variable变量通常在神经网络中表示权重和偏置。 Variable是TensorFlow下可以修改的张量，需要定义一个初始值，初始值可以是数值、列表、numpy矩阵，也可以直接是张量。 123456789#1.使用常量定义var_a = tf.Variable(3, dtype=tf.int32)var_b = tf.Variable([1,2], dtype=tf.float32)var_c = tf.Variable(tf.zeros([1024,10]))#2.使用另一个张量定义rand_t = tf.random_uniform([50,50], mean=0, stddev=10, seed=0)t_a = tf.Variable(rand_t)t_b = tf.Variable(rand_t) 在使用Variable时，必须初始化变量，也就是调用它们的初始化方法。调用初始化的方法可以全局调用，也可以初始化某些变量： Variable变量可以通过assign赋值，赋值的过程是一个op，也是需要执行才会产生效果： 12345678910111213141516import tensorflow as tfvar_a = tf.Variable(3, dtype=tf.int32)#变量重新赋值 assign_op = var_a.assign(5)#变量初始化 init = tf.global_variables_initializer()#开启一个会话 with tf.Session() as sess: #执行变量初始化op sess.run(init) #执行赋值op sess.run(assign_op) #输出赋值后新的结果 print(sess.run(var_a)) 三、TensorFlow占位符TensorFlow允许通过tf.placeholder(dtype,shape=None,name=None)在构建图的时候占据一个位置，然后等到执行图的时候再带入具体的值。占据一个位置： 执行图时赋值： 123456789import tensorflow as tfx = tf.placeholder(dtype=tf.float32)y = 2 * xdata = tf.random_uniform(shape=[4,5], minval=0.0, maxval=10)#开启一个会话 with tf.Session() as sess: x_data = sess.run(data) print(sess.run(y, feed_dict={x: x_data})) （需要注意的是，所有常量、变量和占位符将在代码的构建图部分中定义。如果在定义部分使用 print 语句，只会得到有关张量类型的信息，而不是它的值。为了得到相关的值，需要创建会话图并对需要提取的张量显式使用运行命令） 四、opTensorFlow的基本操作定义为op，以下是常见的op:矩阵的reshape在数据处理方面经常用到：","link":"/blog/2019/09/17/TensorFlow/二-TensorFlow常量、变量、占位符及op详解/"},{"title":"(五)TensorFlow实现线性回归算法","text":"本节以最简单y=0.7x+0.6线性回归为例，将TensorFlow训练、训练后模型保存及可视化串联介绍。 一、相关步骤 数据预处理（可选） 构建模型 定义损失函数及优化方法 使用梯度下降迭代训练(可其它优化方法) 保存模型 画图12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import tensorflow as tf# 消除警告(使用源码安装可自动消除)import osos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'# 回归函数def my_regression(): # 准备数据 with tf.variable_scope(\"data\"): # 准备100 条数据x的平均值为5.0 标准差为1.0 x = tf.random_normal([100, 1], mean = 5.0, stddev=1.0, name=\"x\") # 真实的关系为 y = 0.7x + 0.6 y_true = tf.matmul(x, [[0.7]]) + 0.6 # 创建模型 with tf.variable_scope (\"model\"): # 创建权重变量 weight = tf.Variable(tf.random_normal([1, 1], mean=1.0, stddev=0.1), name=\"weight\") # 创建偏置变量,初始值为1 bias = tf.Variable(1.0, name=\"bias\") # 预测结果 y_predict = tf.matmul(x, weight) + bias # 计算损失 with tf.variable_scope (\"loss\"): # 计算损失 loss = tf.reduce_mean(tf.square(y_predict - y_true)) # 减少损失 with tf.variable_scope(\"optimizer\"): # 梯度下降减少损失,每次的学习率为0.1 train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss) # 收集变量 tf.summary.scalar(\"losses\", loss) tf.summary.histogram(\"weightes\", weight) # 合并变量 merged = tf.summary.merge_all() # 初始化变量 init_op = tf.global_variables_initializer() # 梯度下降优化损失 with tf.Session() as sess: sess.run(init_op) print(\"初始的权重为{}, 初始的偏置为{}\".format(weight.eval(), bias.eval())) # 添加board记录文件 file_write = tf.summary.FileWriter('/Users/liuyan/tensorBoard/my_regression', graph=sess.graph) # 循环训练线性回归模型(20000次) for i in range(20000): sess.run(train_op) print(\"训练第{}次的权重为{}, 偏置为{}\".format(i,weight.eval(), bias.eval())) # 观察每次值的变化 # 运行merge summery = sess.run(merged) # 每次收集到的值添加到文件中 file_write.add_summary(summery, i)if __name__ == '__main__': my_regression() 二、运行结果 三、模型的保存 模型的保存 12saver = tf.train.Saver()saver.save(sess, \"./tmp/ckpt/test\") 模型恢复 1save.restore(sess, \"./tmp/ckpt/test\") 四、可视化 五、扩展关于损失函数和优化器，有很多种方法，其中也包括很多种激活函数，激活函数加在每一层神经网络后，避免多层网络变成单层。在线性回归基础上使用sigmoid激活函数就变成了逻辑回归(常用于二分类)，使用softmax激活函数就是多分类。 常用激活函数 常用损失函数 常用优化器","link":"/blog/2019/09/24/TensorFlow/五-TensorFlow实现线性回归算法/"},{"title":"(四)TensorFlow数据读取","text":"TensorFlow主要提供了三中读取数据的方式： 供给数据(Feeding)： 在TensorFlow程序运行的每一步， 让Python代码来供给数据。 文件读取： 在TensorFlow图的起始， 让一个输入管线从文件中读取数据。 预加载数据：在TensorFlow图中定义常量或变量来保存所有数据(仅适用于数据量比较小的情况)。 一、供给数据供给数据(Feeding)就是之前讲解变量那一节提到的通过feed_dict给placeholder占位符提供值。1234with tf.Session(): input = tf.placeholder(tf.float32) classifier = ... print classifier.eval(feed_dict={input: my_python_preprocessing_fn()}) 二、文件读取当数据集很大，使用此方法可以确保不是所有数据都立即占用内存（如60GB的YouTube-8m数据集）。从文件读取的过程可以通过以下步骤完成： 使用字符串张量 [“file0”，”file1”] 或者 [(“file%d”i)for in in range(2)] 的方式创建文件命名列表，或者使用 files=tf.train.match_filenames_once('*.JPG') 函数创建。 将文件名列表交给tf.train.string_input_producer 函数来生成一个先入先出的队列，文件阅读器会需要它来读取数据。 12#string_input_producer提供的可配置参数来设置文件名乱序和最大的训练迭代数filename_queue = tf.train.string_input_producer(files) Reader用于从文件名队列中读取文件：根据输入文件格式选择相应的阅读器，然后将文件名队列提供给阅读器的read方法。 Decoder：使用一个或多个解码器和转换操作将值字符串解码为构成训练样本的张量：上一步阅读器的read方法会输出一个key来表征输入的文件和其中的纪录(对于调试非常有用)，同时得到一个字符串标量，这个字符串标量可以被一个或多个解析器，或者转换操作将其解码为张量并且构造成为样本。 以CSV格式文件举例：12345678910111213141516171819202122filename_queue = tf.train.string_input_producer([\"file0.csv\", \"file1.csv\"])reader = tf.TextLineReader()key, value = reader.read(filename_queue)# Default values, in case of empty columns. Also specifies the type of the# decoded result.record_defaults = [[1], [1], [1], [1], [1]]col1, col2, col3, col4, col5 = tf.decode_csv(value, record_defaults=record_defaults)features = tf.concat(0, [col1, col2, col3, col4])with tf.Session() as sess: # Start populating the filename quee. coord = tf.train.Coordinator() threads = tf.train.start_queue_runners(coord=coord) for i in range(1200): # Retrieve a single instance: example, label = sess.run([features, col5]) coord.request_stop() coord.join(threads) 每次read的执行都会从文件中读取一行内容， decode_csv 操作会解析这一行内容并将其转为张量列表。如果输入的参数有缺失，record_default参数可以根据张量的类型来设置默认值。在调用run或者eval去执行read之前， 必须调用tf.train.start_queue_runners来将文件名填充到队列。否则read操作会被阻塞到文件名队列中有值为止。 三、预加载数据这仅用于可以完全加载到存储器中的小的数据集。有两种方法： 存储在常量中 123456training_data = ...training_labels = ...with tf.Session as sess: x_data = tf.Constant(training_data) y_data = tf.Constant(training_labels)... 存储在变量中，初始化后，永远不要改变它的值 12345678910training_data = ...training_labels = ...with tf.Session() as sess: data_initializer = tf.placeholder(dtype=training_data.dtype,shape=training_data.shape) label_initializer = tf.placeholder(dtype=training_labels.dtype,shape=training_labels.shape) input_data = tf.Variable(data_initalizer, trainable=False, collections=[]) input_labels = tf.Variable(label_initalizer, trainable=False, collections=[]) ... sess.run(input_data.initializer,feed_dict={data_initializer: training_data}) sess.run(input_labels.initializer,feed_dict={label_initializer: training_lables})","link":"/blog/2019/09/17/TensorFlow/四-TensorFlow数据读取/"},{"title":"web项目外网服务器","text":"本地JAVAWEB项目，如果想通过外网URL可以直接访问，就需要将本地程序包的war包上传到远程服务器指定文件。本文以上传ubuntu服务器为例介绍。 步骤 远程服务器配置安装Apache、Tomcat、MySQL[参考网址] 将本地程序上传服务器 将数据库脚本.sql文件上传服务器 将本地程序上传服务器在配置好服务器环境之后，选择任何一种可连接远程终端工具将项目文件上传即可，在这里推荐Mobatek，支持窗口可视化及shell等多种用户操作方式。 登录方式选择ssh，输入远程服务器ip地址、用户 将压缩后的程序war包放入/var/lib/tomcat/webapps文件夹下 此时连接 服务器ip:8080/项目名即可外网访问项目（服务器运行慢的情况下，刚将项目上传需要多刷新几遍才能出来） 将本地数据库上传服务器 将本地数据库导出到.sql脚本文件，再将该文件上传到服务器任意文件夹下 在命令行进入mysql命令模式 1234mysql -u用户名 -p密码 //进入数据库命令模式show databases; //查看当前有哪些数据库creat database 数据库名 //创建自己的数据库source /var/lib/tomcat/webapps/数据库名.sql //执行上传的sql脚本文件，source后拼接的是.sql文件的路径 额外需注意问题：windows数据库不区分大小写，Linux区分，需要修改解决办法是：修改MySQL的配置文件my.cnf，在[mysqld]部分添加如下配置选项lower_case_table_names = 1，然后重启MySQL服务即可","link":"/blog/2019/08/12/WEB/web项目发布外网服务器/"},{"title":"pycharm进行远程服务器代码编写与调试","text":"使用场景PyCharm是一种Python IDE，带有一整套可以帮助用户在使用Python语言开发时提高其效率的工具。 当跑一些机器学习或者深度学习代码时，由于数据量较大用本机跑可能较慢。此时就可以用一台额外配置了python和深度学习库的服务器，比如tensorflow，keras，pytorch等。 为了实现这些，总不能每次写完一部分代码后，再一次性上传服务器进行调试吧，这种方式很笨拙。真正有效的方法是实现本地编辑代码能够与服务器同步。因此就需要进行以下一些相关配置。 相关操作不难，几分钟左右即可。 步骤 [ ] 配置远程服务器信息能够上传到服务器也能从服务器下载 [ ] 配置Interperter解释器信息能够调用远程服务器的解释器去执行代码 配置远程服务器信息 打开pycharm，选择Tools —- Deployment —- Configuration,如下图所示： 然后，选择左上方的加号，选择SFTP即可，name 按照自己习惯编写。然后填写如下信息： 然后配置mapping（相当于本地项目文件地址与远程服务器项目地址的对应关系）: 配置完这些之后，就发现Development里的上传和下载可以点击了，在这里可以点击自动上传，这样以后本地代码一有改动就会自动的变更到服务器，实现真正同步： 配置Interperter解释器信息 点击files ——setting —- project Interpreter: 可以看到上图中我选择的解释还是本地的，现在需要选择远程服务器的解释器，如果上一部分的服务器信息已经配置好，这里是直接有可以选择的：到此，整体配置完成，已经可以实现本地与远程服务器同步。 测试是否成功现在如果远程服务器已经安装了TensorFlow深度学习框架，那么在本地运行一个简单的TensorFlow程序，如果可以跑通说明配置成功：1234567891011import tensorflow as tf# 创建常量 hello = tf.constant(&apos;Hello,world!&apos;)# 创建会话 sess = tf.Session()# 执行 result = sess.run(hello)# 关闭会话 sess.close()# 输出结果 print(result) 优点 可以直接在本机上编写代码 代码自动同步到远程服务器 在远程服务器上的解释器中执行代码，返回结果。和本地使用pycharm是一样的感觉。","link":"/blog/2019/09/05/python/pycharm进行远程服务器代码编写与调试/"},{"title":"(一)Python基础_面向过程语法介绍","text":"Python是一门易于学习和维护的语言，现在常被用来作为数据分析、人工智能的基础入门语言。也可以来做一些应用，比如YouTube、知乎、豆瓣网。Python3与Python2在一些地方差别还是较大的，推荐以Python3入手。 1.Python3基本数据类型python3中有六个标准的数据类型，下图中标红的为基本数据类型，另外为4个容器： 其中Number、String、Tuple又属于不可变类型；List、Set、Dictionary属于可变类型。关于可变类型与不可变类型，以及Python变量在内存的存储与地址变化，该链接总结很好。[link]类型之间的相互转换： 2.Python3注释 单行注释 1# 这是单行注释 多行注释使用三个单引号将注释括起来 3.Python3运算符 4.条件控制Python中都是以代码缩进划分语句块，并且没有switch分支语句。123456if condition_1: statement_block_1elif condition_2: statement_block_2 else: statement_block_3 statement_block_2 5.循环语句 for循环 12for &lt;variable&gt; in &lt;sequence&gt;: 执行语句 while循环 12while 判断条件： 执行语句 break、continue、pass语句可以通过break、continue、pass分别跳出循环、跳过当前循环中的剩余语句块、不做任何事。 6.导入模块模块是一个包含所有你定义的函数和变量的文件，其后缀名是.py。模块可以被别的程序引入，以使用该模块中的函数等功能。这也是使用 python 标准库的方法。123# import module1, module2, module3 # from module import name1, name2 # from module import * # import module as something # from module import name1 as something python中的__name__属性可以用来使程序块仅在该模块自身运行时执行，如下所示：1234if __name__ == '__main__': print('程序自身在运行') else: print('我来自另一模块') 7.输入输出 输入 键盘输入 12str = input(\"请输入：\");print (\"你输入的内容是: \", str) 文件读取 1234f = open(\"/tmp/foo.txt\", \"w\")f.write( \"I love China!\\n\" )# 关闭打开的文件f.close() 输出print输出，可以选择format格式或者按照print自带的格式。 pickle模块 8.切片关于string类型及四个容器类型的访问都可以通过切片实现。 格式切片[start:end:steps] 其中默认步长为1，start缺省的话就是0，end缺省的话就是到结束。123456li = list(range(10))print(li[2:5]) #输出[2,5)print(li[:4]) #输出[0,4)print(li[5:]) #输出[5,9)print(li[0::1]) #输出[0,9)print(li[9:0:-1]) #步长为负数情况向下,输出[9, 8, 7, 6, 5, 4, 3, 2, 1]","link":"/blog/2019/10/03/python/一-Python基础-面向过程语法介绍/"},{"title":"(三)Python基础_文件访问与函数调用","text":"对基本的文件读取、函数调用进行整理介绍。 1. 文件访问Python中通过open()方法打开一个文件，并返回文件对象，对文件进行处理过程都需要使用这个函数。1open(file, mode = 'r') 方法 2. 函数调用Python的函数调用，不可变对象属于值传递，可变对象属于引用传递。 默认参数调用函数时，如果没有传递参数，则会使用默认参数： 12345def func(x, y=500): return x + y print('1.默认参数:')print(func(100)) #相当于执行100 + 500print(func(100, 600)) #相当于执行100 + 600print(func(y = 300, x = 200))#参数次序不用一定对应，只需标出即可 关键字参数函数调用使用关键字参数来确定传入的参数值，使用关键字参数允许函数调用时参数的顺序与声明时不一致，因为 Python 解释器能够用参数名匹配参数值： 12345def printinfo(name,age): print(\"名字: \",name) print(\"年龄: \",age) #调用printinfo函数 printinfo(age=50, name=\"runoob\") 可变参数如果需要一个函数处理比声明时更多的参数。这些参数叫做不定长参数： 1234567891011121314151617181920 #加了*的参数会以tuple元组的形式导入，存放所有未命名的变量参数 def printinfo( arg1, *vartuple ): print (arg1) print (vartuple) # 调用printinfo 函数 # 输出: 70 (60, 50) printinfo( 70, 60, 50 ) # 加了**的会以字典的形式导入 def printinfo( arg1, **vardict ): print (arg1) print (vardict) # 调用printinfo 函数 # 输出: 1 {'a': 2, 'b': 3} printinfo(1, a=2,b=3) #若单独出现*号，后面的参数必须用关键字传入 def f(a,b,*,c): return a+b+c f(1,2,c=3) 匿名函数 lambda 只是一个表达式，函数体比 def 简单很多。 lambda的主体是一个表达式，而不是一个代码块。仅仅能在lambda表达式中封装有限的逻辑进去。 lambda 函数拥有自己的命名空间，且不能访问自己参数列表之外或全局命名空间里的参数。 虽然lambda函数看起来只能写一行，却不等同于C或C++的内联函数，后者的目的是调用小函数时不占用栈内存从而增加运行效率。 1234sum = lambda arg1,arg2:arg1 + arg2 # 调用sum函数 print(\"相加后的值为:\",sum(10,20)) print(\"相加后的值为:\",sum(20,20))","link":"/blog/2019/10/06/python/三-Python基础-文件访问与函数调用/"},{"title":"(二)Python基础_常用容器","text":"Python中常用的容器主要有四种：List、Set、Dictionary、Tuple。其中Tuple属于不可变类型，其余三类属于可变类型。 1.List列表List列表是最常见的基本数据结构，其数据项可以为不同类型，也可以为子表。 访问列表元素 123list2 = [1, 2, 3, 4, 5, 6, 7]; print(\"list1[0]: \", list1[0]) print(\"list2[1:5]: \", list2[1:5]) 修改列表元素 1list[2] = 2001 删除列表元素 1del list[2] 方法 2.Tuple元组元组与list类似，不同之处在于tuple不可以修改 3.Dictionary字典字典是另一种可变容器模型，且可存储任意类型对象。字典的每个键值(key=&gt;value)对用冒号(:)分割，每个对之间用逗号(,)分割，整个字典包括在花括号{}中 ,格式如下所示：dict = {key1:value1, key2:value2} 访问元素 12# 若用字典里没有的键访问数据，会出现错误dict = {'Name': 'Runoob', 'Age': 7, 'Class': 'First'} print (\"dict['Name']: \", dict['Name']) 修改元素 1dict['Age'] = 8 删除元素 123del dict['Name'] # 删除键 'Name' dict.clear() # 清空字典 del dict # 删除字典 常用方法 4.Set集合set集合是一个无序的不重复元素集合。可以使用大括号{}或set()函数创建集合；创建空集必须用set()。 创建格式 123parame = {value01, value02,...}或者set(value) 添加元素 12#若元素已存在则不执行任何操作s.add(x) 移出元素 1234#若元素不存在则会发生错误s.remove(x)#随机删除一个元素s.pop() 常用方法","link":"/blog/2019/10/05/python/二-Python基础-常用容器/"},{"title":"(四)Python基础_列表推导及迭代器生成器","text":"对Python中的高级特性：列表推导、迭代器生成器进行介绍。 列表推导列表生成式即List Comprehensions，是Python内置的非常简单却强大的可以用来创建list的生成式。举个例子，要生成list [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]可以用list(range(1, 11))如果要生成[1x1, 2x2, 3x3, …, 10x10]，可以通过循环，但这样太繁琐，用列表生成式可以用一行语句生成：12345L = []for i in range(1,11): L.append(ixi)#列表方法L = [ixi for i in range(1,11)] for 循环后还可以添加if判断语句，仅筛选出偶数的平方：1L = [ixi for i in range(1,11) if i % 2 == 0] 还可以使用双重循环生成全排列：12# 输出['AX', 'AY', 'AZ', 'BX', 'BY', 'BZ', 'CX', 'CY', 'CZ'][m + n for m in 'ABC' for n in 'XYZ'] 迭代器生成器 生成器使用了yield函数被称为生成器generator，返回的是一个迭代器。在调用生成器运行过程中,每次遇到yield时函数会暂停并保存当前所有的运行信息，返回yield的值，并在下一次执行next()方法时从当前位置继续运行。 迭代器从集合的第一个元素开始访问，直到所有的元素被访问完结束，只能往前不会后；字符串、列表或元组对象都可用于创建迭代器，next()方法返回的是下一个迭代器对象。1234567891011121314151617#斐波那契数列使用迭代器生成器实现 import sysdef fibonacci(n,w=0): # 生成器函数 - 斐波那契 a, b, counter = 0, 1, 0 while True: if (counter &gt; n): return yield a a, b = b, a + b print('%d,%d' % (a,b)) counter += 1 f = fibonacci(10,0) # f 是一个迭代器，由生成器返回生成 while True: try: print (next(f), end=\" \") except : sys.exit()","link":"/blog/2019/10/06/python/四-Python基础-列表推导及迭代器生成器/"},{"title":"(一)面向对象基础","text":"本文对面向对象基础进行一个简单回顾，方便之后理解各种有趣的设计模式，几分钟就可读完。 前言本人本科专业为软件工程，曾参加过一些算法方面竞赛，因为对研究方面较感兴趣，也是为了进一步扩展知识面，研究生阶段开始投入HCI方面研究，主要是针对视觉方面的人机交互人体目标检测进行研究。 在研究生刚开始阶段曾和实验室小伙伴协同开发过两个企业级WEB方面项目，预计今年九月份都能正式上线： ONOS系统（前端）[中国通号院CRSC])采用Vue框架 设备管理系统（全栈）[北京诚星科技有限]自己搭建环境，后台采用Spring+SpringMVC+Hibernate框架，前端采用EasyUI框架 当然，在这期间也曾遇到项目重构、Bug接连不断等各方面问题，在了解自己能力尚且不足同时，越来越明白设计模式、高内聚低耦合、程序规范化、质量检测等在项目开发中真正的重要性。无论做python开发还是以后做研究，工程类的项目，该思想应该都有值得借鉴之处。因此我准备在文章分类里多添加一个设计模式的分类，在这里对主要经典常用的一些设计模式进行相关介绍总结。相信整体浏览下来你一定能受益匪浅。 知识理解 类与实例化 类：具有相同属性和功能的对象的集合；那么为什么要用类和实例而不是传统的函数调用呢，这就相当于如果居委会的电视放在你家里，而别人家里没有，于是街坊邻里都要到你家来看电视（函数调用），这样并不合适。所以正确的办法应该是将公用的方法放在居委会（类文件）。 实例：类的具体对象；实例化相当于从类工厂又生产出一台电视，任何需要的地方都可以实例化它。 构造方法与方法重载 构造方法：对类进行初始化，比如对于一个猫的实例，我们希望一出生就可以有姓名，那么就应该写一个有参数的构造方法：12Cat cat1 = new Cat(&quot;喵喵&quot;)Cat cat2 = new Cat(&quot;咪咪&quot;) 方法重载：提供了创建同名的多个方法能力，可以在不改变原方法基础上，新增功能；重载也算是提供了方法的可扩展能力，比如实例化一个猫的时候给它起不起名字都可以：12Cat cat1 = new Cat()Cat cat2 = new Cat(&quot;喵喵&quot;) 属性与修饰符属性的getter setter作用是添加控制；修饰符常用的有public private 封装继承多态 封装：每个对象都包含它能进行操作所需要的所有信息 继承：（Java中构造方法不能被继承，继承使得修改和扩展都变得容易，但也增大了两个类之间的耦合性以及破坏包装，将父类暴露给子类） 多态：不同的对象可以执行相同动作，但要通过它们自己的实现代码来执行。比如说有一对父子是表演“京剧”的，有一天父亲发高烧上不了台表演，退票的话肯定会大大影响声誉，于是就决定让儿子代父亲上台表演。这里有几点需要注意：(一)子类以父类的身份出现；(二)子类在工作时以自己的方式来实现；(三)子类以父类身份出现时，子类特有属性和方法不可以使用123arrayAnimal = new Animal[2];arrayAnimal[0] = new Cat(&quot;小花&quot;);arrayAnimal[1] = new Dog(&quot;阿毛&quot;); 重构采用合适的设计模式对原先结构进行重新构建 抽象类与接口 抽象类：实例化没有任何意义不能实例化的类例如说一只猫长得什么样可以想象 ，说动物长什么样，没办法知道。抽象方法必须被子类重写，如果一个类中包含抽象方法也一定是抽象类；实际中看具体情况看父类是否需要改成抽象类。 接口：接口是将隐式公共方法和属性组合起来，以封装特定功能的集合。以下图为例，比如变出东西是小叮当、孙悟空、猪八戒分别具备的功能，如果为了更具有普遍意义而让它们的父类也具有此种功能，显然是不合适的，比如猫并不具备变出来东西功能。所以为了将特定行为进行抽样，采用接口。主要需要注意的是接口与抽象类的区别与联系：| 抽象类 | 接口 || ————- | ———— || 对类的抽取 | 对行为的抽取 || 对一些相似类对象，用继承抽象类 | 若行为跨越不同类的对象可使用接口 || 抽象类是从子类中泛化出父类 | 接口是不知道子类的存在，预先定义 || 一个类只能继承一个抽象类 | 一个类可继承多个接口 | 集合与泛型 集合：用于对数据进行存储和检索的专用类叫做集合比如说ArrayList集合就是对IList接口的实现； 泛型是具有占位符（参数类型）的类、结构、接口、方法泛型集合可以将类型参数作为它所存储对对象类型的占位符。1List &lt;Animal&gt; arraylist;s 委托与事件","link":"/blog/2019/08/13/设计模式/001-面向对象基础/"},{"title":"(二)简单工厂模式","text":"面向对象的好处在于通过封装、继承、多态把程序的耦合度降低。使用设计模式可以使得程序更加的灵活，容易修改且易于复用。 简单工厂模式 例子：使用任何一种面向对象的语言实现一个计算器控制台程序，要求输入两个数和运算符号，得到结果。 首先使用面向对象的封装特将操作抽成一个类： Operation运算类： 客户端代码：在只使用封装的基础上，如果此时要添加一个求根号的操作，则需在Operation类的switch中新增一个分支，但是应该避免在新增分支的时候对原有代码进行修改。因此应该把加减乘除等运算分离，修改其中一个不影响另外几个，增加其它运算算法也不影响其它代码。 使用继承抽离操作类中的具体算法： 定义四个子类继承操作类：在该基础上可以实现，如果想要新增或修改一个算法，只需新增一个子类或修改一个子类，对其它方法不会产生影响。但是问题来了，不知道该如何让计算器去实例化对象，总不能在客户端代码里写一个switch判断，看是什么操作符返回什么运算子类进行操作吧。那么如果再新加一个运算子类（比如说开根号），总不能再去修改客户端代码，在switch里添加一个分支吧。 使用多态添加一个工厂类，实例化出合适的对象： 工厂类： 客户端代码： 类图：这样再新加一个操作，只需新增一个操作子类，然后再在工厂类里添加一个分支就行了。","link":"/blog/2019/08/16/设计模式/002-简单工厂模式/"},{"title":"(三)商场促销——策略模式","text":"面向对象的编程并不是类越多越好，类的划分是为了封装，但分类的基础是抽象，具有相同属性和功能的对象的集合才是类。 需求： 假如设计一个商场收银软件，营业员根据客户所购买的商品的单价和数量，向客户收费。==考虑到不同商品有不同的促销方式：打1折、打3折、满300减100、满200减50、满100积分10点（积分达到一定程度可以领取相对应奖品）==。考虑界面如下： 简单工厂实现 按照上一节的讲法可以根据简单工厂模式建立类图如下，其中注意为了更好的抽象，应将打折操作（1折、2折等）抽象为一个类而不是两个类，其余操作也类似：如果现在需要再添加打5折、满500减200活动，按照简单工厂的思想，只需要在收费生成对象工厂添加两个switch条件，再在下拉选框里加两项就OK了；如果加入满100送10积分的活动，需要添加该收费标准子类，再到界面稍加改动， 可以看出的是，简单工厂只是解决了对象创建的问题，每次维护和扩展收费方式都要改动这个工厂，以致代码需要重新编译部署。面对算法时常变动，应该有更好的设计模式去选择。 策略模式实现 策略模式：是一种定义一系列算法的方法，从概念上看，所有这些算法完成的都是相同的工作，只是实现不同，它可以相同的方式调用所有的算法，减少各种算法与使用算法类之间的耦合。（即通过一个Context类，引入Strategy策略父类，根据传入不同对象，调用具体对应方法），**策略模式封装变化**CashContext类：客户端主要代码： 简单工厂-策略模式相结合 可以发现策略模式又回到未用简单工厂之前的老套路，直接在哭护短去判断使用哪一个算法。显然是不合适的。所以可以将简单工厂在策略模式基础上将判断过程从客户端程序移走。改造后CashContext类：改造后客户端主要代码：对比原先简单工厂的发现，简单工厂需要客户端认识两个类：CashSuper、CashFactory；而现在只需要认识一个CashContext类就可以了。使得具体的收费算法彻底地与客户端分离，连算法的父类CashSuper都不让客户端认识了，耦合度更加降低。不过，目前仍不够完美，因为CashContext里还是用到了switch判断，但是相比原先的简单工厂改动成本较小。当然还有更好的办法，比如反射技术，在后面会进行相关整理介绍。","link":"/blog/2019/08/23/设计模式/003-商场促销——策略模式/"},{"title":"(四)设计模式基本原则法则","text":"本节对4个基本原则及1个基本法则进行简要整理介绍。 拍摄UFO——单一职责原则 单一职责：就一个类而言，应该仅有一个引起它变化的原因。如果一个类承担的职责过多，就等于把这些职责耦合在一起，一个职责的变化可能会削弱或者抑制这个类完成其它职责的能力。软件设计真正要做的许多内容，就是发现职责并把那些职责相互分离。 随着移动终端的发展，智能手机已经集成了众多功能：听音乐、玩游戏、拍照、摄像等。但有时一件产品简单一些，职责单一些，或许是更好的选择，比如摄像机拍摄性能要比手机更好一些。 比如要写一个WinForm窗口程序，总不能把操作的方法都写入窗口类，更好的策略应该是将界面代码和逻辑代码相分离，即将界面职责和逻辑职责单独分开。（手机的发展有它的特点，而编程时，我们却是要在类的职责分离上多思考，做到单一职责，这样代码才是真正的易维护、易扩展、易复用、灵活多样） 考研求职两不误——开闭原则 开闭原则：对于扩展是开放的，对于更改是封闭的。面对需求的改变应保持相对稳定，从而使得系统可以在第一个版本以后不断推出新的版本。当然改变大都是不可预测的，设计人员需要先猜测出最有可能发生的变化种类，然后构造抽象来隔离那些变化。 比如在简单工厂运算器问题中，如果想添加一个乘法运算，只需要新增一个运算子类就行，与加法、减法、客户端隔离开来。这样就是面对需求，对程序的改动是通过增加新代码实现，而不是更改现有的代码。 （比如，考研过程中，考研本身是不会改变的，为了做完全准备，可以在考研期间不影响考研本身前提下，扩展的写一写简历了解招聘的资讯） 会修电脑不会修收音机——依赖倒转原则 依赖倒转原则： 高层模块不应依赖低层模块，两个都应依赖抽象 抽象不应依赖细节，细节应该依赖抽象 我们可以把电脑理解成大的软件系统，任何部件如CPU、内存、硬盘、显卡等都可以理解为程序中封装的类或程序集，不管哪一个出了问题都可以在不影响别的部件的前提下进行修改和替换。具体一点就是接口和抽象类，只要接口是稳定的，那么任何一个的更改都不用担心其它受到影响。这使得高层模块和低层模块都能很好被复用。 收音机就是典型的耦合过度，只要收音机出问题，不懂的人根本没法修，因为任何问题都可能涉及其他部件，各个部件相互依赖，难以维护。 不太理解的话在看下面一个例子：从上面的类图中可以看出，司机类和奔驰车类都属于细节，并没有实现或继承抽象，它们是对象级别的耦合。通过类图可以看出司机有一个drive()方法，用来开车，奔驰车有一个run()方法，用来表示车辆运行，并且奔驰车类依赖于司机类，用户模块表示高层模块，负责调用司机类和奔驰车类。 这样乍一看没问题，但是有一天如果司机想换一辆宝马了，但是却不能开，因为司机类里没有对宝马的依赖。下面引入依赖倒转原则重新设计一下类图：可以看出在新增低层模块（汽车）时，只修改了高层模块（Client），对已有的司机和车类都不用变动。 里氏代换原则 里氏代换原则：子类型必须能够替换掉父类型。 迪米特法则 迪米特法则：如果两个类不直接通信，那么这两个类就不应当发生直接的相互作用。如果其中一个类需要调用另一个类的某一个方法，可以通过第三者转发这个调用。 （类似于在controller层调用service层，再通过service层调用dao层访问数据库一样；而不是直接的由控制层调用数据访问层。）","link":"/blog/2019/09/03/设计模式/004-设计模式基本原则法则总结/"},{"title":"hexo个人博客搭建","text":"Hexo是一款基于Node.js的静态博客框架，依赖少易于安装使用，可以方便的生成静态网页托管在GitHub上。以下步骤操作简单，只需先自行安装好node.js和git，剩余2~3小时大概就可完成。 所需工具 node.js git 安装配置步骤 安装node.js和git之后安装hexo:npm install -g hexo-cli安装完成之后使用npm -v查看是否安装成功 创建hexo项目在本地新建一个blog的文件夹在这个文件右键进入git bash模式命令模式下执行hexo init初始化博客 生成SSH密钥打开Git Bash，使用以下命令配置gitgit config --global user.name &quot;你的github用户的名字&quot;git config --global user.email &quot;你的github账户邮箱&quot;cd ~/.sshssh-keygen -t rsa -C &quot;你的github账户邮箱&quot;连续三个回车eval &quot;$(ssh-agent -s)&quot;，添加密钥到ssh-agentssh-add ~/.ssh/id_rsa，添加生成的SSH key到ssh-agentcat ~/.ssh/id_rsa.pub复制此时显示的内容，内容应该是以ssh-rsa开头 Github新建一个仓库，并配置SSH密钥Ctrl+C退出后，在GitHub上新建一个新的仓库，仓库名随意，不过需要记录下来，我这里起名叫blog，最下面的Initialize this repository with a README要勾选上，然后保存即可。进入这个仓库后选择Settings，在左侧选项卡Options中翻到下面，GItHub Pages这项，Source选择master branch，选择save后，会在这部分的标题处写明这个仓库的url，这就是你博客的url了。还是页面的左侧的选项卡，Deploy 选择Add deploy key，添加密钥。 Title随意，我设置为了blog Key粘贴我们刚才复制的那一段。 最下面Allow write access要打勾. 选择Add Key即可。 然后在Git Bash中使用 ssh -T git@github.com测试，如果看到Hi后面是你的用户名，就说明成功了。 使用npm install安装需要的组件 使用npm install hexo-deployer-git --save安装插件 修改hexo配置文件 打开本地博客的根目录，找到_config.yml文件，在文件的开头处，第二部分，url改成自己Github仓库的地址，root改为自己/自己本地仓库名/，如下所示： 123456# URL## If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos;url: https://github.com/bjutliuyan/blogroot: /blog/permalink: :year/:month/:day/:title/permalink_defaults: 再在最下面添加如下片段，repository这项，应该去GitHub里面新建的那个叫blog的仓库里面找。进入仓库主页后，点击右侧绿色的按钮Clone or download，在新弹出的窗口右上角选择Use SSH，然后将下面的文字复制粘贴到此处。修改完配置文件后保存退出即可。 123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repository: git@github.com:Davidham3/blog.git branch: master 生成博客hexo g 发布博客hexo d 本地预览博客hexo s通过localhost:4000可以本地预览 本地预览效果","link":"/blog/2019/08/04/随笔/hexo个人博客搭建/"},{"title":"hexo主题配置+博客发布方法","text":"如果你已经成功搭建个人博客，可进一步更换不同主题及添加个性化设置，主题传送门：Themes ，我自己使用的是icarus 主题。 更换主题 克隆icarus主题到本地博客theme文件夹下： git clone https://github.com/ppoffice/hexo-theme-icarus.git 打开站点的_config.yml配置文件，修改主题为icarus: theme: hexo-theme-icarus 发布文章文章编写采用Markdown标记语言书写。Markdown的语法简洁明了、学习容易，而且功能比纯文本更强，世界上最流行的博客平台WordPress和大型CMS如Joomla、Drupal都能很好的支持Markdown。Markdown基本语法大概十几分钟即可学会，更有很多专门性编辑文本软件支持Markdown编写及同步预览。在此推荐Yu Writer Pro。 建立新发布文章命令行运行：hexo n &quot;博客名&quot; 采用Markdown编辑博客中需要用到的一些图片建议使用网络图片链接，减少加载时间 发布博客 12hexo ghexo d 个性化设置 鼠标点击、音乐等 添加评论功能：推荐使用gitment 添加动画[模型预览][添加方法] 添加访客统计：推荐使用revolvermaps [操作步骤]，添加到主题theme文件夹/layout/widget下对应的页面相应组件文件内","link":"/blog/2019/08/09/随笔/hexo主题配置+发布博客方法/"},{"title":"随笔篇20190911","text":"学习研究的乐趣便在于在不断的徘徊、困惑、挑战中得出新的总结，一点点成长。 习惯总结是一个很好的习惯，如果你也喜欢写作，很荣幸能在这里与你分享交流。在这里我会慢慢记录自己的成长，也会偶尔分享些生活学习中的一些小趣事。目前的一个规划，会在后面主要针对以下方面进行总结: OpenCV TensorFlow/Pytorch meaching learning deep learning Python Linux命令 多模态目标识别算法 设计模式 Leecode数据结构刷题 论文阅读","link":"/blog/2019/09/11/随笔/随笔篇20190911/"},{"title":"(三)OpenCV图像处理1","text":"对OpenCV图像处理中最常用的操作：空间变换、几何变换，进行整理介绍 颜色空间转换 转换颜色空间OpenCV中常用到的颜色空间转换的方法主要是两种：BGR&lt;-&gt;GRAY和BGR&lt;-&gt;HSV。用到的函数是cv2.cvtColor(input_image, flag)，其中flag是转换类型。 123456import cv2img = cv2.imread(&apos;image&apos;, image02.jpg)hsv = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)cv2.imshow(&apos;image&apos;, hsv)cv2.waitKey(0)cv2.destroyAllWindows() 物体跟踪可以利用转换颜色空间来提取带有某个特定颜色的物体。在HSV颜色空间中要比在BGR中更容易表示一个特定颜色。比如以在一幅图片中提取蓝色物体为例： 123456789101112import cv2import numpy as np#获取图片并转化到HSV img = cv2.imread(&apos;image02.jpg&apos;, cv2.IMREAD_COLOR)hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)#设定蓝色的阀值 lower_blue=np.array([110,50,50])upper_blue=np.array([130,255,255])#根据阀值进行掩模 mask = cv2.inRange(hsv, lower_blue, upper_blue)#对原图像和掩模进行位运算 res = cv2.bitwise_and(img, img, mask=mask)#展示 cv2.imshow(&apos;res&apos;, res)cv2.waitKey(0)cv2.destroyAllWindows() 几何变换图像的集合变换在计算机视觉任务的数据与处理部分很重要，能更好的清洗数据。 扩展缩放 1res = cv2.resize(img, (newWidth,newHeight), interpolation=cv2.INTER_LINEAR) 平移 12M = np.float32([[1, 0, x], [0, 1, y]])shifted = cv2.warpAffine(img, M, (image.shape[1], image.shape[0])) 旋转 12345# 这里的第一个参数为旋转中心，第二个为旋转角度，第三个为旋转后的缩放因子# 可以通过设置旋转中心，缩放因子，以及窗口大小来防止旋转后超出边界的问题M = cv2.getRotationMatrix2D((cols/2,rows/2), 45, 0.6)# 第三个参数是输出图像的尺寸中心dst = cv2.warpAffine(img, M, (2*cols,2*rows)) 仿射变换在仿射变换中，原图中所有的平行线在结果图像中同样平行。为了创建这个矩阵需要从原图像中找到三个点以及它们在输出图像的位置。 1234pts1=np.float32([[50,50],[200,50],[50,200]])pts2=np.float32([[10,100],[200,50],[100,250]])M=cv2.getAffineTransform(pts1,pts2)dst=cv2.warpAffine(img,M,(cols,rows)) 透视变换透视变换就相当于视角的变换： 1234pts1 = np.float32([[56,65],[368,52],[28,387],[389,390]])pts2 = np.float32([[0,0],[300,0],[0,300],[300,300]])M=cv2.getPerspectiveTransform(pts1,pts2)dst=cv2.warpPerspective(img,M,(300,300))","link":"/blog/2019/09/08/OpenCV/02图像/三-OpenCV图像处理1/"},{"title":"(一)OpenCV介绍及安装","text":"OpenCV是一个跨平台的计算机视觉库，可运行在多个平台，由一系列 C 函数和少量 C++ 类，并同时提供了Python、Ruby、MATLAB等语言的接口，实现了图像处理和计算机视觉方面的很多通用算法。以下介绍前提是已经安装好了Python和基本的numpy、matplotlib库。 Windows下安装OpenCV 到OpenCV官网下载合适的win pack版本，然后傻瓜式操作解压安装： 安装完成之后，配置相应环境变量（依次选择计算机—&gt;属性—&gt;高级系统设置—&gt;环境变量，找到Path变量，然后把OpenCV执行文件的路径新增进去；OpenCV执行文件在解压好的OpenCV文件夹里，依次选择build—&gt;x64—&gt;vc15—&gt;bin）： 找到opencv-&gt;build-&gt;python-&gt;3.5-&gt;x64下的cv2.pyd工具包，拷贝到Python安装目录下的Lib-&gt;site-packages下到此Windows下OpenCV配置完成。 一个简单测试：新建text.py，输入如下的程序，cmd下输入python text.py，如果可以正确的显示图片，证明安装成功。 12345678910import cv2import numpy as npimg = cv2.imread(&quot;1.jpg&quot;)emptyImage = np.zeros(img.shape, np.uint8)emptyImage2 = img.copy()emptyImage3=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)cv2.imshow(&quot;EmptyImage3&quot;, emptyImage3)cv2.waitKey (0)cv2.destroyAllWindows() 显示图片如下： Linux下安装OpenCV首先准备一下开发环境： ubuntu 16.04 64位 python3 安装cmake依赖： 1sudo apt-get install build-essential cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev 安装OpenCV 下载OpenCV安装包并通过Mobatek上传到服务器某一个位置，之后解压： 1unzip opencv-4.0.1.zip 在解压的文件夹里新建一个问文件夹来编译OpenCV: 123cd opencv-4.0.1mkdir releasecd release 然后在新建的这个文件夹里运行（OpenCV4默认不生成.pc文件，OPENCV_GENERATE_PKGCONFIG=ON才会生成。）： 1cmake -DCMAKE_BUILD_TYPE=Release -DOPENCV_GENERATE_PKGCONFIG=ON -DCMAKE_INSTALL_PREFIX=/usr/local .. 编译安装： 12make -j8sudo make install 到此linux下安装OpenCV已经完成。 一个简单测试： 12import cv2print(cv2.__version__) 能输出版本号则说明安装成功。","link":"/blog/2019/09/06/OpenCV/01介绍/一-OpenCV介绍及安装/"},{"title":"(二)OpenCV图像基础操作","text":"图像的许多操作与Numpy的关系比OpenCV关系更加紧密，如果熟练Numpy的话可以写出性能更好的代码。 1.读取显示保存图片 读取图片 123import cv2#第一个参数为图片的相对路径，第二个参数为读取方式（cv2.IMREAD_COLOR、cv2.IMREAD_GRAYSCALE）img = cv2.imread(&apos;image01.jpg&apos;, cv2.IMREAD_COLOR) (需注意的是当图片的路径填写有误，解释不会报错，但会得到一个None对象) 显示图片 123cv2.imshow(&apos;窗口名&apos;, img)cv2.waitkey(0) #等待键盘输入cv2.destroyAllWindows() #关闭窗口 (cv2.waitkey()是一个键盘绑定方法，可以用来确定特殊键是否点击；另外也可以通过matplotlib显示图像，OpenCV加载图片是BGR模式，matplotlib是RGB模式) 保存图片1cv2.imwrite(&apos;messigery,jpg&apos;, img) 2.修改像素或灰度值对于彩色图片一个像素点返回的是BGR像素，灰度图像返回的是灰度值1img[x, y] = 100 #将(x,y)位置像素/灰度值改为100 3.获取图像基本属性123print(img.shape) #获取形状：长、宽、通道数print(img.size) #获取像素数print(img.dtype) #获取数据类型 4.图像ROI有时需要在一幅图像的特殊区域工作。比如在一副图像中检测人眼的位置，应该是先找到人头部的区域，再在其中检测眼睛。这样提高程序的准确性和性能。ROI是通过Numpy的索引获得的，如下图所示，把窗户的一部分拷贝到另一部分：12selectArea = img[280:340,330:390]img[273:333,100:160] = selectArea 5.拆分合并图像通道有时需要对图像的BGR三个通道单独进行操作或者合并成一个图像，实现如下：12b, g, r = cv2.split(img) #拆分cv2.merge(b, g, r) #合并 6.图像扩边1cv2.copyMakeBorder(img, top, bottom, left, right, 边界类型参数) 7.图像上的算术运算 图像加方法 123456x = np.uint8([250])y = np.uint8([10])#opencv加法是饱和操作print cv2.add(x,y) # 250+10 = 260 =&gt; 255#numpy加法是求模操作print x+y # 250+10 = 260 % 256 = 4 图像混合图像混合其实也是图像加法的一种，不过是加上了权重： $dst = \\alpha\\cdot img1+\\beta \\cdot img2+\\gamma$ 12345678import cv2import numpy as npimg1=cv2.imread(&apos;ml.png&apos;)img2=cv2.imread(&apos;opencv_logo.jpg&apos;)dst=cv2.addWeighted(img1,0.7,img2,0.3,0)cv2.imshow(&apos;dst&apos;,dst)cv2.waitKey(0)cv2.destroyAllWindow() 如下所示：","link":"/blog/2019/09/06/OpenCV/02图像/二-OpenCV图像基础操作/"},{"title":"(五)OpenCV图像处理3","text":"本节对OpenCV图像处理中：图像梯度、边缘检测，进行整理介绍 图像梯度当用之前提及过的均值滤波器来降低图像噪声时，会带来图像模糊的副作用。我们当然希望看到的是清晰图像。那么，清晰图像和模糊图像之间的差别在哪里呢？从逻辑上考虑，图像模糊是因为图像中物体的轮廓不明显，轮廓边缘灰度变化不强烈，层次感不强造成的，那么反过来考虑，轮廓边缘灰度变化明显些，层次感强些是不是图像就更清晰些呢。这种灰度变化程度可以用微积分定义，梯度简单来说就是求导，OpenCV提供了三种不同的梯度滤波器，或者说高通滤波器：Sobel，Scharr，Laplacian。 Sobel算子、Scharr算子Sobel 算子是高斯平滑与微分操作的结合体，所以它的抗噪声能力很好。可以设定求导的方向（xorder 或 yorder）。还可以设定使用的卷积核的大小（ksize）。如果 ksize=-1，会使用 3x3 的Scharr 滤波器，它的的效果要比 3x3 的 Sobel 滤波器好（而且速度相同，所以在使用 3x3 滤波器时应该尽量使用 Scharr 滤波器）。 1234img = cv2.imread(&apos;image01.jpg&apos;, 0)# cv2.CV_64F为输出图像的深度；1 0/一阶导数0 1表示对x对y轴方向求sobelx = cv2.Sobel(img, cv2.CV_64F, 1,0,ksize=5)sobely = cv2.Sobel(img, cv2.CV_64F, 0,1,ksize=5) Laplacian拉普拉斯算子拉普拉斯算子可以使用二阶导数的形式定义，可假设其离散实现类似于Sobel导数。事实上，OpenCV在计算拉普拉斯算子时直接调用Sobel算子。计算公式如下： \\Delta src=\\frac {\\partial ^{2}src}{\\partial x^2} + \\frac {\\partial ^{2}src}{\\partial y^2}12#cv2.CV_64F是输出图像的深度，可以使用-1与原图像保持一致laplacian = cv2.laplacian(img, cv2.CV_64F) Canny边缘检测 原理 噪声去除由于边缘检测很容易受到噪声影响，所以第一步是使用高斯滤波器去燥。 计算图像梯度对平滑后的图像使用Sobel算子计算水平方向和竖直方向的一阶导数。再根据这两幅梯度图找到边界的梯度和方向，公式如下： Edge\\_Gradient(G) = \\sqrt{G^2_x+G^2_y}； Angle(\\Theta) = tan^{-1}(\\frac{G_x}{G_y}) 梯度的方向一般总是与边界垂直，梯度方向被归为四类：垂直、水平、两个对角线。 非极大值抑制通过上一步获得梯度的方向和大小之后，应该对整幅图像做一个扫描，去除非边界上的点（对每一个像素进行检查，看这个点的梯度是不是周围具有相同梯度方向的点中最大的）。 滞后阀值现在要确定哪些边界才是真正的边界，这时我们需要设置两个阀值：minVal和maxVal。当图像的灰度梯度高于maxVal被认为是真的边界，低于minVal的边界会被抛弃，如果介于两者之间的话，就要看这个点是否与某个被确定为真正的边界点相连，如果是就认为它也是边界点。如下图： A 高于阈值 maxVal 所以是真正的边界点，C 虽然低于 maxVal 但高于minVal 并且与 A 相连，所以也被认为是真正的边界点。而 B 就会被抛弃，因为他不仅低于 maxVal 而且不与真正的边界点相连。所以选择合适的 maxVal和 minVal 对于能否得到好的结果非常重要。在这一步一些小的噪声点也会被除去，因为我们假设边界都是一些长的线段。 OpenCV中的Canny边界检测1234567891011import cv2import numpy as npfrom matplotlib import pyplot as pltimg = cv2.imread(&apos;image01.jpg&apos;,0)#原始图像，minVal，maxValedges = cv2.Canny(img,100,200)plt.subplot(121),plt.imshow(img,cmap = &apos;gray&apos;)plt.title(&apos;Original Image&apos;), plt.xticks([]), plt.yticks([])plt.subplot(122),plt.imshow(edges,cmap = &apos;gray&apos;)plt.title(&apos;Edge Image&apos;), plt.xticks([]), plt.yticks([])plt.show() 如下图所示：","link":"/blog/2019/09/10/OpenCV/02图像/五-OpenCV图像处理3/"},{"title":"(四)OpenCV图像处理2","text":"本节对OpenCV图像处理中：图像阀值、图像模糊、图像的形态学转换，进行整理介绍 图像阀值 [ ] 简单阀值 [ ] 自适应阀值 [ ] Qtsu‘s二值化 简单阀值针对于灰度图片的灰度值，当灰度值高于阀值时，给这个灰度值赋一个新值： 12345#第一个参数是原始图像#第二个参数是阀值#第三个参数是低于或高于阀值时被置于的新值#第四个参数是阀值规则retVal,thresh = cv2.threshold(img, 127, 255, cv2.THRESH_TOZERO) 阀值规则对应如下： 自适应阀值与上面简单阀值类似，不过简单阀值是整幅图像采用一个数作为阀值，这种方法并不是适应于所有情况，尤其是当一幅图像上的不同部分具有不同亮度时，可以同一幅图像的不同区域采用不同的阀值。 12345#第二个是超过或小于阀值被重置的灰度值#第三个参数是指定阀值计算方法#11是指邻域大小#2是一个常数，阀值就等于平均值或者加权平均值减去这个常数thresh = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2) Otsu‘s 二值化在简单阀值时是随便给了一个数来做阀值，那么怎么知道选取的好坏呢？答案就是不停的尝试。如果是一幅双峰图像，就应该根据其直方图计算出一个阀值。 1ret2,th2 = cv2.threshold(img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTS) 图像平滑 [ ] 学习使用不同的低通滤波器对图像进行模糊 [ ] 使用自定义的滤波器对图像进行卷积（2D卷积） 2D卷积与信号一样，可以对2D图像实施低通滤波(LPF)，高通滤波(HPF)等。LPF帮助去除噪声、模糊图像；HPF帮助找得到图像的边缘。 12kernel = np.ones((5,5),np.float32)/25dst = cv2.filter2D(img,-1,kernel) 效果如下： 图像模糊使用低通滤波器可以达到图像模糊的目的，这对去除噪音很有帮助，当然现在更多的是通过生成对抗网络来对图像进行增强等。OpenCV提供了四种模糊计算。 平均使用卷积框覆盖区域所有像素的平均值来替代中心元素 12#使用归一化卷积框blur = cv2.blur(img,(5,5)) 高斯模糊一个卷积框里的值是符合高斯分布的，方框中心的值最大，其余根据距离中心元素的距离递减 12#卷积核的大小必须是一个奇数blur = cv2.GaussianBlur(img, (5,5), 标准差) 中值模糊用卷积框对应像素点的中值来替代中心像素的值，这个滤波器经常用来去除椒盐噪声。 1median = cv2.medianBlur(img, 5) 双边滤波高斯滤波器是求中心点邻近区域像素的高斯加权平均值，这种只考虑像素之间的空间关系，而不会考虑像素值之间的关系（像素的相似度）。所以这种方法不会考虑一个像素是否位于边界，边界也会模糊掉。 双边滤波同时使用空间高斯权重和灰度值相似性高斯权重。空间高斯函数确保只有邻近区域的像素对中心点有影响，灰度值相似性高斯函数确保只有与中心像素灰度值相近的才会被用来做模糊运算。所以这种方法会确保边界不会被模糊掉，因为边界处的灰度值变化比较大。 12#9表示邻域直径，两个75分别是空间高斯函数标准差、灰度值相似性高斯函数标准差 blur = cv2.bilateralFilter(img, 9, 75, 75) 形态学转换 [ ] 腐蚀、膨胀、开运算、闭运算等形态学操作是根据图像形状进行的简单操作。一般情况下对二值化图像进行的操作。需要输入两个参数，一个是原始图像，第二个被称为结构化元素或核，它是用来决定操作的性质的。 1.腐蚀123img = cv2.imread(&apos;j.png&apos;,0)kernel = np.ones((5,5),np.uint8)erosion = cv2.erode(img,kernel,iterations = 1) 2. **膨胀** 1dilation = cv2.dilate(img,kernel,iterations = 1) 3. **开运算** 先腐蚀后膨胀(常用来去除噪声) 1opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel) 4. **闭运算** 先膨胀后腐蚀（常被用来填充前景物体中的小洞） 1closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel) 5. **形态学梯度** 1gradient = cv2.morphologyEx(img, cv2.MORPH_GRADIENT, kernel)","link":"/blog/2019/09/09/OpenCV/02图像/四-OpenCV图像处理2/"},{"title":"(六)TensorFlow_Python调用API","text":"针对于常用的TensorFlow方法进行一个简要的汇总介绍。 1. 变量转换 构造 tf.string_to_number(string_tensor, out_type=None, name=None) tf.to_double(x, name='ToDouble') tf.to_float(x, name='ToFloat') tf.to_bfloat16(x, name='ToBFloat16') tf.to_int32(x, name='ToInt32') tf.to_int64(x, name='ToInt64') tf.cast(x, dtype, name=None) 形状与重塑 tf.shape(input, name=None) tf.size(input, name=None) tf.rank(input, name=None) tf.reshape(tensor, shape, name=None) tf.squeeze(input, squeeze_dims=None, name=None) tf.expand_dims(input, dim, name=None) 切片与插入 tf.slice(input_, begin, size, name=None) tf.split(split_dim, num_split, value, name='split') tf.tile(input, multiples, name=None) tf.pad(input, paddings, name=None) tf.concat(concat_dim, values, name='concat') tf.pack(values, name='pack') tf.unpack(value, num=None, name='unpack') tf.reverse_sequence(input, seq_lengths, seq_dim, name=None) tf.reverse(tensor, dims, name=None) tf.transpose(a, perm=None, name='transpose') tf.gather(params, indices, name=None) tf.dynamic_partition(data, partitions, num_partitions, name=None) tf.dynamic_stitch(indices, data, name=None) 2.构建图 核心图数据结构 class tf.Graph class tf.Operation class tf.Tensor 张量类型 class tf.DType tf.as_dtype(type_value) 公用函数 tf.device(dev) tf.name_scope(name) tf.control_dependencies(control_inputs) tf.convert_to_tensor(value, dtype=None, name=None) tf.get_default_graph() tf.import_graph_def(graph_def, input_map=None, return_elements=None, name=None, op_dict=None) Graph 集合 tf.add_to_collection(name, value) tf.get_collection(key, scope=None) class tf.GraphKeys 定义新的操作 class tf.RegisterGradient tf.NoGradient(op_type) class tf.RegisterShape class tf.TensorShape class tf.Dimension tf.op_scope(values, name, default_name) tf.get_seed(op_seed) 3. 运行图 Session 管理 class tf.Session class tf.InteractiveSession tf.get_default_session() 错误分类 class tf.OpError class tf.errors.CancelledError class tf.errors.UnknownError class tf.errors.InvalidArgumentError class tf.errors.DeadlineExceededError class tf.errors.NotFoundError class tf.errors.AlreadyExistsError class tf.errors.PermissionDeniedError class tf.errors.UnauthenticatedError class tf.errors.ResourceExhaustedError class tf.errors.FailedPreconditionError class tf.errors.AbortedError class tf.errors.OutOfRangeError class tf.errors.UnimplementedError class tf.errors.InternalError class tf.errors.UnavailableError class tf.errors.DataLossError 4. 变量，序列以及随机数 常量张量 tf.zeros(shape, dtype=tf.float32, name=None) tf.zeros_like(tensor, dtype=None, name=None) tf.ones(shape, dtype=tf.float32, name=None) tf.ones_like(tensor, dtype=None, name=None) tf.fill(dims, value, name=None) tf.constant(value, dtype=None, shape=None, name='Const') 序列 tf.linspace(start, stop, num, name=None) tf.range(start, limit, delta=1, name='range') 随机张量 例子: tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) tf.random_uniform(shape, minval=0.0, maxval=1.0, dtype=tf.float32, seed=None, name=None) tf.random_shuffle(value, seed=None, name=None) tf.set_random_seed(seed) 5.控制流 控制流 tf.identity(input, name=None) tf.tuple(tensors, name=None, control_inputs=None) tf.group(*inputs, **kwargs) tf.no_op(name=None) tf.count_up_to(ref, limit, name=None) 逻辑运算符 tf.logical_and(x, y, name=None) tf.logical_not(x, name=None) tf.logical_or(x, y, name=None) tf.logical_xor(x, y, name='LogicalXor') 比较操作 tf.equal(x, y, name=None) tf.not_equal(x, y, name=None) tf.less(x, y, name=None) tf.less_equal(x, y, name=None) tf.greater(x, y, name=None) tf.greater_equal(x, y, name=None) tf.select(condition, t, e, name=None) tf.where(input, name=None) 调试操作 tf.is_finite(x, name=None) tf.is_inf(x, name=None) tf.is_nan(x, name=None) tf.verify_tensor_all_finite(t, msg, name=None) tf.check_numerics(tensor, message, name=None) tf.add_check_numerics_ops() tf.Assert(condition, data, summarize=None, name=None) tf.Print(input_, data, message=None, first_n=None, summarize=None, name=None) 6.图像 编码解码 tf.image.decode_jpeg(contents, channels=None, ratio=None, fancy_upscaling=None, try_recover_truncated=None, acceptable_fraction=None, name=None) tf.image.encode_jpeg(image, format=None, quality=None, progressive=None, optimize_size=None, chroma_downsampling=None, density_unit=None, x_density=None, y_density=None, xmp_metadata=None, name=None) tf.image.decode_png(contents, channels=None, name=None) tf.image.encode_png(image, compression=None, name=None) 调整 tf.image.resize_images(images, new_height, new_width, method=0) tf.image.resize_area(images, size, name=None) tf.image.resize_bicubic(images, size, name=None) tf.image.resize_bilinear(images, size, name=None) tf.image.resize_nearest_neighbor(images, size, name=None) 剪裁 tf.image.resize_image_with_crop_or_pad(image, target_height, target_width) tf.image.pad_to_bounding_box(image, offset_height, offset_width, target_height, target_width) tf.image.crop_to_bounding_box(image, offset_height, offset_width, target_height, target_width) tf.image.random_crop(image, size, seed=None, name=None) tf.image.extract_glimpse(input, size, offsets, centered=None, normalized=None, uniform_noise=None, name=None) 翻转和置换 tf.image.flip_up_down(image) tf.image.random_flip_up_down(image, seed=None) tf.image.flip_left_right(image) tf.image.random_flip_left_right(image, seed=None) tf.image.transpose_image(image) 图像调整 tf.image.adjust_brightness(image, delta, min_value=None, max_value=None) tf.image.random_brightness(image, max_delta, seed=None) tf.image.adjust_contrast(images, contrast_factor, min_value=None, max_value=None) tf.image.random_contrast(image, lower, upper, seed=None) tf.image.per_image_whitening(image) 7.输入与读取 占位符 tf.placeholder(dtype, shape=None, name=None) 读取 class tf.ReaderBase class tf.TextLineReader class tf.WholeFileReader class tf.IdentityReader class tf.TFRecordReader class tf.FixedLengthRecordReader 转换 tf.decode_csv(records, record_defaults, field_delim=None, name=None) tf.decode_raw(bytes, out_type, little_endian=None, name=None) 示例序列缓冲区 tf.parse_example(serialized, names=None, sparse_keys=None, sparse_types=None, dense_keys=None, dense_types=None, dense_defaults=None, dense_shapes=None, name='ParseExample') tf.parse_single_example(serialized, names=None, sparse_keys=None, sparse_types=None, dense_keys=None, dense_types=None, dense_defaults=None, dense_shapes=None, name='ParseSingleExample') 队列 class tf.QueueBase class tf.FIFOQueue class tf.RandomShuffleQueue 文件系统处理 tf.matching_files(pattern, name=None) tf.read_file(filename, name=None) 输入管道 输入管道的开始 tf.train.match_filenames_once(pattern, name=None) tf.train.limit_epochs(tensor, num_epochs=None, name=None) tf.train.range_input_producer(limit, num_epochs=None, shuffle=True, seed=None, capacity=32, name=None) tf.train.slice_input_producer(tensor_list, num_epochs=None, shuffle=True, seed=None, capacity=32, name=None) tf.train.string_input_producer(string_tensor, num_epochs=None, shuffle=True, seed=None, capacity=32, name=None) Batching at the end of an input pipeline tf.train.batch(tensor_list, batch_size, num_threads=1, capacity=32, enqueue_many=False, shapes=None, name=None) tf.train.batch_join(tensor_list_list, batch_size, capacity=32, enqueue_many=False, shapes=None, name=None) tf.train.shuffle_batch(tensor_list, batch_size, capacity, min_after_dequeue, num_threads=1, seed=None, enqueue_many=False, shapes=None, name=None) tf.train.shuffle_batch_join(tensor_list_list, batch_size, capacity, min_after_dequeue, seed=None, enqueue_many=False, shapes=None, name=None) 8.数学相关 算术运算符 tf.add(x, y, name=None) tf.sub(x, y, name=None) tf.mul(x, y, name=None) tf.div(x, y, name=None) tf.mod(x, y, name=None) 基本属性函数 tf.add_n(inputs, name=None) tf.abs(x, name=None) tf.neg(x, name=None) tf.sign(x, name=None) tf.inv(x, name=None) tf.square(x, name=None) tf.round(x, name=None) tf.sqrt(x, name=None) tf.rsqrt(x, name=None) tf.pow(x, y, name=None) tf.exp(x, name=None) tf.log(x, name=None) tf.ceil(x, name=None) tf.floor(x, name=None) tf.maximum(x, y, name=None) tf.minimum(x, y, name=None) tf.cos(x, name=None) tf.sin(x, name=None) 矩阵数学函数 tf.diag(diagonal, name=None) tf.transpose(a, perm=None, name='transpose') tf.matmul(a, b, transpose_a=False, transpose_b=False, a_is_sparse=False, b_is_sparse=False, name=None) tf.batch_matmul(x, y, adj_x=None, adj_y=None, name=None) tf.matrix_determinant(input, name=None) tf.batch_matrix_determinant(input, name=None) tf.matrix_inverse(input, name=None) tf.batch_matrix_inverse(input, name=None) tf.cholesky(input, name=None) tf.batch_cholesky(input, name=None) 复数函数 tf.complex(real, imag, name=None) tf.complex_abs(x, name=None) tf.conj(in_, name=None) tf.imag(in_, name=None) tf.real(in_, name=None) 减值 tf.reduce_sum(input_tensor, reduction_indices=None, keep_dims=False, name=None) tf.reduce_prod(input_tensor, reduction_indices=None, keep_dims=False, name=None) tf.reduce_min(input_tensor, reduction_indices=None, keep_dims=False, name=None) tf.reduce_max(input_tensor, reduction_indices=None, keep_dims=False, name=None) tf.reduce_mean(input_tensor, reduction_indices=None, keep_dims=False, name=None) tf.reduce_all(input_tensor, reduction_indices=None, keep_dims=False, name=None) tf.reduce_any(input_tensor, reduction_indices=None, keep_dims=False, name=None) tf.accumulate_n(inputs, shape=None, tensor_dtype=None, name=None) 分割 tf.segment_sum(data, segment_ids, name=None) tf.segment_prod(data, segment_ids, name=None) tf.segment_min(data, segment_ids, name=None) tf.segment_max(data, segment_ids, name=None) tf.segment_mean(data, segment_ids, name=None) tf.unsorted_segment_sum(data, segment_ids, num_segments, name=None) tf.sparse_segment_sum(data, indices, segment_ids, name=None) tf.sparse_segment_mean(data, indices, segment_ids, name=None) 序列中的比较和索引 tf.argmin(input, dimension, name=None) tf.argmax(input, dimension, name=None) tf.listdiff(x, y, name=None) tf.where(input, name=None) tf.unique(x, name=None) tf.edit_distance(hypothesis, truth, normalize=True, name='edit_distance') tf.invert_permutation(x, name=None) 9.神经网络相关 激活函数 tf.nn.relu(features, name=None) tf.nn.relu6(features, name=None) tf.nn.softplus(features, name=None) tf.nn.dropout(x, keep_prob, noise_shape=None, seed=None, name=None) tf.nn.bias_add(value, bias, name=None) tf.sigmoid(x, name=None) tf.tanh(x, name=None) 卷积 tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, name=None) tf.nn.depthwise_conv2d(input, filter, strides, padding, name=None) tf.nn.separable_conv2d(input, depthwise_filter, pointwise_filter, strides, padding, name=None) 池化 tf.nn.avg_pool(value, ksize, strides, padding, name=None) tf.nn.max_pool(value, ksize, strides, padding, name=None) tf.nn.max_pool_with_argmax(input, ksize, strides, padding, Targmax=None, name=None) 归一化 tf.nn.l2_normalize(x, dim, epsilon=1e-12, name=None) tf.nn.local_response_normalization(input, depth_radius=None, bias=None, alpha=None, beta=None, name=None) tf.nn.moments(x, axes, name=None) 损失函数 tf.nn.l2_loss(t, name=None) 分类 tf.nn.sigmoid_cross_entropy_with_logits(logits, targets, name=None) tf.nn.softmax(logits, name=None) tf.nn.softmax_cross_entropy_with_logits(logits, labels, name=None) 嵌入张量查找值 tf.nn.embedding_lookup(params, ids, name=None) 评估 tf.nn.top_k(input, k, name=None) tf.nn.in_top_k(predictions, targets, k, name=None) 候选采样 采样损失函数 tf.nn.nce_loss(weights, biases, inputs, labels, num_sampled, num_classes, num_true=1, sampled_values=None, remove_accidental_hits=False, name='nce_loss') tf.nn.sampled_softmax_loss(weights, biases, inputs, labels, num_sampled, num_classes, num_true=1, sampled_values=None, remove_accidental_hits=True, name='sampled_softmax_loss') 候选取样器 tf.nn.uniform_candidate_sampler(true_classes, num_true, num_sampled, unique, range_max, seed=None, name=None) tf.nn.log_uniform_candidate_sampler(true_classes, num_true, num_sampled, unique, range_max, seed=None, name=None) tf.nn.learned_unigram_candidate_sampler(true_classes, num_true, num_sampled, unique, range_max, seed=None, name=None) tf.nn.fixed_unigram_candidate_sampler(true_classes, num_true, num_sampled, unique, range_max, vocab_file='', distortion=0.0, num_reserved_ids=0, num_shards=1, shard=0, unigrams=[], seed=None, name=None) 其它候选抽样工具 tf.nn.compute_accidental_hits(true_classes, sampled_candidates, num_true, seed=None, name=None) 10.稀疏张量 稀疏张量表示 class tf.SparseTensor class tf.SparseTensorValue 稀疏到稠密变换 tf.sparse_to_dense(sparse_indices, output_shape, sparse_values, default_value, name=None) tf.sparse_tensor_to_dense(sp_input, default_value, name=None) tf.sparse_to_indicator(sp_input, vocab_size, name=None) 操作 tf.sparse_concat(concat_dim, sp_inputs, name=None) tf.sparse_reorder(sp_input, name=None) tf.sparse_retain(sp_input, to_retain) tf.sparse_fill_empty_rows(sp_input, default_value, name=None) 11.变量 Variables class tf.Variable Variable helper functions tf.all_variables() tf.trainable_variables() tf.initialize_all_variables() tf.initialize_variables(var_list, name='init') tf.assert_variables_initialized(var_list=None) Saving and Restoring Variables class tf.train.Saver tf.train.latest_checkpoint(checkpoint_dir, latest_filename=None) tf.train.get_checkpoint_state(checkpoint_dir, latest_filename=None) tf.train.update_checkpoint_state(save_dir, model_checkpoint_path, all_model_checkpoint_paths=None, latest_filename=None) Sharing Variables tf.get_variable(name, shape=None, dtype=tf.float32, initializer=None, trainable=True, collections=None) tf.get_variable_scope() tf.variable_scope(name_or_scope, reuse=None, initializer=None) tf.constant_initializer(value=0.0) tf.random_normal_initializer(mean=0.0, stddev=1.0, seed=None) tf.truncated_normal_initializer(mean=0.0, stddev=1.0, seed=None) tf.random_uniform_initializer(minval=0.0, maxval=1.0, seed=None) tf.uniform_unit_scaling_initializer(factor=1.0, seed=None) tf.zeros_initializer(shape, dtype=tf.float32) Sparse Variable Updates tf.scatter_update(ref, indices, updates, use_locking=None, name=None) tf.scatter_add(ref, indices, updates, use_locking=None, name=None) tf.scatter_sub(ref, indices, updates, use_locking=None, name=None) tf.sparse_mask(a, mask_indices, name=None) class tf.IndexedSlices 12.训练 Optimizers class tf.train.Optimizer Usage Processing gradients before applying them. Gating Gradients Slots class tf.train.GradientDescentOptimizer class tf.train.AdagradOptimizer class tf.train.MomentumOptimizer class tf.train.AdamOptimizer class tf.train.FtrlOptimizer class tf.train.RMSPropOptimizer Gradient Computation tf.gradients(ys, xs, grad_ys=None, name='gradients', colocate_gradients_with_ops=False, gate_gradients=False, aggregation_method=None) class tf.AggregationMethod tf.stop_gradient(input, name=None) Gradient Clipping tf.clip_by_value(t, clip_value_min, clip_value_max, name=None) tf.clip_by_norm(t, clip_norm, name=None) tf.clip_by_average_norm(t, clip_norm, name=None) tf.clip_by_global_norm(t_list, clip_norm, use_norm=None, name=None) tf.global_norm(t_list, name=None) Decaying the learning rate tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate, staircase=False, name=None) Moving Averages class tf.train.ExponentialMovingAverage Coordinator and QueueRunner class tf.train.Coordinator class tf.train.QueueRunner tf.train.add_queue_runner(qr, collection='queue_runners') tf.train.start_queue_runners(sess=None, coord=None, daemon=True, start=True, collection='queue_runners') Summary Operations tf.scalar_summary(tags, values, collections=None, name=None) tf.image_summary(tag, tensor, max_images=None, collections=None, name=None) tf.histogram_summary(tag, values, collections=None, name=None) tf.nn.zero_fraction(value, name=None) tf.merge_summary(inputs, collections=None, name=None) tf.merge_all_summaries(key='summaries') Adding Summaries to Event Files class tf.train.SummaryWriter tf.train.summary_iterator(path) Training utilities tf.train.global_step(sess, global_step_tensor) tf.train.write_graph(graph_def, logdir, name, as_text=True)","link":"/blog/2019/10/08/TensorFlow/六-TensorFlow_Python调用API/"}],"tags":[{"name":"C++_STL","slug":"C-STL","link":"/blog/tags/C-STL/"},{"name":"TensorFlow","slug":"TensorFlow","link":"/blog/tags/TensorFlow/"},{"name":"TensorBoard","slug":"TensorBoard","link":"/blog/tags/TensorBoard/"},{"name":"LinearRegression","slug":"LinearRegression","link":"/blog/tags/LinearRegression/"},{"name":"WEB","slug":"WEB","link":"/blog/tags/WEB/"},{"name":"Pycharm","slug":"Pycharm","link":"/blog/tags/Pycharm/"},{"name":"Python","slug":"Python","link":"/blog/tags/Python/"},{"name":"设计模式","slug":"设计模式","link":"/blog/tags/设计模式/"},{"name":"hexo搭建","slug":"hexo搭建","link":"/blog/tags/hexo搭建/"},{"name":"Markdown","slug":"Markdown","link":"/blog/tags/Markdown/"},{"name":"OpenCV","slug":"OpenCV","link":"/blog/tags/OpenCV/"}],"categories":[{"name":"C/C++","slug":"C-C","link":"/blog/categories/C-C/"},{"name":"TensorFlow","slug":"TensorFlow","link":"/blog/categories/TensorFlow/"},{"name":"WEB","slug":"WEB","link":"/blog/categories/WEB/"},{"name":"Python","slug":"Python","link":"/blog/categories/Python/"},{"name":"设计模式","slug":"设计模式","link":"/blog/categories/设计模式/"},{"name":"随笔","slug":"随笔","link":"/blog/categories/随笔/"},{"name":"OpenCV","slug":"OpenCV","link":"/blog/categories/OpenCV/"}]}