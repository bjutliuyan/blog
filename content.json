{"pages":[],"posts":[{"title":"C+中STL用法详解二_常用容器用法介绍","text":"熟悉容器的使用是掌握STL的基本步骤。许多容器都支持empty()、size()、front()、back()、begin()、end()等方法，这是因为算法进行了封装。 1. 序列式容器序列容器以线性序列的方式存储元素。元素在序列中的顺序与存储的顺序相同，没有对元素进行排序。 vector的基本用法长度可变序列，但只能在序列的末尾高效地增加或删除元素。 12345678910111213141516vector&lt;类型&gt;T; //构建T.push_back(x); //尾部新增一个元素xT.insert(iterator it, int x); //在it指向的元素前新增一个xT.insert(iterator it, int n, int x); //在it指向元素前插入n个xT.erase(iterator it); //删除it指向的元素T.erase(iterator first, iterator last); //删除[last,first)中元素T.pop_back(); //删除最后一个元素T.empty(); //判断是否为空T.size(); //返回元素个数T.at(index); //返回index编号元素reverse(T.begin(), T.end()); //反转T中元素T.begin(); //返回首元素指针T.end(); //返回最后一个元素+1的指针T.front(); //返回首元素引用T.back(); //返回尾元素引用sort(T.begin(), T.end()); //排序 deque的基本用法以双端队列形式组织元素，在尾部和头部插入删除迅速，在中间插入删除元素则比较费时，因为必须移动中间其它元素。 12345678910111213deque&lt;类型&gt;T； //声明一个双端队列deque&lt;类型&gt;T(size,value); //声明具有size个value默认值的双端队列T.push_front(x); //在头部插入xT.push_back(x); //在尾部插入xT.pop_front(); //弹出头部的一个元素T.pop_back(); //弹出尾部的一个元素T.front(); //返回第一个元素的引用T.back(); //返回最后一个元素的引用T.begin();T.end();T.size();T.at(index);T.insert(it, x); list的基本用法list实现的是双向链表，与vector比它允许快速的插入和删除，但随机访问比较慢。 12345678910111213list&lt;类型&gt;T; // 构建T.push_back(x);T.push_front(x);T.pop_back();T.pop_front();T.front();T.back();T.begin();T.reverse();T.sort();T.end();T.insert(x);T.remove(x); 2. 关联式容器 map的基本用法map是关联容器的一种，通过键值对来存储。map内部自建一颗红黑树(平衡二叉树)，这棵树具有对数据的自动排序功能（按照键值排序，当键值是不可比较元素时需指定比较仿函数）。map容器有4中，每一种都由类模板定义，因为修改键会扰乱容器中元素的顺序，每种map容器模板都有不同的特性： map：唯一&lt;key,value&gt;对应，默认按键值从小到大排序。 mutlimap：允许使用重复的键。 unordered_map：元素的顺序并不是直接由键值确定的，而是由键值的哈希值决定的。 unordered_multimap：通过哈希值确定对象的位置，允许有重复的键。 123456789map&lt; int,string &gt; T; //构建T.insert(make_pair&lt;int,string&gt;(1, \"student_one\")); //插入T.empty();T.size();T.erase(key); //根据key删除，返回删除元素的数量T.erase(iterator it); //删除迭代器指向位置的键值对，并返回指向下一个元素的迭代器T.at(key); // 根据key获取对应value值T.find(key); //返回指向键值key的迭代器指针T.count(key); 2.mutlimap的基本用法mutlimap容器保存的是有序的键值对，键值可以重复。 1//操作与map类似 3.set的基本用法set搜索、移除、插入都是对数复杂度，以红黑树实现。set内的元素会被自动排序，set中的元素即是键值又是实值，set不允许两个元素有相同的值。不能通过set的迭代器去修改set元素，当对容器中的元素进行插入或删除时，操作之前的所有迭代器在操作之后依然有效。定义set容器的模板如下： set： 容器保存 T 类型的对象，而且保存的对象是唯一的。其中保存的元素是有序的，默认用 less 对象比较。 multiset：容器保存 T 类型对象的方式相同，但它可以保存重复的对象。 unordered_set：容器保存 T 类型的对象，而且对象是唯一的。元素在容器中的位置由元素的哈希值决定。 unordered_mutliset：容器保存 T 类型对象的方式和 unorderd_set 相同，但它可以保存重复的对象。 12345set&lt;类型&gt;T; //构建T.insert(x);T.count(x);T.find(x);T.erase(x); 4.mutliset的基本用法 1//操作与set类似 3. 容器适配器 stack的基本用法实现类似于栈的功能（后进先出）： 123456stack &lt;类型&gt; T; //构建T.empty();T.push(item);T.pop();x = T.top();int size = T.size(); queue的基本用法实现类似于队列的功能（先进先出）： 1234567queue &lt;类型&gt; T; //构建T.push(item); //push到队列尾部T.pop(); //弹出队首x = T.front();y = T.back();T.size();T.empty(); prority_queue的基本用法priority_queue 容器适配器定义了一个元素有序排列的队列。默认队列头部的元素优先级最高。它和queue不同的是我们可以自定义其中数据的优先级，让优先级高的排在队列前面，优先出队。但是如何定义“优先级”完全取决于我们自己。如果一个优先级队列记录的是医院里等待接受急救的病人，那么病人病情的严重性就是优先级。如果队列元素是银行的借贷业务，那么借记可能会优先于信贷。 12345678prority_queue &lt;类型&gt; T; //构建priority_queue &lt;int,vector&lt;int&gt;,greater&lt;int&gt; &gt; q; //升序序列priority_queue &lt;int,vector&lt;int&gt;,less&lt;int&gt; &gt; q; //降序序列T.push(x); //按优先级push到合适位置T.pop();T.top();T.size();T.empty();","link":"/blog/2019/10/13/C++/C++中STL用法详解二-常用容器用法介绍/"},{"title":"C+中STL用法详解一_内容介绍","text":"STL即标准模板库，其内封装了诸多的基本数据结构和算法。 1. 什么是STL STL是C++标准程序库的核心，其中包含了很多常用的基本数据结构和算法，提供了一个可扩展的应用框架； STL的一个重要特点是数据结构和算法的分离这使得STL变得很通用，比如sort()方法是完全通用的，可以用它来操作任何数据集合，包括链表、容器、数组 STL是基于template模板实现的，而不是面向对象 2. STL内容介绍STL中主要有六大组件：1. 容器(Container)用来存储并管理某类对象的集合，以模板类的方法提供。每一种容器都有其优缺点，为了访问容器中的数据，可以使用由容器类输出的迭代器。2. 迭代器(Iterator)提供访问容器中对象的方法。例如可以使用一对迭代器指定list或vector中一定范围的对象。迭代器可划分为 5 种类属，这 5 种类属归属两种类型：双向迭代器和随机存取迭代器3. 算法(Algorithm)用来操作容器中数据结构的模板函数。例如STL用sort()来对容器中数据排序，用find()在容器中查找元素。函数本身与它们操作的数据结构和类型无关，因此可以在简单数组和任何高复杂容器中使用4. 仿函数(Functor)5. 适配器(Adaptor)6. 分配器(Allocator) 2.1 容器STL中容器有序列式容器和关联容器，容器适配器(stack、queue、priority_queue)，位集(bit_set)，串包(string_package)等。（1）序列式容器每个元素都有固定位置——取决于插入时机和下标，和元素值无关（vector、deque、list）（2）关联式容器，元素位置取决于特定的排序准则，和插入顺序无关（set/multiset、map/mutlimap）目前STL中已经提供的主要容器如下： vector &lt;T&gt;：一种向量 list &lt;T&gt;：一个双向链表容器，完成了标准C++数据结构中链表的所有功能 deque &lt;T&gt;：双端队列容器 set &lt;T&gt;：一种集合容器，元素不能重复 multiset &lt;T&gt;：一种允许出现重复元素的集合容器 map &lt;key,value&gt;：关键值对的容器，key唯一 multimap &lt;key,value&gt;：一种允许出现重复key值的关联式容器 stack &lt;T&gt;：一种栈容器 queue &lt;T&gt; ：一种队列容器 prority_queue &lt;T&gt;：优先队列容器 2.2 STL迭代器 作用: Iterator(迭代器)模式又称Cursor模式，可以使得我们在不知道对象内部表示情况下，按一定顺序访问聚合对象中的元素； 能让容器与算法不干扰的相互发展，最后又能无间隙的粘合起来，容器提供迭代器，算法使用迭代器访问容器中的数据； 重载了*、++、==、!=、=运算符。 2.3 算法STL提供了大约100个实现算法的模板函数，只需要调用一下就可以很大程度上简化代码。算法部分主要由头文件&lt;algorithm&gt;,&lt;numeric&gt;,&lt;fuctional&gt;组成，STL中的算法大致分为四类： 非可变序列算法：不直接修改所操作容器内容的算法 可变序列算法：可以修改所操作容器内容的算法 排序算法：对序列进行排序、合并、搜索、集合操作等 数值算法：对容器内容进行数值计算常见的部分算法如下： for_each()：用指定函数依次对指定范围内所有元素进行迭代访问，返回所指定的函数类型。 find()： 利用底层元素的等于操作符，对指定范围内的元素与输入值进行比较。当匹配时，结束搜索，返回该元素的一个InputIterator。 find_if()： 使用输入的函数代替等于操作符执行find。 count()：利用等于操作符，把标志范围内的元素与输入值比较，返回相等元素个数。 count_if()： 利用输入的操作符，对标志范围内的元素进行操作，返回结果为true的个数。 replace()： 将指定范围内所有等于vold的元素都用vnew代替。 replace_if()：将指定范围内所有操作结果为true的元素用新值代替。 sort()：以升序重新排列指定范围内的元素。重载版本使用自定义的比较操作。 merge()：合并两个有序序列，存放到另一个序列。重载版本使用自定义的比较。 reverse()：将指定范围内元素重新反序排序。 unique()：清除序列中重复元素。 remove()：删除指定范围内所有等于指定元素的元素。 equal()：如果两个序列在标志范围内元素都相等，返回true。 set_union()：返回两个序列的并集。 set_intersection()：返回两个序列的交集。 set_difference()：返回两个序列的差集。 set_symmetric_difference()：返回两个序列的对称差集。 2.4 仿函数C++中通过一个类或结构体中重载括号运算符的方法使用一个函数对象而不是一个普通函数，要使使用仿函数比一般函数优点参考以下链接link 1234567struct cmp{ bool operator()(ListNode* node1, ListNode* node2){ return node1-&gt;val &gt; node2-&gt;val; }};sort(a, a+n, cmp); //按照cmp的规则迭代排序 用STL内建的仿函数，必须包含头文件。该头文件包含的仿函数分类包括： 算术类仿函数加： plus 减： minus 乘： multiplies 除： dividex 模取： modulus 否定： negate 关系运算类仿函数等于： equal_to 不等于： not_equal_to 大于： greater 大于等于：greater_equal 小于： less 小于等于：less_equal 逻辑运算仿函数逻辑与： logical_and 逻辑或： logical_or 逻辑否： logical_no 除了使用STL内建的仿函数，还可以使用自定义的仿函数。 2.5 容器适配器容器适配器(配接器)对容器进行包装使其表现出另外一种行为。例如stack&lt;int, vector&gt;实现了栈的功能，但其内部使用顺序容器vector来存储数据（相当于是vector表现出了栈的行为） 种类 默认顺序容器 可用顺序容器 说明 stack deque vector、list、deque #include “stack” queue deque list、deque #include “queue”；基础容器必须提供push_front()运算 priority_queue vector vector、deque #include “queue”；基础容器必须提供随机访问功能 定义适配器 初始化 1stack&lt;int&gt;T; 覆盖默认容器类型 1stack&lt;int, vector&lt;int&gt; &gt;T; (需注意两个“&lt;”之间要留一个空格，以免被编译器当作输出流符号) 3. 总结STL作为C++通用库，主要由容器、迭代器、算法、仿函数、容器适配器、内存配置器六大部分组成。使用 STL 最重要的是掌握基本理论和编程方法，了解 STL 编程技术，必须深刻掌握 STL 容器技术和 STL 迭代器技术。STL 提供了一组表示容器、迭代器、仿函数和算法的模板： 容器是类似数组的单元，可存储 若干个值，且STL容器是同质的，即存储的值类型相同； 算法是完成特定任务的处方； 迭代器能够用来遍历容器的对象，与能够遍历数组的指针类似，是广义指针； 仿函数是类似于函数的对象，可以是类对象或函数指针。","link":"/blog/2019/10/12/C++/C++中STL用法详解一-内容介绍/"},{"title":"Linux常用命令及设备文件高级管理技术总结","text":"Linux是一套免费使用和自由传播的类UNIX操作系统。 1 Linux简单介绍 Linux操作系统特性 开放性：遵从国际标准规范，凡遵循国际标准规范所开发的硬件和软件都能彼此兼容； 多用户：系统资源可以被不同用户各自拥有使用； 多任务：多个任务可并行访问微处理器； 良好的用户界面：用户界面、系统调用； 设备独立性：操作系统可以把所有外设当作文件对待，只要安装它们的驱动程序，就可以像访问文件一样操纵和使用这些设备，而不必知道它们的具体存在形式； 提供丰富的网络功能、可考的系统安全、良好的可移植性 发行版本严格来讲，Linux只是操作系统中的一个内核。负责控制硬件、管理文件系统、程序进程等。其并不提供强大的应用程序，没有编译器、系统管理工具、网络工具、Office套件、多媒体和绘图软件等。这样也就无法发挥其强大功能。因此有人便提出以Linux为核心再集成各式系统程序或应用工具程序，组成一套完整的操作系统。组装成的即为Linux发行版本。如：Red Hat、Ubuntu等。 2 Linux基本命令Linux命令格式如下： 1command [-options] [arguments] 具体说明如下：选项是对命令的特别定义，以“-”开始，多个选项可用一个“-”连起来。如“ls -l -a”与“ls -la”作用相同；一般来说，单字符选项前使用一个减号，单词选项前使用两个减号，如“ls –help”；Linux命令区分大小写，往往是表示相应功能英文单词缩写；操作参数可以是文件、目录； 2.1文件和目录管理 定位文件和目录pwd显示用户所在的位置 1pwd cd(改变工作目录) 123456#进入etc目录cd /etc#回到用户主目录cd ~#返回上级目录cd ../ Find(在硬盘上查找文件) 12345678#格式find [&lt;路径&gt;] [匹配条件]#在usr文件夹下查找文件名为password的文件find /usr -name password#列出当前文件夹下所有扩展名是doc的文件find / -name “*.doc”#查找/etc、/home下文件尺寸大于4K的文件find /etc /home -size +4K Locate（定位文件或目录） 1locate apt.conf 浏览文件或目录ls显示用户当前目录或指定目录的内容 1234#输出根目录下文件或目录的详细信息ls -l#列出所有文件ls -a head tail查看文件的开头或结尾部分 1234#查看profile文件开头5行head -5 /etc/profile#查看profile文件结尾4行tail -4 /etc/profile cat（合并文件或显示文件内容） 123456#阅读profile文件cat /etc/profile#追加file2文件内容到file1cat file2 &gt;&gt; file1#合并file1 file2到file3cat file1 file2 &gt; file3 搜索文件内容grep（在文件中查找指定字符串） 1grep 字符串 /etc/profile 操作文件和目录 1234567891011#cp(将/etc/profile文件复制到home文件夹下)cp /etc/profile /home#touch创建文件#mv(将test文件移动到上层目录)mv test ../#rm（删除/etc/profile文件）rm /etc/profile#mkdir创建目录mkdir dir1#rmdir删除目录rmdir dir1 2.2 简单系统管理 .shutdown(关机) 123456#立即关机showdown -h now#关机后立即重启showdown -r now#系统15:30后重启showdown -r 15:30 id（显示当前用户名和所属组）su（切换登录用户） 1su otherUser man（查看某个命令的使用手册） 1man ls 其它 1date(查看日期)，cal（显示日历或年历），df（查看磁盘），du（查看目录或文件容量），free（查看系统内存占用情况） 2.3 文件压缩和解压缩 通常使用zip、unzip、gzip、gunzip、tar命令。 2.4 进程控制与作业控制Linux启动进程有两个主要途径：手工启动和调度启动，手工启动又分为前台启动和后台启动。 进程管理ps（查看系统的进程） 123456#显示当前控制终端的进程ps#显示所有进程ps -A#详细显示包含其它使用者的进程ps -au kill(给进程发送信号) 12#当某个进程运行错误时，对于前台进程可以Ctrl+C终止该进程，后台进程可以使用kill命令给进程发送信息，比如强行终止信息kill -9 进程号 bg（将作业放到后台执行） 1234567#在手工启动前台进程时，如果没有执行完毕，则可使用Ctrl+Z 暂停进程的执行，然后可使用bg命令将进程放到后台执行，前台继续其它任务。#列出所有作业及作业ID信息jobs#将作业放到后台bg %作业ID#将作业放到前台fg %作业ID 作业调度(略)","link":"/blog/2019/10/23/Linux/Linux常用命令及设备文件高级管理技术总结/"},{"title":"Linux常用网络命令及APT软件的安装与升级","text":"对Linux常用命令及APT安装软件技术做出总结。 一、常用网络命令 网络测试命令ping(测试本主机与目标主机连通性) 123456#使用IP地址ping主机之间的连通性（以下命令发出三次信息，并显示）ping -c 3 192.168.0.1#使用IP地址ping主机之间的连通性（不显示中间结果）ping -c 3 -q 192.168.0.1#使用域名ping主机之间的连通性ping -c 5 ubuntu.org.cn traceroute(显示本机到目标主机的路由路径) 1traceroute ubuntu.prg.cn 网络查看命令host(显示主机名称) 1234#查看ubuntu.org.cn的IPhost ubuntu.org.cn#查看202.103.224.68的主机名host 202.103.224.68 netstat(显示网络连接、路由表、网络接口统计数等信息) 12345678#显示路由表netstat -r#显示网络接口信息netstat -i eth0#显示正在监听网络服务netstat -tul#显示网络所有的连接netstat -an hostname(显示或设置系统的主机名) 1hostname 通信命令telnet(远程登录客户程序) 12#远程登录服务器192.168.0.200telnet 192.168.0.200 ftp(连接远程FTP服务器) 1ftp 192.168.0.10 wall(向所有终端发送字符信息) 12#没有提示，可输入一行或多行信息，按Ctrl+D结束wall mail(发送电子邮件) 12#向student用户发送电子邮件，按Ctrl+D结束输入mail student 二、APT软件的安装与升级Ubuntu提供了APT，在软件安装、维护方面更加易用，使用起来甚至比Windows的安装与升级更加方便。","link":"/blog/2019/10/24/Linux/Linux常用网络命令及APT软件的安装与升级/"},{"title":"Linux设备与用户组管理以及网络常用命令总结","text":"针对设备管理进行简单介绍，并对用户组管理常用命令进行总结。 一、设备管理Linux设备管理的主要任务是控制设备完成输入/输出操作，所以又称输入输出（I/O）子系统。Linux把设备看作特殊文件，系统通过处理文件的接口——虚拟文件系统来管理和控制各种设备。它的任务是把各种设备硬件的复杂物理特性的细节屏蔽起来，提供一个队各种不同设备使用同一方式进行操作的接口。 设备的绝对号Linux系统按照某种规则为每台设备分配了唯一的号码，用作设备的区分和识别。 设备文件 为方便管理，Linux系统中的每样设备都被当作文件。应用程序能够像访问文件一样，通过各种系统函数方便的访问设备文件所对应的的硬件设备。 设备文件也有文件名，一般置于/dev目录下，设备文件主要包含权限和设备类型有关的信息，以及可供系统内核唯一识别的设备号：主设备号与自设备号。主设备号确定设备的类型，使用什么样的驱动程序；子设备号表示同类型设备的相对序号。 Linux系统本身对于如何控制外设并无内建的指令，所有用于和外设通信的指令都包含在一个叫设备驱动程序的文件中。==Linux操作系统就是通过设备文件对设备和设备驱动程序的跟踪，实现对设备的管理。== 设备的分类设备主要分为：块设备、字符设备、网卡。 二、用户与组的管理Linux下用户可分为三类：超级用户(root)、系统用户(系统正常工作所必需的内建用户，不能用来登录的虚拟用户)、普通用户。Linux下每个用户都有一个ID，Ubuntu中超级用户的UID为0，系统用户的一般为1~499，普通用户默认为1000~60000之间的值。Linux系统采用纯文本来保护账号的各种信息。其中最重要的文件有/etc/password、/etc/shadow、/etc/group，Linux用户登录系统的过程实质上是系统读取和核对这几个文件的过程。 用户的管理useradd（添加用户） 1useradd 用户名 passwd（修改密码） userMod（改变用户的属性） userdel （删除用户） 组管理 groupadd(添加组名) groupmod(修改组属性) gpasswd(改变组中成员用户或组的密码) 123456#修改user1组的密码gpasswd user1#添加student用户到user1组gpasswd -a student user1#删除student从user1组gpasswd -d student user1 groupdel(删除组) 权限控制访问权限内规定了不同用户的3种访问文件或目录的方式：读(r)、写(w)、可执行或查找(x)。基本上文件或目录的9个属性分别属于文件主、组用户、其他用户这3类用户，在设定权限时可以对3类用户采用如下方式表示：u(user)：表示文件所有者g(group)：表示文件的所属组o(others)：表示其他用户a(all)：表示所有用户(即u+g+o) chmod（改变或设置文件或目录的访问权限） 123456#将文件profile的权限改为所有用户对其都有执行权限chmod a+x profile#将文件profile的权限重新设置为文件主可以读和执行，组用户可以执行，其他用户无权访问chmod u=rx,g=x,o= profile#将文件profile的权限重新设置为只有文件主可以读和执行chmod g-x profile chown（改变某个文件或目录的拥有者和所属的组） 12#改变文件profile的文件主和用户组为student、group1chown student:group1 profile 三、sudo工具的使用在Linux系统中，提供了一个特殊的工具sudo，sudo允许系统管理员让普通用户执行部分或全部需要root权限命令的工具。要使用sudo工具的用户需要在/etc/sudoers里添加一行student ALL=(ALL) ALL使用sudo命令很简单，只要在运行的命令前加上sudo，然后再运行，如： 1sudo mkdir file","link":"/blog/2019/10/23/Linux/Linux设备与用户组管理以及网络常用命令总结/"},{"title":"MySql命令总结大全","text":"对MySql中命令进行详细总结。 最近详细过了下SQL有关知识，以及刷完了Leetcode上所有有关数据库的题目。因此在这里想开启一段MySql探讨之旅，从常用命令-&gt;数据库基础知识-&gt;安全保护机制-&gt;并发版本控制(锁协议)-&gt;物理索引机制-&gt;LeetCode优秀题目-&gt;JDBC中间件-&gt;数据库系统架构等进行一份超全总结。不完善的地方小伙伴们可以在评论里提醒作者进行补充。 1.SQL分类SQL一共分为四类： 数据定义语言(DDL): 创建、删除、修改：模式、数据库、索引、视图、表结构等； 数据操作语言(DML): 对关系表进行增、删、改； 数据控制语言(DCL): 定义数据控访问权限和安全级别，及创建用户和相关授权； 数据查询语言(DQL): 对关系表进行查询；平时我们用的较多的主要是DML、DQL。 2.DDL 数据库的创建与删除 1234# 创建数据库create database 数据库名；# 删除数据库drop database 数据库名 [distinct|casecade]; 表的创建、删除、修改 1234567891011121314151617181920212223# 创建表create table 表名( 列名1 数据类型1 列级完整性约束1， 列名2 数据类型2 列级完整性约束2) CHARACTER set gbk;# 删除表drop table 表名 [distinct|casecade];# 增加列alter table 表名 add 列名 数据类型 [列级完整性约束]；# 删除列alter table 表名 drop 列名；# 添加表级完整性约束alter table 表名 add 表级完整性约束；# 修改列alter table 表名 modify 列名 新的数据类型 [列级完整性约束]# 删除主键alter table drop primary key; 索引的定义与删除 12345678# 创建索引create [UNIQUE|FULLTEXT|SPATIAL] INDEX 索引名 on 表名(列名) [using 索引方法]； # 删除索引drop index 索引名 on 表名；# 查看已创建索引show index from 表名； 视图的定义与删除 12345678# 创建视图create view 视图名 as select语句；# 查询视图describe 视图名；# 删除视图drop 视图名； 3.DML 表中插入数据 1234567insert into 表名 values (0,&apos;小明&apos;,18,180.00,2,1,0),(0,&apos;小月月&apos;,18,180.00,2,2,1),insert into 表名(列名1，列名2) values(&apos;小明&apos;，18)，(&apos;小月月&apos;，18) 删除表中数据 123456# 删除部分数据delete from table 表名 where name = &apos;小明&apos;# 删除所有数据delete from 表名;truncate table 表名;(属于先删除表结构再新建表结构，数据量大的时候可使用) 修改表中数据 1update 表名 set 列名 = 新值 where name = &apos;小明&apos; 4.DCL 用户管理 12345678# 创建用户create user 用户名 identified by 密码；# 删除用户drop user 用户名@&apos;%&apos;;# 查看权限show grants for 用户名; 权限管理 12345# 授权GRANT &lt;权限&gt; ON &lt;数据库&gt;.&lt;表名&gt; TO &lt;用户名&gt;；# 删除权限remove 权限 on 数据库.表名 from 用户名； 5.DQL 一般格式 12345select [all|distinct] 目标列 from 表名或视图或派生表where条件group by 列名having 条件表达式order by 列名 [ASC|DESC]; 连接查询 1select * from 表1 inner或left或right join 表2 on 表1.列 = 表2.列 嵌套查询 1234select Sname from studentwhere Sno IN( select ...) 集合查询 1234567891011121314# 并集select * from student where Sdept=&apos;&apos;CSUNIONselect * from student where Sage&gt;19;# 交集(Mysql不能直接支持交集，需要自行实现)SELECT a.oname,a.odesc FROM object_a a INNER JOIN object_b b ON a.oname=b.oname AND a.odesc=b.odesc# 差集(Mysql不能直接支持差集，需要自行实现)SELECT a.oname, a.odesc FROMobject_a a LEFT JOIN object_b b ON a.oname = b.oname AND a.odesc = b.odescWHERE b.id IS NULL 基于派生表的查询 1select Sno,Cno From SC, (select * from student); 常用聚集函数 12345COUNT（个数）SUM（求和）AVG（求平均）MAX（最大值）MIN（最小值） 补充说明 DQL补充关于数据库查询的主要分为以上几类，具体点的一些细节(字段重命名，表达式计算等)可以参考该链接[link]。 where与having区别where作用于基本表和视图；having作用于组where语句不能使用聚集函数；having可以使用聚集函数 MySQL函数使用Mysql内置了很多优秀函数可以方便我们的查询；我平时用的较多的主要是关于日期的函数、数学的函数、和case if两个语句。可参考该链接link","link":"/blog/2019/12/04/MySql/MySql命令总结大全/"},{"title":"MySql并发版本_四种事务隔离的级别","text":"本文对数据库操作中的事务以及事务的隔离级别进行详细介绍。 本文实验的测试环境: Windows10+cmd+MySQL5.6.36+InnoDB 一、事务的四个特性（ACID） 原子性（Atomicity）：事务开始后所有操作，要么全部做完，要么全部不做，不可能停滞在中间环节。事务执行过程中出错，会回滚到事务开始前的状态，所有的操作就像没有发生一样。也就是说事务是一个不可分割的整体，就像化学中学过的原子，是物质构成的基本单位。 一致性（Consistency）：事务开始前和结束后，数据库的完整性约束没有被破坏 。比如A向B转账，不可能A扣了钱，B却没收到。 隔离性（Isolation）：同一时间，只允许一个事务请求同一数据，不同的事务之间彼此没有任何干扰。比如A正在从一张银行卡中取钱，在A取钱的过程结束前，B不能向这张卡转账。 持久性（Durability）：事务完成后，事务对数据库的所有更新将被保存到数据库，不能回滚。 二、事务的并发问题 脏读：事务A读取了事务B更新的数据，然后B回滚操作，那么A读取到的数据是脏数据。 不可重复读：事务 A 多次读取同一数据，事务 B 在事务A多次读取的过程中，对数据作了更新并提交，导致事务A多次读取同一数据时，结果不一致。 幻读：系统管理员A将数据库中所有学生的成绩从具体分数改为ABCDE等级，但是系统管理员B就在这个时候插入了一条具体分数的记录，当系统管理员A改结束后发现还有一条记录没有改过来，就好像发生了幻觉一样，这就叫幻读。 小结**：不可重复读的和幻读很容易混淆，不可重复读侧重于修改，幻读侧重于新增或删除。解决不可重复读的问题只需锁住满足条件的行，解决幻读需要锁表。 三、MySQL事务隔离级别 事务隔离级别 脏读 不可重复读 幻读 读未提交（read-uncommitted） 是 是 是 读已提交（read-committed） 否 是 是 可重复读（repeatable-read） 否 否 是 串行化（serializable） 否 否 否 mysql默认的事务隔离级别是可重复读 四、用例子说明各个隔离级别的情况 读未提交（1）打开一个客户端A，并设置当前事务模式为read uncommitted（未提交读），查询表account的初始值：（2）在客户端A的事务提交之前，打开另一个客户端B，更新表account：（3）这时，虽然客户端B的事务还没提交，但是客户端A就可以查询到B已经更新的数据：（4）一旦客户端B的事务因为某种原因回滚，所有的操作都将会被撤销，那客户端A查询到的数据其实就是脏数据：（5）在客户端A执行更新语句update account set balance = balance - 50 where id =1，lilei的balance没有变成350，居然是400，是不是很奇怪，数据不一致啊，如果你这么想就太天真了，在应用程序中，我们会用400-50=350，并不知道其他会话回滚了，要想解决这个问题可以采用读已提交的隔离级别 读已提交（1）打开一个客户端A，并设置当前事务模式为read committed（读已提交），查询表account的所有记录：（2）在客户端A的事务提交之前，打开另一个客户端B，更新表account：（3）这时，客户端B的事务还没提交，客户端A不能查询到B已经更新的数据，解决了脏读问题：（4）客户端B的事务提交（5）客户端A执行与上一步相同的查询，结果 与上一步不一致，即产生了不可重复读的问题 可重复读（1）打开一个客户端A，并设置当前事务模式为可重复读(repeatable read)，查询表account的所有记录（2）在客户端A的事务提交之前，打开另一个客户端B，更新表account并提交（3）在客户端A查询表account的所有记录，与步骤（1）查询结果一致，没有出现不可重复读的问题（4）在客户端A，接着执行update balance = balance - 50 where id = 1，balance没有变成400-50=350，balance值用的是步骤（2）中的350来算的，所以是300，数据的一致性倒是没有被破坏。可重复读的隔离级别下使用了MVCC机制，select操作不会更新版本号，是快照读（历史版本）；insert、update和delete会更新版本号，是当前读（当前版本）。（5）重新打开客户端B，插入一条新数据后提交(6）在客户端A查询表account的所有记录，没有查出新增数据，所以没有出现幻读 串行化（1）打开一个客户端A，并设置当前事务模式为serializable，查询表account的初始值：（2）打开一个客户端B，并设置当前事务模式为可串行化(serializable)，插入一条记录报错，表被锁了插入失败（因为客户端A的事务还未提交），mysql中事务隔离级别为serializable时会锁表，因此不会出现幻读的情况，这种隔离级别并发性极低，开发中很少会用到。 五、补充 事务隔离级别为读已提交时，写数据只会锁住相应的行 事务隔离级别为可重复读时，如果检索条件有索引（包括主键索引）的时候，默认加锁方式是next-key锁；如果检索条件没有索引，更新数据时会锁住整张表。一个间隙被事务加了锁，其他事务是不能在这个间隙插入记录的，这样可以防止幻读。 事务隔离级别为串行化时，读写数据都会锁住整张表 隔离级别越高，越能保证数据的完整性和一致性，但是对并发性能的影响也越大。 关于多版本控制MMVC的解释可以参考以下链接(最好的解释，没有之一)[link] 关于next-key 锁可以参考链接[link]","link":"/blog/2019/12/17/MySql/MySql并发版本-四种事务隔离的级别/"},{"title":"MySql并发版本_锁机制超全大总结","text":"本文对MySQL中的所激进进行一份详细总结。 宏观数据库锁粒度小，方便用于集群环境 代码锁粒度大，需要封装 微观分类只有明确指定主键，才会执行行锁，否则执行表锁 无锁主键不存在 行锁 表锁主键不明确 锁算法（机制） 行锁算法 Record Lock(普通行锁)键值在条件范围内，记录存在 Gap Lock(间隙锁)键值不在条件范围内，叫做间隙(GAP)，引擎就会对这个间隙枷锁，这种机制就是Gap机制 Next-Key Lock(行&amp;间隙)在键值范围条件内，同时键值又不存在条件范围内 表锁算法 意向锁当一个事务带着表锁去访问另一个被加了行锁的资源，那么此时这个行锁就会升级成意向锁，将表锁住。 自增锁事务插入自增类型的列时，获取自增锁。如果一个事务正在往表中插入自增记录，其它事务都必须等待。 实现行锁和表锁是粒度的概念，共享锁和排它锁是它们的具体体现。 共享锁（S）允许一个事务去读一行，阻止其它事务去获取该行的排它锁 排它锁（X）允许持有排它锁的事务读写数据，阻止其它事务获取该资源的共享锁和排它锁注意点：某个事务获取数据的排它锁，其它事务不能获取该数据的任何锁，并不代表其它事务不能无锁读取数据。 另一种实现不管什么锁都需要加失败重试 乐观锁 悲观锁 1.前言锁是协调多个进程或线程并发访问某一资源的一种机制。相对于其他的数据库而言，MySQL的锁机制比较简单，最显著的特点就是不同的存储引擎支持不同的锁机制。根据不同的存储引擎，MySQL中锁的特性可以大致归纳如下： 引擎 行锁 表锁 页锁 MyISAM √ BDB √ √ InnoDB √ √ 表锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突概率高，并发度最低 行锁：开销大，加锁慢；会出现死锁；锁定粒度小，发生锁冲突的概率低，并发度高 页锁：开销和加锁速度介于表锁和行锁之间；会出现死锁；锁定粒度介于表锁和行锁之间，并发度一般 总结：*表锁更适用于以查询为主，只有少量按索引条件更新数据的应用；行锁更适用于有大量按索引条件并发更新少量不同数据，同时又有并发查询的应用。 2.MyISAM引擎MyISAM存储引擎只支持表锁，这也是MySQL开始几个版本中唯一支持的锁类型。随着应用对事务完整性和并发性要求的不断提高，MySQL才开始开发基于事务的存储引擎，后来慢慢出现了支持页锁的BDB存储引擎和支持行锁的InnoDB存储引擎（实际 InnoDB是单独的一个公司，现在已经被Oracle公司收购）。但是MyISAM的表锁依然是使用最为广泛的锁类型。","link":"/blog/2019/12/17/MySql/MySql并发版本-锁机制超全大总结/"},{"title":"MySql索引机制大总结","text":"","link":"/blog/2019/12/17/MySql/MySql索引机制大总结/"},{"title":"数据库基础知识大总结","text":"本节针对上一篇基础之上，对数据库的一些原理进行总结。主要包括 数据库系统概述 数据库系统定义 数据模型 三层模式两层映像 关系数据库 三个完整性 视图 数据字典 范式 数据库编程 断言 触发器 存储过程 MySQL中的存储引擎 1.数据库系统概述1.1 数据库系统数据库系统由 数据库、数据库管理系统、应用程序、数据库管理人员 组成的存储管理、处理和维护数据的系统。 1.2 数据模型 分类 概念模型按用户观点对数据信息建模（比如E-R图） 逻辑模型按计算机系统观点对数据建模（建立具体的数据库表）。现在企业常用的都是关系模型，除此之外还有层次模型和网状模型等。 物理模型数据在磁盘或磁带上的存储方式和存取方法（比如索引） 组成元素数据结构、数据操作、数据完整性约束 1.3 三层模式两层映像数据库系统结构如下： 模式也称为逻辑模式，是数据库中所有用户的公共数据视图。 外模式也称为用户模式，模式的子集。 内模式也称为存储模式，数据物理结构和存储方式的描述。两层映像分别保证数据与程序的逻辑独立性和物理独立性。 2.关系模型关系模型是属于逻辑模型的一种，就是现在主流使用的。2.1 三个完整性实体完整性(主键非空且唯一)；参照完整性(外键有的对应主键必须有)；用户自定义完整性(比如设置某整型字段数据&lt;100) 2.2 视图视图是从一个或几个基本表(或视图)导出的表，是一个虚表，不存放对应的数据。视图消解: 将对视图的操作转换为对具体表的操作。 视图作用：简化用户操作、提供一定程度逻辑独立性、对机密数据提供安全保护 3.范式合理的关系模型数据库设计能尽量避免：数据冗余、更新插入删除异常等问题。因此一些大佬们设计了5个范式。 4.数据库编程4.1 断言断言可以定义为涉及多个表或聚集操作的比较复杂的完整性约束。好吧，上面一句话不好懂，说白了断言就是会对断言中的子句进行检查，当为真值才允许执行。 断言定义 1create assertion 断言名 检查子句 举例 1234# 限制每学期每门课最多60人选修create assertion 断言名 check( 60 &gt;= select count(*) from SC group by cno ) 这样在插入的时候就会触发检查该断言，满足条件才允许插入，否则就会拒绝插入。 4.2 触发器触发器又称为 事件-条件-动作规则，当执行事件时会对条件进行检查，满足条件会进一步执行相应动作。比如我们平时写功能，在一个表插入数据，另一个也需要跟着插入数据就可以通过触发器实现。 4.3 存储过程存储过程（Stored Procedure）是一组为了完成特定功能的SQL 语句集，经编译后存储在数据库。用户通过指定存储过程的名字并给出参数（如果该存储过程带有参数）来执行它。存储过程只在创造时进行编译，以后每次执行存储过程都不需再重新编译，而 一般SQL 语句每执行一次就编译一次,所以使用存储过程可提高数据库执行速 度。 4.4 MySQL中的存储引擎有两个常用存储引擎：MyISAM与InnoDB（MySQL默认的）关于两个存储引擎的区别可参考该链接[link]","link":"/blog/2019/12/05/MySql/数据库基础知识大总结/"},{"title":"数据库安全机制及恢复技术大总结","text":"本文对数据库中的安全保护机制进行详细总计。 数据库安全性 不安全因素 安全性控制 视图机制 审计 数据加密 其他 数据库恢复技术 事务的基本概念 故障种类 恢复技术(转储、镜像) 1.数据库安全性 不安全因素 非授权用户对数据库的恶意存取和破坏 数据库中重要或敏感数据被泄露 安全环经的脆弱 安全控制机制 用户身份鉴别(静态口令鉴别、动态口令鉴别、生物特征鉴别、智能卡鉴别) 存取控制定义用户权限（将用户权限登记到数据字典中）合法权限检查（自主存取控制、强制存取控制） 视图机制把保密的数据对无权存取的用户隐藏起来 审计相当于把用户对数据库的所有操作自动记录下来放入审计日志，根据审计日志就可以找到非法操作的人、时间、操作内容。 数据加密存储加密、传输加密 其它推理控制、隐蔽信道、数据隐私 2.数据库恢复技术 事务的基本概念事务是用户定义的一个数据库操作序列，这些操作要么全做，要么全不做。需要保证以下四个特性ACID: 原子性事务中的操作要么全做要么全不做 一致性事务执行的结果必须是使数据库从一个一致性状态变到另一个一致性状态。 隔离性一个事务的执行不能被其它事务干扰。 持续性一个事务一旦提交，它对数据库中数据的改变应该是永久性的。 故障的种类各类故障对数据库的影响可能有两种可能性：一是数据库本身被破坏，二是数据库没有破坏但是数据可能不正确。1）事务内部的故障（非预期的，可能由运算溢出、并发事务发生死锁而被撤销选中事务等原因造成）2）系统故障（如操作系统故障、系统断电等）3）介质故障（外存故障）4）计算机病毒 恢复的实现技术恢复的基本原理是通过冗余。所以恢复技术涉及的两个关键问题是：如何建立冗余数据、如何利用这些冗余数据实施数据库恢复。 数据转存分类：1）静态海量存储：在系统中无运行事务时进行，每次转储全部数据库。2）静态增量存储：在系统中无运行事务时进行，每次转储上次转储后更新的数据。3）动态海量存储：转储期间允许数据库进行存取或修改，每次转储全部数据库。需要日志文件。4）动态增量存储：转储期间允许数据库进行存取或修改，每次转储上次转储后更新的数据。需要日志文件 恢复策略1）事务故障的恢复：反向扫描日志文件，对故障事务的更新操作执行逆操作。2）系统故障的恢复：正向扫描日志文件，对撤销队列中的各事务进行撤销操作，对重做队列中的各事务进行重做操作 。3）介质故障的恢复：重装数据库，利用数据库副本和日志文件重做已完成的事务。 或者利用镜像技术。 补充：利用日志技术进行数据库恢复时，恢复子系统必须搜索日志，确定哪些事务需要重做，哪些需要撤销。一般来说，需要检查所有日志记录。这样做有两个问题，一是搜索日志将耗费大量时间，而是很多需要重做处理的事务实际上已经将它们的更新操作写到了数据库中，然后恢复子系统又重新执行了这些操作，浪费了大量的时间。为了解决这些问题，又增加了检查点记录。","link":"/blog/2019/12/05/MySql/数据库安全保护机制大总结/"},{"title":"(七)手写数字识别_线性回归、多层感知机、CNN实现","text":"以手写数字识别为出发点，对深度学习中的网络构建、预测分析、迭代优化等主要流程进行相关介绍。 一、前言MNIST可以算的上是机器学习界的“Hello World”，感觉可以算是深度学习中一个很好的入门范例。MNIST数据集主要由一些手写数字的图片和相应的标签组成。图片一共10类，分别对应0-9，每张图片都是由28*28的矩阵表示。其中训练集一共60000张图片，测试集一共10000张图片，训练集又分出来5000张为验证集图片。如下所示： 数据集下载运行如下代码，会自动的将数据集下载到工程文件下的MNIST_data文件夹下，如果下载失败，也可以到该链接手动下载保存到本地[link] 12from tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True) 图片保存以上下载的数据集是向量集，如果想保存为具体的图片，可执行如下操作： 12345678910111213141516171819from tensorflow.examples.tutorials.mnist import input_datafrom PIL import Imageimport osmnist = input_data.read_data_sets(\"../MNIST_data/\", one_hot=True)# 如果没这个文件夹自动创建 save_dir = '../MNIST_data/raw/' if os.path.exists(save_dir) is False: os.makedirs(save_dir)for i in range(20): image_array = mnist.train.images[i, :] image_array = image_array.reshape(28, 28) #MNIST数据集图片灰度值都是0或者1，这里乘以256便于直观化图像 image_array *= 256 filename = save_dir + 'mnist_train_%d.jpg' % i image = Image.fromarray(image_array); if image.mode == \"F\": image = image.convert('RGB') image.save(filename) 独热表示手写数字识别的标签是通过独热编码表示的。 12345678from tensorflow.examples.tutorials.mnist import input_dataimport numpy as npmnist = input_data.read_data_sets(\"../MNIST_data\", one_hot=True)#看前20张训练图片的label for i in range(20): one_hot_label = mnist.train.labels[i, :] label = np.argmax(one_hot_label) print('mnist_train_%d.jpg label: %d' % (i, label)) 二、线性回归识别MNIST 原理因为是多分类问题，整体模型采用softmax回归；并用交叉熵作为概率分布的准确度估计；然后通过梯度下降迭代的进行训练；最后在测试集或验证集上对准确度进行评测。 代码示例SR-mnist.py 准确度0.91 三、多层感知机(MLT或FCN)识别MNIST 原理softmax和传统的神经网络的区别是没有隐含层，拟合能力有限。同时隐含层越多，越容易拟合复杂函数。使用较多层次的神经网络越容易过拟合、参数难以调试、梯度弥散等。对于这些问题需要很多Trick来解决，比如Dropout、Adagrad、ReLU等。当引入隐含层并使用非线性激活函数后，就可以使用曲线来划分一些样本，可以轻松解决异或函数的分类问题，神经网络的隐含层越多，就可以对原有特征进行更抽象的变换，模型的拟合能力就越强。没有隐含层的Softmax Regression只能直接从图像的像素点推断出是哪个数字，而没有特征抽象的过程。多层神经网络依靠隐含层，可以组合出高阶特征，之后可以将这些高阶特征组合成数字，就能实现精准的匹配和分类。现在加上隐含层，并使用减轻过拟合的Dropout、自适应学习速率的Adagrad、以及可以解决梯度弥散的激活函数ReLU。 代码示例MLT-mnist.py 准确度0.978 四、CNN识别MNIST 原理全连接的网络因为参数过多和梯度弥散等问题，在早期很难顺利地进行多层的训练，卷积神经网络可以利用空间结构关系减少需要学习的参数量，从而提高反向传播算法的训练效率。卷积神经网络的特点是局部连接(降低了参数量)、权值共享(赋予了对平移的容忍性)、池化层降采样(进一步降低参数量，赋予模型对轻度形变的容忍性)。 代码示例CNN-simple-mnist.py 精确度0.992","link":"/blog/2019/11/03/TensorFlow/七-手写数字识识识别-线性回归、多层感知机、CNN实现/"},{"title":"(三)TensorBoard可视化及模型数据的保存加载","text":"模型的可视化与保存在深度学习训练中起到很重要的作用。 一、模型的保存与恢复在通过TensorFlow进行深度学习训练时，会产生大量的参数，那么下一次进行训练的时候又得重新进行训练。所以可以将已经训练好的模型保存到本地。等需要的时候直接将模型读入到内存中来。 模型的保存 1234567891011121314import tensorflow as tfv1 = tf.Variable(1, name=\"v1\")v2 = tf.Variable(2, name=\"v2\")#定义初始化所有变量opinit_op = tf.initialize_all_variables()#增加保存模型opsaver = tf.train.Saver()#开启一个会话with tf.Session() as sess: sess.run(init_op) save_path = saver.save(sess=sess, save_path=\"/home/liuyan/Desktop/OpenCV/checkpoints/model.ckpt\") 运行之后就会生成保存的模型： 模型的恢复模型的恢复需要两步： 恢复模型的结构（如果自己又重新写了一份静态图，则不需要该步） 恢复模型的参数123456789101112import tensorflow as tf#恢复模型结构(省略的话如下所示需要自己再重新构建图)#saver = tf.train.import_meta_graph('/home/liuyan/Desktop/OpenCV/checkpoints/model.ckpt.meta')v1 = tf.Variable(0, name=\"v1\")v2 = tf.Variable(0, name=\"v2\")saver = tf.train.Saver()#开启会话 with tf.Session() as sess: saver.restore(sess, '/home/liuyan/Desktop/OpenCV/checkpoints/model.ckpt') print(sess.run(v1)) 选择存储和恢复哪些变量如果不给tf.train.Saver()传入参数，那么会保存所有的变量，其中每一个变量都以被创建时的名称被保存。 12#Add ops to save and restore only 'v2' using the name \"my_v2\"saver = tf.train.Saver({\"my_v2\": v2}) 二、TensorBoard可视化学习为了方便对TensorFlow程序的理解、调试与优化，可以使用TensorBoard来展现TensorFlow的静态图、绘制图像生成的定量指标图以及附加数据。相关操作十分简单，具体步骤如下： 将当前的图写入硬盘 123456789101112import tensorflow as tfv1 = tf.placeholder(dtype=tf.int32)v2 = tf.placeholder(dtype=tf.int32)v3 = tf.add(v1, v2, name=\"v3\")#开启一个会话with tf.Session() as sess: print(sess.run(v3, feed_dict={v1: 1, v2: 2})) #定义一个生成tensorboard图操作，第一个参数是图的存储路径 graph_writer_op = tf.summary.FileWriter('.', sess.graph) sess.run(graph_writer_op) 启动TensorBoard显示输入下面的指令来启动TensorBoard： 1python tensorflow/tensorboard/tensorboard.py --logdir=path/to/log-directory 这里的参数 logdir 指向 SummaryWriter 序列化数据的存储路径。如果logdir目录的子目录中包含另一次运行时的数据，那么 TensorBoard 会展示所有运行的数据。TensorBoard 开始运行后，可以通过在浏览器中输入 localhost:6006 来查看 TensorBoard。 如果已经通过pip安装了TensorBoard，也可以通过执行以下命令来访问： 1tensorboard --logdir=/path/to/log-directory TensorBoard图表可视化TensorBoard的图表计算强大而又复杂，在可视化和理解网络结构时非常有帮助，，如下所示：典型的网络结构都有数以千计的节点，那么多的节点难以一下子全部看到，简单起见我们可以为变量名划定范围。下面这个例子使用tf.name_scope在hidden命名域下定义了三个操作： 123456import tensorflow as tfwith tf.name_scope('hidden') as scope: a = tf.constant(5, name='alpha') W = tf.Variable(tf.random_uniform([1, 2], -1.0, 1.0), name='weights') b = tf.Variable(tf.zeros([1]), name='biases') 通过静态图获取具体tensorTensorFlow是通过定义静态图来运行的，我们也可以在程序最后获得这张图： 1g = tf.get_default_graph 然后可以进一步从图中获取变量： 12var_a = g.get_tensor_by_name(“v1”)print(sess.run(var_a))","link":"/blog/2019/09/17/TensorFlow/三-TensorBoard可视化及模型数据的保存与加载/"},{"title":"(一)TensorFlow相关介绍及安装步骤详解","text":"TensorFlow即是实现机器学习算法的接口，也是执行机器学习算法的框架 一、TensorFlow介绍 TensorFlow基础架构TensorFlow是谷歌2015年发布的第二代分布式机器学习系统。前端支持Python、C++、Go、Java等多种开发语言，后端使用C++、CUDA等写成，可以方便的部署到各种平台。 构件图、执行图TensorFlow使用数据流式图来规划计算流程，图中的节点被称之为 op (operation 的缩写)。 一个 op 获得 0 个或多个 Tensor, 执行计算，产生 0 个或多个 Tensor。一个 TensorFlow 图描述了计算的过程，不过图是静态的，为了进行计算，图必须在 会话 里被启动。 会话 将图的 op 分发到诸如 CPU 或 GPU 之类的 设备 上， 同时提供执行 op 的方法。执行后再将产生的 tensor 返回。在 Python 语言中, 返回的 tensor 是 numpy ndarray 对象; 在 C 和 C++ 语言中, 返回的 tensor 是tensorflow::Tensor 实例. 在一个会话中启动图 构建图完成后，执行图过程需要通过session会话完成： 1234567891011import tensorflow as tf#构建图matrix1 = tf.constant([[3., 3.]])matrix2 = tf.constant([[2.],[2.]])product = tf.matmul(matrix1, matrix2)#执行图#函数调用 'run(product)' 触发了图中三个 op (两个常量 op 和一个矩阵乘法 op) 的执行，返回resultsess = tf.Session()result = sess.run(product)print (result)sess.close() session使用完后需要关闭释放资源，除了显示调用close()，也可以用with自动完成： 123with tf.session() as sess: result = sess.run(product) print(result) 分配设备：一般不需要显示指定使用GPU还是CPU，TensorFlow能自动检测；如果检测到GPU会尽可能地利用找到的第一个GPU来执行操作。如果机器上有多个可用GPU，除了第一个其余默认是不参与计算的，需要明确指派。 123with tf.Session() as sess: with tf.device(\"/gpu:1\"): ... 交互式使用为了便于使用类与IPython的Python交互环境，可用InteractiveSession代替Session类，使用Tensor.eval()和Operation.run()代替Session.run()： 123456789import tensorflow as tfsess = tf.InteractiveSession()x = tf.Variable([1.0, 2.0])a = tf.constant([3.0, 3.0])# 使用初始化器 initializer op 的 run() 方法初始化 'x' x.initializer.run()# 增加一个减法 sub op, 从 'x' 减去 'a'. 运行减法 op, 输出结果 sub = tf.sub(x, a)print sub.eval() Fetch、Feed Fetch需要在op的一次运行中获取多个tensor值： 1result = sess.run([mul, intermed]) FeedFeed机制相当于提供数据作为run()调用的参数，只在调用它的方法内有效，方法结束feed就会消失。 12345input1 = tf.placeholder(tf.types.float32)input2 = tf.placeholder(tf.types.float32)output = tf.mul(input1, input2)with tf.Session() as sess: print sess.run([output], feed_dict={input1:[7.], input2:[2.]}) 二、TensorFlow实现原理TensorFlow有单机模式和分布式模式两种实现，单机指的是client、master、worker全在一台机器上的同一个进程中，分布式版本允许client、master、worker在不同机器的不同进程中，同时由 集群调度系统统一管理各项任务。在只有一个设备时，计算图会按依赖关系被顺序执行，当有多个设备时，复杂点有二： 每一个节点选择什么硬件设备执行 如何管理节点间的数据通信针对第一个问题，TensorFlow设计了一套为节点分配设备的策略，称为代价模型，会在模拟执行一个节点时，把每一个能执行这个节点的设备都测试一遍，现在一个综合时间最短的设备作为这个节点的运算设备。TensorFlow还能对节点的分配设置限制条件。针对第二个问题，整个计算图会被划分为许多子图，使用同一个设备并且相邻的节点会被划分到同一个子图： 性能优化关于TensorFlow的性能优化，TensorFlow会自动识别一些冲重复计算，同时改写计算图，只执行一次重复的计算。TensorFlow提供了三种不同的加速神经网络训练的并行计算模式： 数据并行 模型并行 流水线并行 三、Liunx下TensorFlow_GPU版本安装步骤TensorFlow并不是全部由Python写成的库，底层有很多C++乃至CUDA的代码。 先在本地安装Python3.5 安装AnacondaAnaconda是Python的一个科学计算发行版，内置了数百个Python经常会使用的库，单独安装这些库时很容易出现兼容性问题，建议安装Anaconda。 到Anaconda官网下载Anaconda3 4.2.0版 到Anaconda下载目录执行bash Anaconda3-4.2.0-Linux-x86_64.sh 接下来会看到安装提示，直接按回车确认进入下一步 安装完成后，程序提示是否把anaconda3的binary路径加入.bashrc，建议添加（这样以后python和ipython命令会自动使用Anaconda Python3.5的环境） 安装正确的CUDA版本 到CUDA官网下载CUDA安装包，一般里面集成了显卡驱动 安装前需要暂停当前NVIDIA驱动的X server，如果是远程连接的Linux机器，可以运行该命令：sudo init 3 将CUDA安装包权限设置成可执行的，并执行安装程序： 12chmod u+x cuda_8.0.44_linux.runsudo ./cuda_8.0.44_linux.run 接下来是CUDA安装的一些确认，除了确认是否安装CUDA 8.0 Samples选择n之外，其余的选择y；等待CUDA安装完成 到系统环境设置CUDA路径 1234567vim ~/.bashrc#cuda的绝对路径export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:/usr/local/cuda-8.0/extras/CUPTI/lib64:$LD_LIBRARY_PATHexport CUDA_HOME=/usr/local/cuda-8.0export PATH=/usr/local/cuda-8.0/bin:$PATHsource ~/.bashrc 安装正确的CUDNN版本cuDNN是NVIDIA推出的深度学习中CNN和RNN高度优化的实现，目前绝大多数的深度学习框架都使用cuDNN来驱动GPU计算。 从CUDNN官网下载，并解压 安装TensorFlow 下载安装 1pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.0rc0-cp35-cp35m-linux_x86_64.whl 选择是否确认信息到是否支持CUDA这一步，选择支持 选择要支持使用的CUDA、CUDNN版本及安装路径CUDA选择8.0版本，路径选择/usr/local/cuda-8.0；CUDNN选择5.1版本，路径也设置为/usr/local/cuda-8.0 12345Please specify the location where CUDA 8.0 toolkit is installed. Refer toREADME.md for more details. [default is: /usr/local/cuda]: /usr/local/cuda-8.0Please specify the location where CUDNN 5.1 V2 library is installed. Refer toREADME.md for more details. [default is: /usr/local/cuda]: /usr/local/cuda-8.0","link":"/blog/2019/09/16/TensorFlow/一-TensorFlow相关介绍及安装步骤详解/"},{"title":"(二)TensorFlow常量、变量、占位符及op详解","text":"TensorFLow提供了一个库来定义和执行对张量的各种数学运算。张量可理解为一个n维矩阵，所有类型的数据，包括标量、矢量、和矩阵等都是特殊类型的张量。 数据的类型 张量 形状 标量 0维张量 [] 向量 1维张量 [D0] 矩阵 2维张量 [D0,D1] 张量 N维张量 [D0,D1…Dn-1] TensorFlow支持以下三种类型的张量： 一、TensorFlow常量常量是值不能改变的张量。 声明标量、向量、矩阵的常量 1234567891011121314151617#1.声明一个标量常量：t_1 = tf.constant(4)#2.声明一个向量常量：t_2 = tf.constant([4,3,2])#3.声明一个2*3的零矩阵常量、全1矩阵常量（第二个参数表示类型）：zero_t = tf.zeros([2,3],tf.int32)ones_t = tf.ones([2,3],tf.int32)#4.创建一个与现有Numpy数组或张量常量具有相同形状的张量常量t_3 = tf.zeros_like(t_2)t_4 = tf.ones_like(t_2)#5.声明一个在一定范围内等差排列的序列t_5 = tf.linspace(start,stop,num)t_6 = tf.range(start,limit,delta) 创建随机张量TensorFlow允许创建具有不同分布的随机张量： 12345678#1.创建标准正态随机分布(mean表示均值，stddev表示标准差)t_random = tf.random_normal([2,3],mean=2.0,stddev=4,seed=None)#2.创建正态随机分布t_random = tf.truncated_normal([2,3],mean=2.0,stddev=2,seed=None)#3.创建均匀随机分布t_random = tf.random_uniform([2,3],minval=0..0,maxval=1.0,seed=None) 如果t_random是要将给定的[3,6]张量随机剪裁为[2,5]大小： 1tf.random_crop(t_random, [2,5], seed=None) 二、TensorFlow变量Variable变量通常在神经网络中表示权重和偏置。 Variable是TensorFlow下可以修改的张量，需要定义一个初始值，初始值可以是数值、列表、numpy矩阵，也可以直接是张量。 123456789#1.使用常量定义var_a = tf.Variable(3, dtype=tf.int32)var_b = tf.Variable([1,2], dtype=tf.float32)var_c = tf.Variable(tf.zeros([1024,10]))#2.使用另一个张量定义rand_t = tf.random_uniform([50,50], mean=0, stddev=10, seed=0)t_a = tf.Variable(rand_t)t_b = tf.Variable(rand_t) 在使用Variable时，必须初始化变量，也就是调用它们的初始化方法。调用初始化的方法可以全局调用，也可以初始化某些变量： Variable变量可以通过assign赋值，赋值的过程是一个op，也是需要执行才会产生效果： 12345678910111213141516import tensorflow as tfvar_a = tf.Variable(3, dtype=tf.int32)#变量重新赋值 assign_op = var_a.assign(5)#变量初始化 init = tf.global_variables_initializer()#开启一个会话 with tf.Session() as sess: #执行变量初始化op sess.run(init) #执行赋值op sess.run(assign_op) #输出赋值后新的结果 print(sess.run(var_a)) 三、TensorFlow占位符TensorFlow允许通过tf.placeholder(dtype,shape=None,name=None)在构建图的时候占据一个位置，然后等到执行图的时候再带入具体的值。占据一个位置： 执行图时赋值： 123456789import tensorflow as tfx = tf.placeholder(dtype=tf.float32)y = 2 * xdata = tf.random_uniform(shape=[4,5], minval=0.0, maxval=10)#开启一个会话 with tf.Session() as sess: x_data = sess.run(data) print(sess.run(y, feed_dict={x: x_data})) （需要注意的是，所有常量、变量和占位符将在代码的构建图部分中定义。如果在定义部分使用 print 语句，只会得到有关张量类型的信息，而不是它的值。为了得到相关的值，需要创建会话图并对需要提取的张量显式使用运行命令） 四、opTensorFlow的基本操作定义为op，以下是常见的op:矩阵的reshape在数据处理方面经常用到：","link":"/blog/2019/09/17/TensorFlow/二-TensorFlow常量、变量、占位符及op详解/"},{"title":"(五)TensorFlow实现线性回归算法","text":"本节以最简单y=0.7x+0.6线性回归为例，将TensorFlow训练、训练后模型保存及可视化串联介绍。 一、相关步骤 数据预处理（可选） 构建模型 定义损失函数及优化方法 使用梯度下降迭代训练(可其它优化方法) 保存模型 画图12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import tensorflow as tf# 消除警告(使用源码安装可自动消除)import osos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'# 回归函数def my_regression(): # 准备数据 with tf.variable_scope(\"data\"): # 准备100 条数据x的平均值为5.0 标准差为1.0 x = tf.random_normal([100, 1], mean = 5.0, stddev=1.0, name=\"x\") # 真实的关系为 y = 0.7x + 0.6 y_true = tf.matmul(x, [[0.7]]) + 0.6 # 创建模型 with tf.variable_scope (\"model\"): # 创建权重变量 weight = tf.Variable(tf.random_normal([1, 1], mean=1.0, stddev=0.1), name=\"weight\") # 创建偏置变量,初始值为1 bias = tf.Variable(1.0, name=\"bias\") # 预测结果 y_predict = tf.matmul(x, weight) + bias # 计算损失 with tf.variable_scope (\"loss\"): # 计算损失 loss = tf.reduce_mean(tf.square(y_predict - y_true)) # 减少损失 with tf.variable_scope(\"optimizer\"): # 梯度下降减少损失,每次的学习率为0.1 train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss) # 收集变量 tf.summary.scalar(\"losses\", loss) tf.summary.histogram(\"weightes\", weight) # 合并变量 merged = tf.summary.merge_all() # 初始化变量 init_op = tf.global_variables_initializer() # 梯度下降优化损失 with tf.Session() as sess: sess.run(init_op) print(\"初始的权重为{}, 初始的偏置为{}\".format(weight.eval(), bias.eval())) # 添加board记录文件 file_write = tf.summary.FileWriter('/Users/liuyan/tensorBoard/my_regression', graph=sess.graph) # 循环训练线性回归模型(20000次) for i in range(20000): sess.run(train_op) print(\"训练第{}次的权重为{}, 偏置为{}\".format(i,weight.eval(), bias.eval())) # 观察每次值的变化 # 运行merge summery = sess.run(merged) # 每次收集到的值添加到文件中 file_write.add_summary(summery, i)if __name__ == '__main__': my_regression() 二、运行结果 三、模型的保存 模型的保存 12saver = tf.train.Saver()saver.save(sess, \"./tmp/ckpt/test\") 模型恢复 1save.restore(sess, \"./tmp/ckpt/test\") 四、可视化 五、扩展关于损失函数和优化器，有很多种方法，其中也包括很多种激活函数，激活函数加在每一层神经网络后，避免多层网络变成单层。在线性回归基础上使用sigmoid激活函数就变成了逻辑回归(常用于二分类)，使用softmax激活函数就是多分类。 常用激活函数 常用损失函数 常用优化器","link":"/blog/2019/09/24/TensorFlow/五-TensorFlow实现线性回归算法/"},{"title":"(八)CIFAR-10与ImageNet图像识别","text":"本文以CIFAR-10数据集为例，从数据集下载、显示、数据预处理、训练模型，到查看测试模型效果整体进行介绍。 一、前言CIFAR-10是一个经典的数据集，包含60000张32*32的彩色图片，一共10类图片（分别是airplane、automobile、bird、cat、deer、dog、frog、horse、ship、truck）其中训练集50000张，测试集10000张。","link":"/blog/2019/11/04/TensorFlow/八-CIFAR-10与ImageNet图像识别/"},{"title":"(四)TensorFlow数据读取","text":"TensorFlow主要提供了三中读取数据的方式： 供给数据(Feeding)： 在TensorFlow程序运行的每一步， 让Python代码来供给数据。 文件读取： 在TensorFlow图的起始， 让一个输入管线从文件中读取数据。 预加载数据：在TensorFlow图中定义常量或变量来保存所有数据(仅适用于数据量比较小的情况)。 一、供给数据供给数据(Feeding)就是之前讲解变量那一节提到的通过feed_dict给placeholder占位符提供值。 1234with tf.Session(): input = tf.placeholder(tf.float32) classifier = ... print classifier.eval(feed_dict={input: my_python_preprocessing_fn()}) 二、文件读取当数据集很大，使用此方法可以确保不是所有数据都立即占用内存（如60GB的YouTube-8m数据集）。从文件读取的过程可以通过以下步骤完成： 使用字符串张量 [“file0”，”file1”] 或者 [(“file%d”i)for in in range(2)] 的方式创建文件命名列表，或者使用 files=tf.train.match_filenames_once('*.JPG') 函数创建。 将文件名列表交给tf.train.string_input_producer 函数来生成一个先入先出的队列，文件阅读器会需要它来读取数据。 12#string_input_producer提供的可配置参数来设置文件名乱序和最大的训练迭代数filename_queue = tf.train.string_input_producer(files) Reader用于从文件名队列中读取文件：根据输入文件格式选择相应的阅读器，然后将文件名队列提供给阅读器的read方法。 Decoder：使用一个或多个解码器和转换操作将值字符串解码为构成训练样本的张量：上一步阅读器的read方法会输出一个key来表征输入的文件和其中的纪录(对于调试非常有用)，同时得到一个字符串标量，这个字符串标量可以被一个或多个解析器，或者转换操作将其解码为张量并且构造成为样本。 以CSV格式文件举例： 12345678910111213141516171819202122filename_queue = tf.train.string_input_producer([\"file0.csv\", \"file1.csv\"])reader = tf.TextLineReader()key, value = reader.read(filename_queue)# Default values, in case of empty columns. Also specifies the type of the# decoded result.record_defaults = [[1], [1], [1], [1], [1]]col1, col2, col3, col4, col5 = tf.decode_csv(value, record_defaults=record_defaults)features = tf.concat(0, [col1, col2, col3, col4])with tf.Session() as sess: # Start populating the filename quee. coord = tf.train.Coordinator() threads = tf.train.start_queue_runners(coord=coord) for i in range(1200): # Retrieve a single instance: example, label = sess.run([features, col5]) coord.request_stop() coord.join(threads) 每次read的执行都会从文件中读取一行内容， decode_csv 操作会解析这一行内容并将其转为张量列表。如果输入的参数有缺失，record_default参数可以根据张量的类型来设置默认值。在调用run或者eval去执行read之前， 必须调用tf.train.start_queue_runners来将文件名填充到队列。否则read操作会被阻塞到文件名队列中有值为止。 三、预加载数据这仅用于可以完全加载到存储器中的小的数据集。有两种方法： 存储在常量中 123456training_data = ...training_labels = ...with tf.Session as sess: x_data = tf.Constant(training_data) y_data = tf.Constant(training_labels)... 存储在变量中，初始化后，永远不要改变它的值 12345678910training_data = ...training_labels = ...with tf.Session() as sess: data_initializer = tf.placeholder(dtype=training_data.dtype,shape=training_data.shape) label_initializer = tf.placeholder(dtype=training_labels.dtype,shape=training_labels.shape) input_data = tf.Variable(data_initalizer, trainable=False, collections=[]) input_labels = tf.Variable(label_initalizer, trainable=False, collections=[]) ... sess.run(input_data.initializer,feed_dict={data_initializer: training_data}) sess.run(input_labels.initializer,feed_dict={label_initializer: training_lables})","link":"/blog/2019/09/17/TensorFlow/四-TensorFlow数据读取/"},{"title":"JAVA面向过程基础总结","text":"","link":"/blog/2019/11/01/WEB/JAVA面向过程基础总结/"},{"title":"web项目外网服务器","text":"本地JAVAWEB项目，如果想通过外网URL可以直接访问，就需要将本地程序包的war包上传到远程服务器指定文件。本文以上传ubuntu服务器为例介绍。 步骤 远程服务器配置安装Apache、Tomcat、MySQL[参考网址] 将本地程序上传服务器 将数据库脚本.sql文件上传服务器 将本地程序上传服务器在配置好服务器环境之后，选择任何一种可连接远程终端工具将项目文件上传即可，在这里推荐Mobatek，支持窗口可视化及shell等多种用户操作方式。 登录方式选择ssh，输入远程服务器ip地址、用户 将压缩后的程序war包放入/var/lib/tomcat/webapps文件夹下 此时连接 服务器ip:8080/项目名即可外网访问项目（服务器运行慢的情况下，刚将项目上传需要多刷新几遍才能出来） 将本地数据库上传服务器 将本地数据库导出到.sql脚本文件，再将该文件上传到服务器任意文件夹下 在命令行进入mysql命令模式 1234mysql -u用户名 -p密码 //进入数据库命令模式show databases; //查看当前有哪些数据库creat database 数据库名 //创建自己的数据库source /var/lib/tomcat/webapps/数据库名.sql //执行上传的sql脚本文件，source后拼接的是.sql文件的路径 额外需注意问题：windows数据库不区分大小写，Linux区分，需要修改解决办法是：修改MySQL的配置文件my.cnf，在[mysqld]部分添加如下配置选项lower_case_table_names = 1，然后重启MySQL服务即可","link":"/blog/2019/08/12/WEB/web项目发布外网服务器/"},{"title":"pycharm进行远程服务器代码编写与调试","text":"使用场景PyCharm是一种Python IDE，带有一整套可以帮助用户在使用Python语言开发时提高其效率的工具。 当跑一些机器学习或者深度学习代码时，由于数据量较大用本机跑可能较慢。此时就可以用一台额外配置了python和深度学习库的服务器，比如tensorflow，keras，pytorch等。 为了实现这些，总不能每次写完一部分代码后，再一次性上传服务器进行调试吧，这种方式很笨拙。真正有效的方法是实现本地编辑代码能够与服务器同步。因此就需要进行以下一些相关配置。 相关操作不难，几分钟左右即可。 步骤 配置远程服务器信息能够上传到服务器也能从服务器下载 配置Interperter解释器信息能够调用远程服务器的解释器去执行代码 配置远程服务器信息 打开pycharm，选择Tools — Deployment — Configuration,如下图所示： 然后，选择左上方的加号，选择SFTP即可，name 按照自己习惯编写。然后填写如下信息： 然后配置mapping（相当于本地项目文件地址与远程服务器项目地址的对应关系）: 配置完这些之后，就发现Development里的上传和下载可以点击了，在这里可以点击自动上传，这样以后本地代码一有改动就会自动的变更到服务器，实现真正同步： 配置Interperter解释器信息 点击files —-setting — project Interpreter: 可以看到上图中我选择的解释还是本地的，现在需要选择远程服务器的解释器，如果上一部分的服务器信息已经配置好，这里是直接有可以选择的：到此，整体配置完成，已经可以实现本地与远程服务器同步。 测试是否成功现在如果远程服务器已经安装了TensorFlow深度学习框架，那么在本地运行一个简单的TensorFlow程序，如果可以跑通说明配置成功： 1234567891011import tensorflow as tf# 创建常量 hello = tf.constant(&apos;Hello,world!&apos;)# 创建会话 sess = tf.Session()# 执行 result = sess.run(hello)# 关闭会话 sess.close()# 输出结果 print(result) 优点 可以直接在本机上编写代码 代码自动同步到远程服务器 在远程服务器上的解释器中执行代码，返回结果。和本地使用pycharm是一样的感觉。","link":"/blog/2019/09/05/python/pycharm进行远程服务器代码编写与调试/"},{"title":"(一)Python基础_面向过程语法介绍","text":"Python是一门易于学习和维护的语言，现在常被用来作为数据分析、人工智能的基础入门语言。也可以来做一些应用，比如YouTube、知乎、豆瓣网。Python3与Python2在一些地方差别还是较大的，推荐以Python3入手。 1.Python3基本数据类型python3中有六个标准的数据类型，其中包含4个容器： 其中Number、String、Tuple又属于不可变类型；List、Set、Dictionary属于可变类型。关于可变类型与不可变类型，以及Python变量在内存的存储与地址变化，该链接总结很好。[link]类型之间的相互转换： 2.Python3注释 单行注释 1# 这是单行注释 多行注释使用三个单引号将注释括起来 3.Python3运算符 4.条件控制Python中都是以代码缩进划分语句块，并且没有switch分支语句。 123456if condition_1: statement_block_1elif condition_2: statement_block_2 else: statement_block_3 statement_block_2 5.循环语句 for循环 12for &lt;variable&gt; in &lt;sequence&gt;: 执行语句 while循环 12while 判断条件： 执行语句 break、continue、pass语句可以通过break、continue、pass分别跳出循环、跳过当前循环中的剩余语句块、不做任何事。 6.导入模块模块是一个包含所有你定义的函数和变量的文件，其后缀名是.py。模块可以被别的程序引入，以使用该模块中的函数等功能。这也是使用 python 标准库的方法。 123# import module1, module2, module3 # from module import name1, name2 # from module import * # import module as something # from module import name1 as something python中的__name__属性可以用来使程序块仅在该模块自身运行时执行，如下所示： 1234if __name__ == '__main__': print('程序自身在运行') else: print('我来自另一模块') 7.输入输出 输入 键盘输入 12str = input(\"请输入：\");print (\"你输入的内容是: \", str) 文件读取 1234f = open(\"/tmp/foo.txt\", \"w\")f.write( \"I love China!\\n\" )# 关闭打开的文件f.close() 输出print输出，可以选择format格式或者按照print自带的格式。 pickle模块 8.切片关于string类型及四个容器类型的访问都可以通过切片实现。 格式切片[start:end:steps] 其中默认步长为1，start缺省的话就是0，end缺省的话就是到结束。 123456li = list(range(10))print(li[2:5]) #输出[2,5)print(li[:4]) #输出[0,4)print(li[5:]) #输出[5,9)print(li[0::1]) #输出[0,9)print(li[9:0:-1]) #步长为负数情况向下,输出[9, 8, 7, 6, 5, 4, 3, 2, 1]","link":"/blog/2019/10/03/python/一-Python基础-面向过程语法介绍/"},{"title":"(三)Python基础_文件访问与函数调用","text":"对基本的文件读取、函数调用进行整理介绍。 1. 文件访问Python中通过open()方法打开一个文件，并返回文件对象，对文件进行处理过程都需要使用这个函数。 1open(file, mode = 'r') 方法 2. 函数调用Python的函数调用，不可变对象属于值传递，可变对象属于引用传递。 默认参数调用函数时，如果没有传递参数，则会使用默认参数： 12345def func(x, y=500): return x + y print('1.默认参数:')print(func(100)) #相当于执行100 + 500print(func(100, 600)) #相当于执行100 + 600print(func(y = 300, x = 200))#参数次序不用一定对应，只需标出即可 关键字参数函数调用使用关键字参数来确定传入的参数值，使用关键字参数允许函数调用时参数的顺序与声明时不一致，因为 Python 解释器能够用参数名匹配参数值： 12345def printinfo(name,age): print(\"名字: \",name) print(\"年龄: \",age) #调用printinfo函数 printinfo(age=50, name=\"runoob\") 可变参数如果需要一个函数处理比声明时更多的参数。这些参数叫做不定长参数： 1234567891011121314151617181920 #加了*的参数会以tuple元组的形式导入，存放所有未命名的变量参数 def printinfo( arg1, *vartuple ): print (arg1) print (vartuple) # 调用printinfo 函数 # 输出: 70 (60, 50) printinfo( 70, 60, 50 ) # 加了**的会以字典的形式导入 def printinfo( arg1, **vardict ): print (arg1) print (vardict) # 调用printinfo 函数 # 输出: 1 {'a': 2, 'b': 3} printinfo(1, a=2,b=3) #若单独出现*号，后面的参数必须用关键字传入 def f(a,b,*,c): return a+b+c f(1,2,c=3) 匿名函数 lambda 只是一个表达式，函数体比 def 简单很多。 lambda的主体是一个表达式，而不是一个代码块。仅仅能在lambda表达式中封装有限的逻辑进去。 lambda 函数拥有自己的命名空间，且不能访问自己参数列表之外或全局命名空间里的参数。 虽然lambda函数看起来只能写一行，却不等同于C或C++的内联函数，后者的目的是调用小函数时不占用栈内存从而增加运行效率。 1234sum = lambda arg1,arg2:arg1 + arg2 # 调用sum函数 print(\"相加后的值为:\",sum(10,20)) print(\"相加后的值为:\",sum(20,20))","link":"/blog/2019/10/06/python/三-Python基础-文件访问与函数调用/"},{"title":"(二)Python基础_常用容器","text":"Python中常用的容器主要有四种：List、Set、Dictionary、Tuple。其中Tuple属于不可变类型，其余三类属于可变类型。 1.List列表List列表是最常见的基本数据结构，其数据项可以为不同类型，也可以为子表。 访问列表元素 123list2 = [1, 2, 3, 4, 5, 6, 7]; print(\"list1[0]: \", list1[0]) print(\"list2[1:5]: \", list2[1:5]) 修改列表元素 1list[2] = 2001 删除列表元素 1del list[2] 方法 2.Tuple元组元组与list类似，不同之处在于tuple不可以修改 3.Dictionary字典字典是另一种可变容器模型，且可存储任意类型对象。字典的每个键值(key=&gt;value)对用冒号(:)分割，每个对之间用逗号(,)分割，整个字典包括在花括号{}中 ,格式如下所示：dict = {key1:value1, key2:value2} 访问元素 12# 若用字典里没有的键访问数据，会出现错误dict = {'Name': 'Runoob', 'Age': 7, 'Class': 'First'} print (\"dict['Name']: \", dict['Name']) 修改元素 1dict['Age'] = 8 删除元素 123del dict['Name'] # 删除键 'Name' dict.clear() # 清空字典 del dict # 删除字典 常用方法 4.Set集合set集合是一个无序的不重复元素集合。可以使用大括号{}或set()函数创建集合；创建空集必须用set()。 创建格式 123parame = {value01, value02,...}或者set(value) 添加元素 12#若元素已存在则不执行任何操作s.add(x) 移出元素 1234#若元素不存在则会发生错误s.remove(x)#随机删除一个元素s.pop() 常用方法","link":"/blog/2019/10/05/python/二-Python基础-常用容器/"},{"title":"(四)Python基础_列表推导及迭代器生成器","text":"对Python中的高级特性：列表推导、迭代器生成器进行介绍。 列表推导列表生成式即List Comprehensions，是Python内置的非常简单却强大的可以用来创建list的生成式。举个例子，要生成list [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]可以用list(range(1, 11))如果要生成[1x1, 2x2, 3x3, …, 10x10]，可以通过循环，但这样太繁琐，用列表生成式可以用一行语句生成： 12345L = []for i in range(1,11): L.append(ixi)#列表方法L = [ixi for i in range(1,11)] for 循环后还可以添加if判断语句，仅筛选出偶数的平方： 1L = [ixi for i in range(1,11) if i % 2 == 0] 还可以使用双重循环生成全排列： 12# 输出['AX', 'AY', 'AZ', 'BX', 'BY', 'BZ', 'CX', 'CY', 'CZ'][m + n for m in 'ABC' for n in 'XYZ'] 迭代器生成器 生成器使用了yield函数被称为生成器generator，返回的是一个迭代器。在调用生成器运行过程中,每次遇到yield时函数会暂停并保存当前所有的运行信息，返回yield的值，并在下一次执行next()方法时从当前位置继续运行。 迭代器从集合的第一个元素开始访问，直到所有的元素被访问完结束，只能往前不会后；字符串、列表或元组对象都可用于创建迭代器，next()方法返回的是下一个迭代器对象。1234567891011121314151617#斐波那契数列使用迭代器生成器实现 import sysdef fibonacci(n,w=0): # 生成器函数 - 斐波那契 a, b, counter = 0, 1, 0 while True: if (counter &gt; n): return yield a a, b = b, a + b print('%d,%d' % (a,b)) counter += 1 f = fibonacci(10,0) # f 是一个迭代器，由生成器返回生成 while True: try: print (next(f), end=\" \") except : sys.exit()","link":"/blog/2019/10/06/python/四-Python基础-列表推导及迭代器生成器/"},{"title":"AOGNets与或文法网络结构理解","text":"AOG是CVPR2019年收录的一篇文章，因其独特思想，本文对AOG进行简单介绍。AOG是对CNN卷积网络提出的一种通用框架，传统的卷积网络各有优缺点。但基本的发展路线有三： 加深网络结构：LeNet-&gt;AlexNet-&gt;VGG 增加多尺度卷积：GoogleNet 增加跳跃连接，实现特征重用和减少梯度消失与网络退化：ResNet、DenseNet AOG相当于提出了一种通用的架构，结合以上各网络的优点，在与与或文法结合的基础上提出一种通往的卷积网络架构。 1.深度网络结构设计 神经网络结构深度网络的设计过程可以考虑成如下模式：一个输入、一个输出、中间网络结构的设计。中间网络结构的设计咱们可以进一步分成若干块。 传统填充网络块设计对于块的设计，如下图所示，咱们可以用传统的神经网络结构，如：ResNet、VGG等。 AOG网络结构本文提出一种通用的神经网络卷积结构，可以用AOG构建块代替过往的所有卷积网络结构块，如下图所示： 2.AOG构建块结构设计AOG结构块的设计如下图所示： 其中蓝色节点表示And-Node（不停的对特征进行拆分） 绿色节点表示Or-Node（不停的对And-Node拆分的特征进行组合） 标红框的节点表示Terminal-Node（对特征实现传统网络的变换，又实现类似于ResNet的跳跃连接） 又加入了紫色的横向连接（相当于在不引入额外参数的情况下增加了特征重用与流动节点深度） 与或图组成式架构：提供了更灵活的信息流，实现多种特征组合方案 整体与或语法：实现了有效的网络深度和宽度的平衡 AOG构建块的具体构建结合了广度优先算法，具体可参考原文。 3.AOG构建块的简化由于语法句子结构是存在冗余情况的，类似于我是LY与LY是我表达的是同样的意思，所以可以进一步再对AOG构建块进行剪枝。剪枝策略：凡是碰到Or-Node，其节点子集中若存在对称节点，则删除其一。比如下图，最上面的根节点的第二个节点与第四个节点对称，所以就可以剪去第四个节点。 4.实验结果作者针对本网络对ImageNet 1K与COCO测试集进行了测试，发现在使用更小参数的情况下，对图片的识别效率更好。","link":"/blog/2019/12/20/深度学习/AOGNets与或文法网络结构理解/"},{"title":"(二)简单工厂模式","text":"面向对象的好处在于通过封装、继承、多态把程序的耦合度降低。使用设计模式可以使得程序更加的灵活，容易修改且易于复用。 简单工厂模式 例子：使用任何一种面向对象的语言实现一个计算器控制台程序，要求输入两个数和运算符号，得到结果。 首先使用面向对象的封装特将操作抽成一个类： Operation运算类： 客户端代码：在只使用封装的基础上，如果此时要添加一个求根号的操作，则需在Operation类的switch中新增一个分支，但是应该避免在新增分支的时候对原有代码进行修改。因此应该把加减乘除等运算分离，修改其中一个不影响另外几个，增加其它运算算法也不影响其它代码。 使用继承抽离操作类中的具体算法： 定义四个子类继承操作类：在该基础上可以实现，如果想要新增或修改一个算法，只需新增一个子类或修改一个子类，对其它方法不会产生影响。但是问题来了，不知道该如何让计算器去实例化对象，总不能在客户端代码里写一个switch判断，看是什么操作符返回什么运算子类进行操作吧。那么如果再新加一个运算子类（比如说开根号），总不能再去修改客户端代码，在switch里添加一个分支吧。 使用多态添加一个工厂类，实例化出合适的对象： 工厂类： 客户端代码： 类图：这样再新加一个操作，只需新增一个操作子类，然后再在工厂类里添加一个分支就行了。","link":"/blog/2019/08/16/设计模式/002-简单工厂模式/"},{"title":"(一)面向对象基础","text":"本文对面向对象基础进行一个简单回顾，方便之后理解各种有趣的设计模式，几分钟就可读完。 前言本人本科专业为软件工程，曾参加过一些算法方面竞赛，因为对研究方面较感兴趣，也是为了进一步扩展知识面，研究生阶段开始投入HCI方面研究，主要是针对视觉方面的人机交互人体目标检测进行研究。 在研究生刚开始阶段曾和实验室小伙伴协同开发过两个企业级WEB方面项目，预计今年九月份都能正式上线： ONOS系统（前端）[中国通号院CRSC]采用Vue框架 设备管理系统（全栈）[北京诚星科技有限]自己搭建环境，后台采用Spring+SpringMVC+Hibernate框架，前端采用EasyUI框架 当然，在这期间也曾遇到项目重构、Bug接连不断等各方面问题，在了解自己能力尚且不足同时，越来越明白设计模式、高内聚低耦合、程序规范化、质量检测等在项目开发中真正的重要性。无论做python开发还是以后做研究，工程类的项目，该思想应该都有值得借鉴之处。因此我准备在文章分类里多添加一个设计模式的分类，在这里对主要经典常用的一些设计模式进行相关介绍总结。相信整体浏览下来你一定能受益匪浅。 知识理解 类与实例化 类：具有相同属性和功能的对象的集合；那么为什么要用类和实例而不是传统的函数调用呢，这就相当于如果居委会的电视放在你家里，而别人家里没有，于是街坊邻里都要到你家来看电视（函数调用），这样并不合适。所以正确的办法应该是将公用的方法放在居委会（类文件）。 实例：类的具体对象；实例化相当于从类工厂又生产出一台电视，任何需要的地方都可以实例化它。 构造方法与方法重载 构造方法：对类进行初始化，比如对于一个猫的实例，我们希望一出生就可以有姓名，那么就应该写一个有参数的构造方法：12Cat cat1 = new Cat(&quot;喵喵&quot;)Cat cat2 = new Cat(&quot;咪咪&quot;) 方法重载：提供了创建同名的多个方法能力，可以在不改变原方法基础上，新增功能；重载也算是提供了方法的可扩展能力，比如实例化一个猫的时候给它起不起名字都可以：12Cat cat1 = new Cat()Cat cat2 = new Cat(&quot;喵喵&quot;) 属性与修饰符属性的getter setter作用是添加控制；修饰符常用的有public private 封装继承多态 封装：每个对象都包含它能进行操作所需要的所有信息 继承：（Java中构造方法不能被继承，继承使得修改和扩展都变得容易，但也增大了两个类之间的耦合性以及破坏包装，将父类暴露给子类） 多态：不同的对象可以执行相同动作，但要通过它们自己的实现代码来执行。比如说有一对父子是表演“京剧”的，有一天父亲发高烧上不了台表演，退票的话肯定会大大影响声誉，于是就决定让儿子代父亲上台表演。这里有几点需要注意：(一)子类以父类的身份出现；(二)子类在工作时以自己的方式来实现；(三)子类以父类身份出现时，子类特有属性和方法不可以使用123arrayAnimal = new Animal[2];arrayAnimal[0] = new Cat(&quot;小花&quot;);arrayAnimal[1] = new Dog(&quot;阿毛&quot;); 重构采用合适的设计模式对原先结构进行重新构建 抽象类与接口 抽象类：实例化没有任何意义不能实例化的类例如说一只猫长得什么样可以想象 ，说动物长什么样，没办法知道。抽象方法必须被子类重写，如果一个类中包含抽象方法也一定是抽象类；实际中看具体情况看父类是否需要改成抽象类。 接口：接口是将隐式公共方法和属性组合起来，以封装特定功能的集合。以下图为例，比如变出东西是小叮当、孙悟空、猪八戒分别具备的功能，如果为了更具有普遍意义而让它们的父类也具有此种功能，显然是不合适的，比如猫并不具备变出来东西功能。所以为了将特定行为进行抽样，采用接口。主要需要注意的是接口与抽象类的区别与联系： 抽象类 接口 对类的抽取 对行为的抽取 对一些相似类对象，用继承抽象类 若行为跨越不同类的对象可使用接口 抽象类是从子类中泛化出父类 接口是不知道子类的存在，预先定义 一个类只能继承一个抽象类 一个类可继承多个接口 集合与泛型 集合：用于对数据进行存储和检索的专用类叫做集合比如说ArrayList集合就是对IList接口的实现； 泛型是具有占位符（参数类型）的类、结构、接口、方法泛型集合可以将类型参数作为它所存储对对象类型的占位符。1List &lt;Animal&gt; arraylist;s 委托与事件","link":"/blog/2019/08/13/设计模式/001-面向对象基础/"},{"title":"(三)商场促销——策略模式","text":"面向对象的编程并不是类越多越好，类的划分是为了封装，但分类的基础是抽象，具有相同属性和功能的对象的集合才是类。 需求： 假如设计一个商场收银软件，营业员根据客户所购买的商品的单价和数量，向客户收费。==考虑到不同商品有不同的促销方式：打1折、打3折、满300减100、满200减50、满100积分10点（积分达到一定程度可以领取相对应奖品）==。考虑界面如下： 简单工厂实现 按照上一节的讲法可以根据简单工厂模式建立类图如下，其中注意为了更好的抽象，应将打折操作（1折、2折等）抽象为一个类而不是两个类，其余操作也类似：如果现在需要再添加打5折、满500减200活动，按照简单工厂的思想，只需要在收费生成对象工厂添加两个switch条件，再在下拉选框里加两项就OK了；如果加入满100送10积分的活动，需要添加该收费标准子类，再到界面稍加改动， 可以看出的是，简单工厂只是解决了对象创建的问题，每次维护和扩展收费方式都要改动这个工厂，以致代码需要重新编译部署。面对算法时常变动，应该有更好的设计模式去选择。 策略模式实现 策略模式：是一种定义一系列算法的方法，从概念上看，所有这些算法完成的都是相同的工作，只是实现不同，它可以相同的方式调用所有的算法，减少各种算法与使用算法类之间的耦合。（即通过一个Context类，引入Strategy策略父类，根据传入不同对象，调用具体对应方法），**策略模式封装变化**CashContext类：客户端主要代码： 简单工厂-策略模式相结合 可以发现策略模式又回到未用简单工厂之前的老套路，直接在哭护短去判断使用哪一个算法。显然是不合适的。所以可以将简单工厂在策略模式基础上将判断过程从客户端程序移走。改造后CashContext类：改造后客户端主要代码：对比原先简单工厂的发现，简单工厂需要客户端认识两个类：CashSuper、CashFactory；而现在只需要认识一个CashContext类就可以了。使得具体的收费算法彻底地与客户端分离，连算法的父类CashSuper都不让客户端认识了，耦合度更加降低。不过，目前仍不够完美，因为CashContext里还是用到了switch判断，但是相比原先的简单工厂改动成本较小。当然还有更好的办法，比如反射技术，在后面会进行相关整理介绍。","link":"/blog/2019/08/23/设计模式/003-商场促销——策略模式/"},{"title":"(四)设计模式基本原则法则","text":"本节对4个基本原则及1个基本法则进行简要整理介绍。 拍摄UFO——单一职责原则 单一职责：就一个类而言，应该仅有一个引起它变化的原因。如果一个类承担的职责过多，就等于把这些职责耦合在一起，一个职责的变化可能会削弱或者抑制这个类完成其它职责的能力。软件设计真正要做的许多内容，就是发现职责并把那些职责相互分离。 随着移动终端的发展，智能手机已经集成了众多功能：听音乐、玩游戏、拍照、摄像等。但有时一件产品简单一些，职责单一些，或许是更好的选择，比如摄像机拍摄性能要比手机更好一些。 比如要写一个WinForm窗口程序，总不能把操作的方法都写入窗口类，更好的策略应该是将界面代码和逻辑代码相分离，即将界面职责和逻辑职责单独分开。（手机的发展有它的特点，而编程时，我们却是要在类的职责分离上多思考，做到单一职责，这样代码才是真正的易维护、易扩展、易复用、灵活多样） 考研求职两不误——开闭原则 开闭原则：对于扩展是开放的，对于更改是封闭的。面对需求的改变应保持相对稳定，从而使得系统可以在第一个版本以后不断推出新的版本。当然改变大都是不可预测的，设计人员需要先猜测出最有可能发生的变化种类，然后构造抽象来隔离那些变化。 比如在简单工厂运算器问题中，如果想添加一个乘法运算，只需要新增一个运算子类就行，与加法、减法、客户端隔离开来。这样就是面对需求，对程序的改动是通过增加新代码实现，而不是更改现有的代码。 （比如，考研过程中，考研本身是不会改变的，为了做完全准备，可以在考研期间不影响考研本身前提下，扩展的写一写简历了解招聘的资讯） 会修电脑不会修收音机——依赖倒转原则 依赖倒转原则： 高层模块不应依赖低层模块，两个都应依赖抽象 抽象不应依赖细节，细节应该依赖抽象 我们可以把电脑理解成大的软件系统，任何部件如CPU、内存、硬盘、显卡等都可以理解为程序中封装的类或程序集，不管哪一个出了问题都可以在不影响别的部件的前提下进行修改和替换。具体一点就是接口和抽象类，只要接口是稳定的，那么任何一个的更改都不用担心其它受到影响。这使得高层模块和低层模块都能很好被复用。 收音机就是典型的耦合过度，只要收音机出问题，不懂的人根本没法修，因为任何问题都可能涉及其他部件，各个部件相互依赖，难以维护。 不太理解的话在看下面一个例子：从上面的类图中可以看出，司机类和奔驰车类都属于细节，并没有实现或继承抽象，它们是对象级别的耦合。通过类图可以看出司机有一个drive()方法，用来开车，奔驰车有一个run()方法，用来表示车辆运行，并且奔驰车类依赖于司机类，用户模块表示高层模块，负责调用司机类和奔驰车类。 这样乍一看没问题，但是有一天如果司机想换一辆宝马了，但是却不能开，因为司机类里没有对宝马的依赖。下面引入依赖倒转原则重新设计一下类图：可以看出在新增低层模块（汽车）时，只修改了高层模块（Client），对已有的司机和车类都不用变动。 里氏代换原则 里氏代换原则：子类型必须能够替换掉父类型。 迪米特法则 迪米特法则：如果两个类不直接通信，那么这两个类就不应当发生直接的相互作用。如果其中一个类需要调用另一个类的某一个方法，可以通过第三者转发这个调用。 （类似于在controller层调用service层，再通过service层调用dao层访问数据库一样；而不是直接的由控制层调用数据访问层。）","link":"/blog/2019/09/03/设计模式/004-设计模式基本原则法则总结/"},{"title":"hexo个人博客搭建","text":"Hexo是一款基于Node.js的静态博客框架，依赖少易于安装使用，可以方便的生成静态网页托管在GitHub上。以下步骤操作简单，只需先自行安装好node.js和git，剩余2~3小时大概就可完成。 所需工具 node.js git 安装配置步骤 安装node.js和git之后安装hexo:npm install -g hexo-cli安装完成之后使用npm -v查看是否安装成功 创建hexo项目在本地新建一个blog的文件夹在这个文件右键进入git bash模式命令模式下执行hexo init初始化博客 生成SSH密钥打开Git Bash，使用以下命令配置gitgit config --global user.name &quot;你的github用户的名字&quot;git config --global user.email &quot;你的github账户邮箱&quot;cd ~/.sshssh-keygen -t rsa -C &quot;你的github账户邮箱&quot;连续三个回车eval &quot;$(ssh-agent -s)&quot;，添加密钥到ssh-agentssh-add ~/.ssh/id_rsa，添加生成的SSH key到ssh-agentcat ~/.ssh/id_rsa.pub复制此时显示的内容，内容应该是以ssh-rsa开头 Github新建一个仓库，并配置SSH密钥Ctrl+C退出后，在GitHub上新建一个新的仓库，仓库名随意，不过需要记录下来，我这里起名叫blog，最下面的Initialize this repository with a README要勾选上，然后保存即可。进入这个仓库后选择Settings，在左侧选项卡Options中翻到下面，GItHub Pages这项，Source选择master branch，选择save后，会在这部分的标题处写明这个仓库的url，这就是你博客的url了。还是页面的左侧的选项卡，Deploy 选择Add deploy key，添加密钥。 Title随意，我设置为了blog Key粘贴我们刚才复制的那一段。 最下面Allow write access要打勾. 选择Add Key即可。 然后在Git Bash中使用 ssh -T git@github.com测试，如果看到Hi后面是你的用户名，就说明成功了。 使用npm install安装需要的组件 使用npm install hexo-deployer-git --save安装插件 修改hexo配置文件 打开本地博客的根目录，找到_config.yml文件，在文件的开头处，第二部分，url改成自己Github仓库的地址，root改为自己/自己本地仓库名/，如下所示： 123456# URL## If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos;url: https://github.com/bjutliuyan/blogroot: /blog/permalink: :year/:month/:day/:title/permalink_defaults: 再在最下面添加如下片段，repository这项，应该去GitHub里面新建的那个叫blog的仓库里面找。进入仓库主页后，点击右侧绿色的按钮Clone or download，在新弹出的窗口右上角选择Use SSH，然后将下面的文字复制粘贴到此处。修改完配置文件后保存退出即可。 123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repository: git@github.com:Davidham3/blog.git branch: master 生成博客hexo g 发布博客hexo d 本地预览博客hexo s通过localhost:4000可以本地预览 本地预览效果","link":"/blog/2019/08/04/随笔/hexo个人博客搭建/"},{"title":"hexo主题配置+博客发布方法","text":"如果你已经成功搭建个人博客，可进一步更换不同主题及添加个性化设置，主题传送门：Themes ，我自己使用的是icarus 主题。 更换主题 克隆icarus主题到本地博客theme文件夹下： git clone https://github.com/ppoffice/hexo-theme-icarus.git 打开站点的_config.yml配置文件，修改主题为icarus: theme: hexo-theme-icarus发布文章文章编写采用Markdown标记语言书写。Markdown的语法简洁明了、学习容易，而且功能比纯文本更强，世界上最流行的博客平台WordPress和大型CMS如Joomla、Drupal都能很好的支持Markdown。Markdown基本语法大概十几分钟即可学会，更有很多专门性编辑文本软件支持Markdown编写及同步预览。在此推荐Yu Writer Pro。 建立新发布文章命令行运行：hexo n &quot;博客名&quot; 采用Markdown编辑博客中需要用到的一些图片建议使用网络图片链接，减少加载时间 发布博客 12hexo ghexo d 个性化设置 鼠标点击、音乐等 添加评论功能：推荐使用gitment 添加动画[模型预览][添加方法] 添加访客统计：推荐使用revolvermaps [操作步骤]，添加到主题theme文件夹/layout/widget下对应的页面相应组件文件内 修改鼠标样式[link]","link":"/blog/2019/08/09/随笔/hexo主题配置+发布博客方法/"},{"title":"随笔篇20190911","text":"学习研究的乐趣便在于在不断的徘徊、困惑、挑战中得出新的总结，一点点成长。 习惯总结是一个很好的习惯，如果你也喜欢写作，很荣幸能在这里与你分享交流。在这里我会慢慢记录自己的成长，也会偶尔分享些生活学习中的一些小趣事。目前的一个规划，会在后面主要针对以下方面进行总结: OpenCV TensorFlow/Pytorch meaching learning deep learning Python Linux命令 多模态目标识别算法 设计模式 Leecode数据结构刷题 论文阅读","link":"/blog/2019/09/11/随笔/随笔篇20190911/"},{"title":"CNN网络架构演进:从LeNet到DenseNet","text":"本文对卷积网络的演进进行详细介绍。 前言卷积神经网络可谓是现在深度学习领域中大红大紫的网络框架，尤其在计算机视觉领域更是一枝独秀。CNN从90年代的LeNet开始，21世纪初沉寂了10年，直到12年AlexNet开始又再焕发第二春，从ZF Net到VGG，GoogLeNet再到ResNet和最近的DenseNet，网络越来越深，架构越来越复杂，解决反向传播时梯度消失的方法也越来越巧妙。本文针对过往一些主流CNN网络架构进行总结。本文将会谈及到以下经典的卷积神经网络： LeNet AlexNet ZF VGG Network in Network GoogLeNet(Inception-v1/v2/v3/v4) ResNet DenseNet 1.开山之作：LeNet LeNet是卷积神经网络的祖师爷LeCun在1998年提出，用于解决手写数字识别的视觉任务。自那时起，CNN的最基本的架构就定下来了：卷积层、池化层、全连接层。如今各大深度学习框架中所使用的LeNet都是简化改进过的LeNet-5（-5表示具有5个层），和原始的LeNet有些许不同，比如把激活函数改为了现在很常用的ReLu。LeNet-5跟现有的conv-&gt;pool-&gt;ReLU的套路不同，它使用的方式是conv1-&gt;pool-&gt;conv2-&gt;pool2再接全连接层，但是不变的是，卷积层后紧接池化层的模式依旧不变。 2.王者归来：AlexNetAlexNet在2012年ImageNet竞赛中以超过第二名10.9个百分点的绝对优势一举夺冠，从此深度学习和卷积神经网络名声鹊起，深度学习的研究如雨后春笋般出现，AlexNet的出现可谓是卷积神经网络的王者归来。 闪光点： 更深的网络以上图AlexNet架构为例，这个网络前面5层是卷积层，后面三层是全连接层，最终softmax输出是1000类。层数比LeNet多了不少，但卷积神经网络总的流程并没有变化，只是在深度上加了不少。 数据增广AlexNet针对的是1000类的分类问题，输入图片规定是256×256的三通道彩色图片，为了增强模型的泛化能力，避免过拟合，作者使用了随机裁剪的思路对原来256×256的图像进行随机裁剪，得到尺寸为3×224×224的图像，输入到网络训练。 多GPU训练（结构上双分支） ReLu最后一层的激活函数，由LeNet的Sigmoid改为ReLu加快随机梯度下降的收敛速度。 Dropout缓解过拟合 LRN对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，增强了模型的泛化能力。 3.稳步前行：ZF-NetZFNet是2013ImageNet分类任务的冠军，其网络结构没什么改进，只是调了调参，性能较Alex提升了不少。ZF-Net只是将AlexNet第一层卷积核由11变成7，步长由4变为2，第3，4，5卷积层转变为384，384，256。这一年的ImageNet还是比较平静的一届，其冠军ZF-Net的名堂也没其他届的经典网络架构响亮。 4.越走越深：VGG-NetsVGG-Nets是由牛津大学VGG（Visual Geometry Group）提出，是2014年ImageNet竞赛定位任务的第一名和分类任务的第二名的中的基础网络。VGG可以看成是加深版本的AlexNet. 都是conv layer + FC layer。优点：VGG采用的是一种Pre-training的方式，这种方式在经典的神经网络中常见得到，就是先训练一部分小网络，然后再确保这部分网络稳定之后，再在这基础上逐渐加深。下表从左到右体现的就是这个过程，并且当网络处于D阶段的时候，效果是最优的，因此D阶段的网络也就是VGG-16了！E阶段得到的网络就是VGG-19了！VGG-16的16指的是conv+fc的总层数是16，是不包括max pool的层数！使用更小的卷积核3X3,1X1,3X3减小计算参数，1X1减小输入维度，同时都增加了非线性。 下面这个图就是VGG-16的网络结构: 从上图看出，VGG网络filter的个数（卷积后的输出通道数）从64开始，然后没接一个pooling后其成倍的增加，128、512，VGG的注意贡献是使用小尺寸的filter，及有规则的卷积-池化操作。闪光点： 卷积层使用更小的尺寸 3×3卷积核的优点： 多个3×3的卷基层比一个大尺寸filter卷基层有更多的非线性，使得判决函数更加具有判决性 多个3×3的卷积层比一个大尺寸的filter有更少的参数，假设卷基层的输入和输出的特征图大小相同为C，那么三个3×3的卷积层参数个数3×（3×3×C×C）=27CC；一个7×7的卷积层参数为49CC；所以可以把三个3×3的filter看成是一个7×7filter的分解（中间层有非线性的分解） 1*1卷积核的优点： 升降维：比如，一张500 * 500且厚度depth为100 的图片在20个filter上做11的卷积，那么结果的大小为500500*20。 加入非线性：卷积层之后经过激励层，1*1的卷积在前一层的学习表示上添加了非线性激励（ non-linear activation ），提升网络的表达能力； 5.端到端：Network in Network优点： 数据处理，随机裁剪等； 使用好的激活函数，relu替换sigmoid，防止过拟合采用dropout，lrn，卷积核变小减少计算量和参数，增加非线性，采用1X1减少维度， 加速训练分组训练； 使用全局平均池化层代替全连接 全连接的目的是什么呢？因为传统的网络我们的输出都是分类，也就是几个类别的概率甚至就是一个数–类别号，那么全连接层就是高度提纯的特征了，方便交给最后的分类器或者回归。但是全连接的参数太多了，所以现在的趋势是尽量避免全连接，近期的大部分论文FC多用全局平均池化层（GAP，Global Average Pooling）的方法代替。后者的思想就是：用 feature map 直接表示属于某个类的 confidence map，比如有10个类，就在最后输出10个 feature map，每个feature map中的值加起来求平均值，这十个数字就是对应的概率或者叫置信度。然后把得到的这些平均值直接作为属于某个类别的 confidence value，再输入softmax中分类， 更重要的是实验效果并不比用 FC 差。关于Network in Network的解释参考以下链接[link] 6.大浪推手：GoogLeNet优点： 使用Network in Network中1x1 conv及AvgPool替换fc 引入inception模块想通过一种spared layer architecture来实现较优的多维度特征表达，然后通过对这种结构进行叠加，中间不时再插入一些MaxPool层以减少参数数目（从而节省内存与计算开销）。具体来说，就是将CNN中常用的卷积（1x1，3x3，5x5）、池化操作（3x3）堆叠在一起（卷积、池化后的尺寸相同，将通道相加），一方面增加了网络的宽度，另一方面也增加了网络对尺度的适应性。输入（可以是被卷积完的长方体输出作为该层的输入）进来后，通常可以选择直接使用像素信息(1x1卷积)传递到下一层，可以选择3x3卷积，可以选择5x5卷积，还可以选择max pooling对刚被卷积后的特征图降采样。但在实际的网络设计中，究竟该如何选择需要大量的实验和经验。Inception就不用我们来选择，而是将4个选项给神经网络，让网络自己去选择最合适的解决方案。 上图结构就是Inception，结构里的卷积stride都是1，另外为了保持特征响应图大小一致，都用了零填充。最后每个卷积层后面都立刻接了个ReLU层。在输出前有个叫concatenate的层，直译的意思是“并置”，即把4组不同类型但大小相同的特征响应图一张张并排叠起来，形成新的特征响应图。 7.坚持可持续性发展：Inception-v1/v2/v3/v4这四个网络都是对Inception结构进行调整。 8.里程碑式创新：ResNet2015年何恺明推出的ResNet在ISLVRC和COCO上横扫所有选手，获得冠军。ResNet在网络结构上做了大创新，而不再是简单的堆积层数，ResNet在卷积神经网络的新思路，绝对是深度学习发展历程上里程碑式的事件。详解参见该链接[link]。当卷积网络层数增加的同时容易引发梯度消失或梯度爆炸问题，ResNet的主要思想是在网络中增加了直连通道，即Highway Network的思想。此前的网络结构是性能输入做一个非线性变换，而Highway Network则允许保留之前网络层的一定比例的输出。ResNet的思想和Highway Network的思想也非常类似，允许原始输入信息直接传到后面的层中，如下图所示。可以直接学习输入和输出的差值F(x) 优点： 层数已经非常深，超过百层 引入残差单元来解决退化问题 9.继往开来：DenseNet优点：有效地复用各layers之间的feature map计算，从而减少每层需用的训练参数。自Resnet提出以后，ResNet的变种网络层出不穷，都各有其特点，网络性能也有一定的提升。本文介绍的最后一个网络是CVPR 2017最佳论文DenseNet，论文中提出的DenseNet（Dense Convolutional Network）主要还是和ResNet及Inception网络做对比，思想上有借鉴，但却是全新的结构，网络结构并不复杂，却非常有效，在CIFAR指标上全面超越ResNet。可以说DenseNet吸收了ResNet最精华的部分，并在此上做了更加创新的工作，使得网络性能进一步提升。","link":"/blog/2019/12/11/深度学习/CNN网络架构演进-从LeNet到DenseNet/"},{"title":"(一)OpenCV介绍及安装","text":"OpenCV是一个跨平台的计算机视觉库，可运行在多个平台，由一系列 C 函数和少量 C++ 类，并同时提供了Python、Ruby、MATLAB等语言的接口，实现了图像处理和计算机视觉方面的很多通用算法。以下介绍前提是已经安装好了Python和基本的numpy、matplotlib库。 Windows下安装OpenCV 到OpenCV官网下载合适的win pack版本，然后傻瓜式操作解压安装： 安装完成之后，配置相应环境变量（依次选择计算机—&gt;属性—&gt;高级系统设置—&gt;环境变量，找到Path变量，然后把OpenCV执行文件的路径新增进去；OpenCV执行文件在解压好的OpenCV文件夹里，依次选择build—&gt;x64—&gt;vc15—&gt;bin）： 找到opencv-&gt;build-&gt;python-&gt;3.5-&gt;x64下的cv2.pyd工具包，拷贝到Python安装目录下的Lib-&gt;site-packages下到此Windows下OpenCV配置完成。 一个简单测试：新建text.py，输入如下的程序，cmd下输入python text.py，如果可以正确的显示图片，证明安装成功。 12345678910import cv2import numpy as npimg = cv2.imread(\"1.jpg\")emptyImage = np.zeros(img.shape, np.uint8)emptyImage2 = img.copy()emptyImage3=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)cv2.imshow(\"EmptyImage3\", emptyImage3)cv2.waitKey (0)cv2.destroyAllWindows() 显示图片如下： Linux下安装OpenCV首先准备一下开发环境： ubuntu 16.04 64位 python31pip install opencv-python (国内很多大学提供了pip安装镜像，安装过程中，推荐使用国内镜像安装，速度会快很多[link]) 到此linux下安装OpenCV已经完成。3. 一个简单测试： 12import cv2print(cv2.__version__) 能输出版本号则说明安装成功。","link":"/blog/2019/09/06/OpenCV/01介绍/一-OpenCV介绍及安装/"},{"title":"(三)OpenCV图像处理_图像变换基础","text":"对OpenCV图像处理中最常用的操作：颜色空间变换、图像掩码、图像变换（仿射变换、透视变换等），进行整理介绍 一、颜色空间转换 转换颜色空间OpenCV中常用到的颜色空间转换的方法主要是两种：BGR&lt;-&gt;GRAY和BGR&lt;-&gt;HSV。用到的函数是cv2.cvtColor(input_image, flag)，其中flag是转换类型。 123456import cv2img = cv2.imread('image', image02.jpg)hsv = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)cv2.imshow('image', hsv)cv2.waitKey(0)cv2.destroyAllWindows() 物体跟踪可以利用转换颜色空间来提取带有某个特定颜色的物体。在HSV颜色空间中要比在BGR中更容易表示一个特定颜色。比如以在一幅图片中提取蓝色物体为例： 12345678910111213141516import cv2import numpy as np#获取图片并转化到HSV img = cv2.imread('image02.jpg', cv2.IMREAD_COLOR)hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)#设定蓝色的阀值 lower_blue=np.array([110,50,50])upper_blue=np.array([130,255,255])#根据阀值进行掩模 mask = cv2.inRange(hsv, lower_blue, upper_blue)#对原图像和掩模进行位运算 res = cv2.bitwise_and(img, img, mask=mask)#展示 cv2.imshow('res', res)cv2.waitKey(0)cv2.destroyAllWindows() 二、图像变换图像的集合变换在计算机视觉任务的数据与处理部分很重要，能更好的清洗数据。可采用的变换模型有：刚性变换、仿射变换、透视变换、非线性变换等（参考链接[link1][link2][link3]）。目标图像变换后所得点坐标不一定为整数像素，此时应进行插值。OpenCV提供了两个变换函数： 12345#参数：输入图像，变换矩阵，输出图像大小，插值方法的组合，边界像素模式，边界填充值(默认为0)#1.仿射变换cv2.warpAffine(src, M, dsize, flags, borderMode, borderValue);#2.透视变换cv2.warpPerspective(src, M, dsize, flags, borderMode, borderValue); 扩展缩放 12#参数：图像、newSize、插值方法res = cv2.resize(img, (newWidth,newHeight), interpolation=cv2.INTER_LINEAR) 平移 123M = np.float32([[1, 0, x], [0, 1, y]])# image.shape[0]垂直尺寸、shape[1]水平尺寸、shape[2]通道数shifted = cv2.warpAffine(img, M, (image.shape[1], image.shape[0])) 旋转 12345# 通过内置的函数得到仿射矩阵M，这里的第一个参数为旋转中心，第二个为旋转角度，第三个为旋转后的缩放因子# 可以通过设置旋转中心，缩放因子，以及窗口大小来防止旋转后超出边界的问题M = cv2.getRotationMatrix2D((cols/2,rows/2), 45, 0.6)# 第三个参数是输出图像的尺寸中心dst = cv2.warpAffine(img, M, (2*cols,2*rows)) 仿射变换在仿射变换中，原图中所有的平行线在结果图像中同样平行。为了创建这个矩阵需要从原图像中找到三个点以及它们在输出图像的位置。1234pts1=np.float32([[50,50],[200,50],[50,200]])pts2=np.float32([[10,100],[200,50],[100,250]])M=cv2.getAffineTransform(pts1,pts2)dst=cv2.warpAffine(img,M,(cols,rows)) 透视变换透视变换就相当于视角的变换：1234pts1 = np.float32([[56,65],[368,52],[28,387],[389,390]])pts2 = np.float32([[0,0],[300,0],[0,300],[300,300]])M=cv2.getPerspectiveTransform(pts1,pts2)dst=cv2.warpPerspective(img,M,(300,300))","link":"/blog/2019/09/08/OpenCV/02图像/三-OpenCV图像处理-图像变换基础/"},{"title":"(二)OpenCV图像基础操作","text":"图像的许多操作与Numpy的关系比OpenCV关系更加紧密，如果熟练Numpy的话可以写出性能更好的代码。 1.读取显示保存图片 读取图片 123import cv2#第一个参数为图片的相对路径，第二个参数为读取方式（cv2.IMREAD_COLOR、cv2.IMREAD_GRAYSCALE）img = cv2.imread(&apos;image01.jpg&apos;, cv2.IMREAD_COLOR) (需注意的是当图片的路径填写有误，解释不会报错，但会得到一个None对象) 显示图片 123cv2.imshow(&apos;窗口名&apos;, img)cv2.waitkey(0) #等待键盘输入cv2.destroyAllWindows() #关闭窗口 (cv2.waitkey()是一个键盘绑定方法，可以用来确定特殊键是否点击；另外也可以通过matplotlib显示图像，OpenCV加载图片是BGR模式，matplotlib是RGB模式) 保存图片 1cv2.imwrite(&apos;messigery.jpg&apos;, img) 2.修改像素或灰度值对于彩色图片一个像素点返回的是BGR像素，灰度图像返回的是灰度值 1img[x, y] = 100 #将(x,y)位置像素/灰度值改为100 3.获取图像基本属性123print(img.shape) #获取形状：长、宽、通道数print(img.size) #获取像素数print(img.dtype) #获取数据类型 4.图像ROI有时需要在一幅图像的特殊区域工作。比如在一副图像中检测人眼的位置，应该是先找到人头部的区域，再在其中检测眼睛。这样提高程序的准确性和性能。ROI是通过Numpy的索引获得的，如下图所示，把窗户的一部分拷贝到另一部分： 12selectArea = img[280:340,330:390]img[273:333, 100:160] = selectArea 5.拆分合并图像通道有时需要对图像的BGR三个通道单独进行操作或者合并成一个图像，实现如下： 1234#将BGR格式图片转换为RGB格式的img = cv2.imread(&apos;image04.jpg&apos;, cv2.IMREAD_COLOR)b,g,r = cv2.split(img) #拆分img = cv2.merge((r,g,b)) #合并 6.图像扩边1cv2.copyMakeBorder(img, top, bottom, left, right, 边界类型参数) 7.图像上的算术运算 图像加方法 123456x = np.uint8([250])y = np.uint8([10])#opencv加法是饱和操作print cv2.add(x,y) # 250+10 = 260 =&gt; 255#numpy加法是求模操作print x+y # 250+10 = 260 % 256 = 4 图像混合图像混合其实也是图像加法的一种，不过是加上了权重： $dst = \\alpha\\cdot img1+\\beta \\cdot img2+\\gamma$ 12345678import cv2import numpy as npimg1=cv2.imread(&apos;ml.png&apos;)img2=cv2.imread(&apos;opencv_logo.jpg&apos;)dst=cv2.addWeighted(img1,0.7,img2,0.3,0)cv2.imshow(&apos;dst&apos;,dst)cv2.waitKey(0)cv2.destroyAllWindow() 如下所示： 8.检测程序效率 运行时间检测检测运行时间方法主要有两种：通过time.time()和cv2.getTickCount()： 123456import cv2import numpy as npe1 = cv2.getTickCount()# your code executione2 = cv2.getTickCount()time = (e2 - e1)/ cv2.getTickFrequency() 默认优化OpenCV中很多函数都被优化过(使用SSE2，AVX等)。也包含一些没有被优化的代码。如果系统支持的话要尽量利用这一点。在编译时优化是默认开启的。可以使用cv2.useOptimized()来查看优化是否被开启，使用函数cv2.setUseOptimized()来开启优化。 效率优化技术 尽量避免使用循环 算法中尽量使用向量操作，因为Numpy和OpenCV都对向量操作进行了优化 使用高速缓存一致性 没有必要的话就不要复制数组。使用视图来代替复制。数组复制是非常浪费资源的。","link":"/blog/2019/09/06/OpenCV/02图像/二-OpenCV图像基础操作/"},{"title":"(五)OpenCV图像处理_图像梯度、边缘检测","text":"本节对OpenCV图像处理中：图像梯度、边缘检测，进行整理介绍 图像梯度当用之前提及过的均值滤波器来降低图像噪声时，会带来图像模糊的副作用。我们当然希望看到的是清晰图像。那么，清晰图像和模糊图像之间的差别在哪里呢？从逻辑上考虑，图像模糊是因为图像中物体的轮廓不明显，轮廓边缘灰度变化不强烈，层次感不强造成的，那么反过来考虑，轮廓边缘灰度变化明显些，层次感强些是不是图像就更清晰些呢。这种灰度变化程度可以用微积分定义，梯度简单来说就是求导，OpenCV提供了三种不同的梯度滤波器，或者说高通滤波器：Sobel，Scharr，Laplacian。 Sobel算子、Scharr算子Sobel 算子是高斯平滑与微分操作的结合体，所以它的抗噪声能力很好。可以设定求导的方向（xorder 或 yorder）。还可以设定使用的卷积核的大小（ksize）。如果 ksize=-1，会使用 3x3 的Scharr 滤波器，它的的效果要比 3x3 的 Sobel 滤波器好（而且速度相同，所以在使用 3x3 滤波器时应该尽量使用 Scharr 滤波器）。 1234img = cv2.imread('image01.jpg', 0)# cv2.CV_64F为输出图像的深度；1 0/一阶导数0 1表示对x对y轴方向求sobelx = cv2.Sobel(img, cv2.CV_64F, 1,0,ksize=5)sobely = cv2.Sobel(img, cv2.CV_64F, 0,1,ksize=5) Laplacian拉普拉斯算子拉普拉斯算子可以使用二阶导数的形式定义，可假设其离散实现类似于Sobel导数。事实上，OpenCV在计算拉普拉斯算子时直接调用Sobel算子。计算公式如下：$$\\Delta src=\\frac {\\partial ^{2}src}{\\partial x^2} + \\frac {\\partial ^{2}src}{\\partial y^2}$$ 123#cv2.CV_64F是输出图像的深度，可以使用-1与原图像保持一致#图像深度是指存储每个像素所用的位数，确定了彩色图像的每个像素可能有的彩色数或者灰度图像每个像素可能有的灰度级数laplacian = cv2.laplacian(img, cv2.CV_64F) Canny边缘检测 原理 噪声去除由于边缘检测很容易受到噪声影响，所以第一步是使用高斯滤波器去燥。 计算图像梯度对平滑后的图像使用Sobel算子计算水平方向和竖直方向的一阶导数。再根据这两幅梯度图找到边界的梯度和方向，公式如下：$$Edge_Gradient(G) = \\sqrt{G^2_x+G^2_y}；Angle(\\Theta) = tan^{-1}(\\frac{G_x}{G_y})$$ 梯度的方向一般总是与边界垂直，梯度方向被归为四类：垂直、水平、两个对角线。 非极大值抑制通过上一步获得梯度的方向和大小之后，应该对整幅图像做一个扫描，去除非边界上的点（对每一个像素进行检查，看这个点的梯度是不是周围具有相同梯度方向的点中最大的）。 滞后阀值现在要确定哪些边界才是真正的边界，这时我们需要设置两个阀值：minVal和maxVal。当图像的灰度梯度高于maxVal被认为是真的边界，低于minVal的边界会被抛弃，如果介于两者之间的话，就要看这个点是否与某个被确定为真正的边界点相连，如果是就认为它也是边界点。如下图： A 高于阈值 maxVal 所以是真正的边界点，C 虽然低于 maxVal 但高于minVal 并且与 A 相连，所以也被认为是真正的边界点。而 B 就会被抛弃，因为他不仅低于 maxVal 而且不与真正的边界点相连。所以选择合适的 maxVal和 minVal 对于能否得到好的结果非常重要。在这一步一些小的噪声点也会被除去，因为我们假设边界都是一些长的线段。 OpenCV中的Canny边界检测1234567891011import cv2import numpy as npfrom matplotlib import pyplot as pltimg = cv2.imread('image01.jpg',0)#原始图像，minVal，maxValedges = cv2.Canny(img,100,200)plt.subplot(121),plt.imshow(img,cmap = 'gray')plt.title('Original Image'), plt.xticks([]), plt.yticks([])plt.subplot(122),plt.imshow(edges,cmap = 'gray')plt.title('Edge Image'), plt.xticks([]), plt.yticks([])plt.show() 如下图所示：","link":"/blog/2019/09/10/OpenCV/02图像/五-OpenCV图像处理-图像梯度、边缘检测/"},{"title":"(八)OpenCV图像处理_图像分割、交互式前景提取","text":"对OpenCV中图像分割和交互式前景的方法进行简单介绍。 一、分水岭算法图像分割 原理 二、交互式前景提取","link":"/blog/2019/11/02/OpenCV/02图像/八-OpenCV图像处理-图像分割、交互式前景提取/"},{"title":"(六)OpenCV图像处理_轮廓以及直方图","text":"轮廓可以是简单连续的点连在一起的曲线，具有相同的颜色或灰度。轮廓在形状分析和物体的检测和识别中很有用。 1 轮廓1.1 初识轮廓为了更加准确，要使用二值化图像。在寻找图像之前，要进行阀值化处理或Canny边界检测。查找轮廓的函数会修改原始图像，如果在找到轮廓之后还想使用原始图像的话，应该将原始图像存储到其它变量之中。在OpenCV中，查找轮廓就像在黑白背景中找白色物体。 12345678#在二值图像中查找轮廓，参数:输入图像、轮廓检索模式、轮廓近似方法，返回值：图像、轮廓、层析结构img = cv2.imread('test.jpg');imgray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY);ret,thresh = cv2.threshold(imgray,127,255,0)image,contours,hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE);#参数：原始图像、轮廓、轮廓的索引img = cv2.drawContour(img, contours, -1, (0,255,0), 3) 1.2 轮廓的特征1234567891011121314151617181920#图像的矩(根据图像的矩可近一步计算出图像的重心)M = cv2.moments(contours[0]);#轮廓的面积area = cv2.contourArea(contours[0]);#轮廓的周长perimeter = cv2.arcLength(contours[0]);#凸性检测flag = cv2.isContourConvex(contours[0]);#边界矩形(直角边界矩形)x,y,w,h = cv2.boundingRect(contours[0]);img = cv2.rectangle(img, (x,y), (x+w,y+h), (0,255,0), 2);#边界矩形(旋转的边界矩形)略#最小外接圆(x,y),radius = cv2.minEnclosingCircle(contours[0]);center = (int(x), int(y));radius = int(radius);img = cv2.circle(img, center, radius, (0,255,0), 2);#椭圆拟合ellipse = cv2.fitEllipse(contours[0]);img = cv2.ellipse(img, ellipse, (0,255,0), 2); 2.直方图直方图是整幅图像的灰度分布，x轴表示灰度值，y轴表示图片中具有同一个灰度值点的数目。 2.1 一维直方图 统计直方图 1234img = cv2.imread(&apos;home.jpg&apos;,0)# 别忘了中括号 [img],[0],None,[256],[0,256] ，只有 mask 没有中括号# 参数:原图像、图像通道、掩模、直方图x轴分组后每组大小、像素值的范围hist = cv2.calcHist([img],[0],None,[256],[0,256]) 画出彩色图的一维直方图 12345678910#画出彩色BGR的直方图import cv2from matplotlib import pyplot as pltimg = cv2.imread('image01.jpg')color = ('b','g','r')for i,col in enumerate(color): histr = cv2.calcHist([img],[i],None,[256],[0,256]) plt.plot(histr,color = col) plt.xlim([0,256])plt.show() 画出灰度图的一维直方图123456#画出灰度值的直方图import cv2from matplotlib import pyplot as pltimg = cv2.imread('image01.jpg', cv2.IMREAD_COLOR)plt.hist(img.ravel(),256,[0,256]);plt.show() 使用掩模要统计图像某个局部区域的直方图只需要构建一副掩模图像。将要统计的部分设置成白色，其余部分为黑色，就构成了一副掩模图像。然后把这个掩模图像传给函数就可以了。 直方图均衡化常用来作为图像增强。 123456img = cv2.imread('image01.jpg', 0);# openCV中的直方图均衡化函数，返回均衡化后图像equ = cv2.equalizeHist(img);# 横向拼接res = np.hstack((img, equ));cv2.imwrite('res.jpg', res); CLAHE直方图均衡化123456import numpy as npimport cv2img = cv2.imread('tsukuba_l.png',0)clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))cl1 = clahe.apply(img)cv2.imwrite('clahe_2.jpg',cl1) 2.2 2D直方图上面所介绍的是一维直方图，多用来处理灰度图。对于彩色图像，通常情况下我们需要考虑每个的颜色(Hue)和饱和度(Saturation)。根据这两个特征绘制2D直方图。 统计2D直方图 使用cv2.calcHist()之前，要先将图像的颜色空间从BGR转换到HSV。 123456import cv2from matplotlib import pyplot as pltimg = cv2.imread('image01.jpg', cv2.IMREAD_COLOR)hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)#参数：原始图像，通道(这里0,1分别表示颜色H和饱和度S)，掩模，histSize，ranges(H和S的range)hist = cv2.calcHist(images=hsv, channels=[0,1], mask=None, histSize=[180,256], ranges=[0,180, 0,256]) 绘制2D直方图 12plt.imshow(hist, interpolation = 'nearest')plt.show(); 2.3 直方图反向投影直方图反向投影可以用来做图像分割，或者在图像中找寻感兴趣的部分。它会输出与输入图像同样大小的图像，其中的每一个像素值代表了输入图像对应点属于目标图像的概率。更简单来说，输出图像中像素值越高的点就越可能代表要搜索的目标（在输入图像所在位置）。直方图投影经常与camshift算法等一起使用。 1234567891011121314151617181920212223242526272829import cv2import numpy as nproi = cv2.imread('tar.jpg')hsv = cv2.cvtColor(roi,cv2.COLOR_BGR2HSV)target = cv2.imread('roi.jpg')hsvt = cv2.cvtColor(target,cv2.COLOR_BGR2HSV)# calculating object histogramroihist = cv2.calcHist([hsv],[0, 1], None, [180, 256], [0, 180, 0, 256] )# normalize histogram and apply backprojection# 归一化：原始图像，结果图像，映射到结果图像中的最小值，最大值，归一化类型#cv2.NORM_MINMAX 对数组的所有值进行转化，使它们线性映射到最小值和最大值之间# 归一化之后的直方图便于显示，归一化之后就成了 0 到 255 之间的数了。cv2.normalize(roihist,roihist,0,255,cv2.NORM_MINMAX)dst = cv2.calcBackProject([hsvt],[0,1],roihist,[0,180,0,256],1)# Now convolute with circular disc# 此处卷积可以把分散的点连在一起disc = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(5,5))dst=cv2.filter2D(dst,-1,disc)# threshold and binary ANDret,thresh = cv2.threshold(dst,50,255,0)# 别忘了是三通道图像，因此这里使用 merge 变成 3 通道thresh = cv2.merge((thresh,thresh,thresh))# 按位操作res = cv2.bitwise_and(target,thresh)res = np.hstack((target,thresh,res))cv2.imwrite('res.jpg',res)# 显示图像cv2.imshow('1',res)cv2.waitKey(0) 下面是我使用的一幅图像。我使用图中蓝色矩形中的区域作为取样对象，再根据这个样本搜索图中所有的类似区域（草地）。","link":"/blog/2019/10/21/OpenCV/02图像/六-OpenCV图像处理-轮廓以及直方图/"},{"title":"(四)OpenCV图像处理_图像阀值、平滑、形态学转换","text":"本节对OpenCV图像处理中：图像阀值、图像模糊、图像的形态学转换，进行整理介绍对于图像的处理，基本步骤是：取得图像数据 —— 将图像进行平滑处理 —— 进行边缘检测，阈值分析 —— 进行形态学的操作 —— 获取某些特征点 —— 分析数据 一、图像阀值 简单阀值 自适应阀值 Qtsu‘s二值化 简单阀值针对于灰度图片的灰度值，当灰度值高于阀值时，给这个灰度值赋一个新值： 123456#参数：原始图像，阀值，新值，阀值规则ret,thresh1 = cv2.threshold(img,127,255,cv2.THRESH_TOZERO);ret,thresh2 = cv2.threshold(img,127,255,cv2.THRESH_BINARY_INV);ret,thresh3 = cv2.threshold(img,127,255,cv2.THRESH_TRUNC);ret,thresh4 = cv2.threshold(img,127,255,cv2.THRESH_TOZERO);ret,thresh5 = cv2.threshold(img,127,255,cv2.THRESH_TOZERO_INV); 阀值规则对应如下： 自适应阀值与上面简单阀值类似，不过简单阀值是整幅图像采用一个数作为阀值，这种方法并不是适应于所有情况，尤其是当一幅图像上的不同部分具有不同亮度时，可以同一幅图像的不同区域采用不同的阀值。 12#参数：原图，新增，阀值计算方法（有cv2.ADAPTIVE_THRESH_MEAN_C和cv2.ADAPTIVE_GAUSSIAN_C两类），阀值规则，邻域大小，阈值计算方法中的常数项thresh = cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2); Otsu‘s 二值化在简单阀值时是随便给了一个数来做阀值，那么怎么知道选取的好坏呢？答案就是不停的尝试。如果是一幅双峰图像，就应该根据其直方图计算出一个阀值。 1ret2,th2 = cv2.threshold(img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTS); 二、图像平滑 学习使用不同的低通滤波器对图像进行模糊 使用自定义的滤波器对图像进行卷积（2D卷积） 2D卷积与信号一样，可以对2D图像实施低通滤波(LPF)，高通滤波(HPF)等。LPF帮助去除噪声、模糊图像；HPF帮助找得到图像的边缘。 123kernel = np.ones((5,5),np.float32)/25;#when ddepth=-1, the output image will have the same depth as the sourcedst = cv2.filter2D(img,-1,kernel); 效果如下： 图像模糊使用低通滤波器可以达到图像模糊的目的，这对去除噪音很有帮助，当然现在更多的是通过生成对抗网络来对图像进行增强等。OpenCV提供了四种模糊计算： 平均使用卷积框覆盖区域所有像素的平均值来替代中心元素 12#使用归一化卷积框blur = cv2.blur(img,(5,5)); 高斯模糊一个卷积框里的值是符合高斯分布的，方框中心的值最大，其余根据距离中心元素的距离递减 12#卷积核的大小必须是一个奇数blur = cv2.GaussianBlur(img, (5,5), 标准差); 中值模糊用卷积框对应像素点的中值来替代中心像素的值，这个滤波器经常用来去除椒盐噪声。 12#参数中的5表示当前的方框尺寸median = cv2.medianBlur(img, 5); 双边滤波高斯滤波器是求中心点邻近区域像素的高斯加权平均值，这种只考虑像素之间的空间关系，而不会考虑像素值之间的关系（像素的相似度）。所以这种方法不会考虑一个像素是否位于边界，边界也会模糊掉。双边滤波同时使用空间高斯权重和灰度值相似性高斯权重。空间高斯函数确保只有邻近区域的像素对中心点有影响，灰度值相似性高斯函数确保只有与中心像素灰度值相近的才会被用来做模糊运算。所以这种方法会确保边界不会被模糊掉，因为边界处的灰度值变化比较大。 12#9表示邻域直径，两个75分别是空间高斯函数标准差、灰度值相似性高斯函数标准差 blur = cv2.bilateralFilter(img, 9, 75, 75) 三、形态学转换 腐蚀、膨胀、开运算、闭运算等形态学操作是根据图像形状进行的简单操作。一般情况下对二值化图像进行的操作。需要输入两个参数，一个是原始图像，第二个被称为结构化元素或核，它是用来决定操作的性质的。两个基本的形态学操作是腐蚀和膨胀。他们的变体构成了开运算，闭运算，梯度等。主要介绍三个方法：cv2.erode()、cv2.dilate()、cv2.morphologyEx() 1.腐蚀 1234img = cv2.imread('j.png',0);kernel = np.ones((5,5),np.uint8);#参数：原图像，核，腐蚀操作被递归执行的次数erosion = cv2.erode(img,kernel,iterations = 1); 2.膨胀 1dilation = cv2.dilate(img,kernel,iterations = 1); 3.开运算先腐蚀后膨胀(常用来去除噪声) 1opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel); 4.闭运算先膨胀后腐蚀（常被用来填充前景物体中的小洞） 1closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel); 5.形态学梯度其实就是一幅图像膨胀与腐蚀的差别，结果看上去就像前景物体的轮廓。 1gradient = cv2.morphologyEx(img,cv2.MORPH_GRADIENT,kernel); 6.礼帽与黑帽原始图像与开运算或闭运算的差。 1234#礼帽（原始图像与开运算的差）tophat = cv2.morphologyEx(img, cv2.MORPH_TOPHAT, kernel);#黑帽（原始图像与闭运算的差）tophat = cv2.morphologyEx(img, cv2.MORPH_TOPHAT, kernel);","link":"/blog/2019/09/09/OpenCV/02图像/四-OpenCV图像处理-图像阀值、平滑、形态学转换/"},{"title":"(七)OpenCV图像处理_图像变换进阶、模板匹配","text":"针对图像处理中的傅里叶变换、霍夫变换、模板检测进行总结。 1 傅里叶变换傅里叶变换经常用来分析不同滤波器的频率特性。我们可以使用2D离散傅里叶变换(DFT)分析图像的频域特性。实现DFT的一个快速算法被称为快速傅里叶变换(FFT)。对于一个正弦信号：$x(t)=Asin(2\\Pi ft)$，它的频率为$f$，可以把图像想象成沿着两个方向采集的信号，所以对图像同时进行X方向和Y方向的傅里叶变换，就会得到这幅图像的频域表示。更直观一点，对于一个正弦信号，如果幅度变化非常快则为高频信号，否则为低频信号。对应到图像，图像哪里的幅度变化非常大呢？边界点或噪声。所以说边界和噪声点是图像中的高频分量。 OpenCV的傅里叶变换OpenCV中相应的函数是cv2.dft()和cv2.idft()。123456789101112import numpy as npimport cv2from matplotlib import pyplot as pltimg = cv2.imread('messi5.jpg',0)dft = cv2.dft(np.float32(img),flags = cv2.DFT_COMPLEX_OUTPUT)dft_shift = np.fft.fftshift(dft)magnitude_spectrum = 20*np.log(cv2.magnitude(dft_shift[:,:,0],dft_shift[:,:,1]))plt.subplot(121),plt.imshow(img, cmap = 'gray')plt.title('Input Image'), plt.xticks([]), plt.yticks([])plt.subplot(122),plt.imshow(magnitude_spectrum, cmap = 'gray')plt.title('Magnitude Spectrum'), plt.xticks([]), plt.yticks([])plt.show() 2 霍夫变换霍夫变换在检测各种形状的技术中非常流行，如果要检测的形状可以用数学表达式表示，就可以用霍夫变换检测它。即使要检测的形状存在一点破坏或者扭曲也可以使用。 2.1 霍夫直线变换 概念原理一条直线可以用数学表达式$y=mx+c 或者 \\rho = xcos\\Theta+ysin\\Theta 表示$，因为第一种表达方式存在斜率为无穷大情况，所以可以用第二种表示方法，其中的两个变量分别表示：原点垂直于直线的距离，垂线与x轴夹角角度。对一条确定的直线来讲，这两个变量是固定的。 因为每一条直线都可以用$(\\rho,\\Theta)$表示，所以可以创建一个2D数组(累加器)。初始化累加器所有值都为0.行表示$\\rho$列表时$\\Theta$。这个数组的大小决定了最后结果的准确性。$\\rho$最大为180列，表示180度，$\\Theta$最大值为图片对角线的长度。 过程 先对一张图像Canny边缘检测，然后遍历每一个像素点，如果是边缘像素点，就根据该像素点的坐标$(x,y)$遍历$\\Theta$的取值，分别求出对应的$\\rho$值。这样就得到一系列$(\\rho,\\Theta)$的数值对。 如果这个数值对在累加器中也存在相应位置，就在这个位置上加1。然后遍历下一个像素点。 最后判断每个累加器中的值是否高于阀值，高于的话说明该位置(假如是$(\\rho,\\Theta)$)在图像中有一条直线，可以进行标记。 代码示例 12345678910111213141516171819202122232425262728293031import cv2import numpy as npfrom matplotlib import pyplot as pltimg = cv2.imread('image05.jpg')detect = np.copy(img)# 变换为灰度图 gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)# 进行Canny边缘检测 edge = cv2.Canny(gray, 50, 150, apertureSize=3)# 进行霍夫直线运算 lines = cv2.HoughLines(edge, 1, np.pi / 180, 180)# 对检测到的每一条线段 for line in lines: # 霍夫变换返回的是 r 和 theta 值 rho, theta = line[0] a = np.cos(theta) b = np.sin(theta) # 确定x0 和 y0 x0 = a * rho y0 = b * rho # 认为构建（x1,y1）,(x2, y2) x1 = int(x0 + 1000 * (-b)) y1 = int(y0 + 1000 * a) x2 = int(x0 - 1000 * (-b)) y2 = int(y0 - 1000 * a) # 用cv2.line()函数在image上画直线 cv2.line(detect, (x1, y1), (x2, y2), (0, 255, 255), 2)#展示检测结果 plt.subplot(121),plt.imshow(img)plt.title('Resource Img'), plt.xticks([]), plt.yticks([])plt.subplot(122), plt.imshow(detect)plt.title('Detect Result'), plt.xticks([]), plt.yticks([])plt.show() 2.2 霍夫直线变换的优化从上面的过程可以看出：仅仅是一条直线就需要两个参数，这需要大量的计算。 概念前面的方法又称为标准霍夫变换，它会计算图像中的每一个点，计算量比较大，另外它得到的是整一条线（$\\rho$和$\\Theta$），并不知道原图中直线的端点。所以提出了统计概率霍夫直线变换(Probabilistic Hough Transform)，是一种改进的霍夫变换： 代码 12345678910111213141516import cv2import numpy as npfrom matplotlib import pyplot as pltimg = cv2.imread('image05.jpg')detect = np.copy(img)gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)edges = cv2.Canny(gray,50,150,apertureSize = 3)# minLineLength：最短长度阈值，比这个长度短的线会被排除 minLineLength = 100 # maxLineGap：同一直线两点之间的最大距离 maxLineGap = 2 lines = cv2.HoughLinesP(edges,1,np.pi/180,10,minLineLength,maxLineGap)for line in lines: x1, y1, x2, y2 = line[0] cv2.line(detect,(x1,y1),(x2,y2),(0,255,255),2)plt.subplot(121),plt.imshow(img)plt.title('Resource Img'), plt.xticks([]), plt.yticks([])plt.subplot(122), plt.imshow(detect)plt.title('Detect Result'), plt.xticks([]), plt.yticks([])plt.show() 2.3 霍夫圆环变换 概念霍夫圆环变换域直线变换类似，只不过线是用（$\\rho$和$\\Theta$）表示，圆是用（$x_center$和$y_center$和$\\rho$）表示，从二维变成了三维，数据量变大了很多；所以一般使用霍夫梯度法减少计算量。 代码 1234567891011121314151617181920212223import cv2import numpy as npfrom matplotlib import pyplot as pltimg = cv2.imread('image06.jpg')detect = np.copy(img)gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)edges = cv2.Canny(gray, 50, 150, apertureSize=3)plt.imshow(edges)plt.show()circles = cv2.HoughCircles(edges, cv2.HOUGH_GRADIENT, 1, 50, param2=60)circles = np.int0(np.around(circles))for i in circles[0, :]: # 画圆 cv2.circle(detect, (i[0], i[1]), i[2], (0, 255, 0), 5) # 画圆心 cv2.circle(detect, (i[0], i[1]), 2, (255, 0, 0), 3)plt.subplot(121), plt.imshow(img)plt.title('Resource Img'), plt.xticks([]), plt.yticks([])plt.subplot(122), plt.imshow(detect)plt.title('Detect Result'), plt.xticks([]), plt.yticks([])plt.show() 3 模板匹配模板匹配是用来在一副大图中搜寻查找模板图像位置的方法。和2D卷积一样，它也是用模板图像在输入图像上滑动，并在每一个位置对模板图像和与其对应的输入图像的子区域进行比较。返回的结果是一个灰度图像，每一个像素值表示了此区域与模板的匹配程度。如果输入的图像大小是(W*H)，模板的大小是(w*h)，输出的结果就是(W-w+1, H-h+1)。当得到这幅图之后就可以使用函数cv2.minMaxLoc()来找到其中的最小值和最大值的位置了。 opencv中的模板匹配 123456789101112131415161718192021222324252627import cv2from matplotlib import pyplot as pltimg = cv2.imread('image04.jpg', cv2.IMREAD_COLOR)img2 = img.copy()template = cv2.imread('image04_face.jpg', cv2.IMREAD_COLOR)w, h, c = template.shape[::1]# All the 6 methods for comparison in a list # methods = ['cv2.TM_CCOEFF', 'cv2.TM_CCOEFF_NORMED', 'cv2.TM_CCORR','cv2.TM_CCORR_NORMED','cv2.TM_SQDIFF', 'cv2.TM_SQDIFF_NORMED'] res = cv2.matchTemplate(image=img, templ=template, method=cv2.TM_CCOEFF_NORMED);# 使用不同的比较方法，对结果的解释不同,If the method is TM_SQDIFF or TM_SQDIFF_NORMED, take min_loc, ELSE take max_loc top_left = max_loc;min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(res);bottom_right = (top_left[0] + w, top_left[1] + h)cv2.rectangle(img,top_left, bottom_right, (0,0,255), 2)#bgr转rgb b,g,r = cv2.split(img)img = cv2.merge((r,g,b))b,g,r = cv2.split(img2)img2 = cv2.merge((r,g,b))#显示结果 plt.subplot(121),plt.imshow(img2)plt.title('Resource Img'), plt.xticks([]), plt.yticks([])plt.subplot(122), plt.imshow(img)plt.title('Matching Result'), plt.xticks([]), plt.yticks([])plt.show() 以下图为例，是我在一张黑寡妇英雄人物图片中检测出其面部，并标记出的示例： 多对象的模板匹配目标对象在图像中出现了多次的情况。 12345678910111213141516import cv2import numpy as npfrom matplotlib import pyplot as pltimg_rgb = cv2.imread('mario.png')img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_BGR2GRAY)template = cv2.imread('mario_coin.png',0)w, h = template.shape[::-1]res = cv2.matchTemplate(img_gray,template,cv2.TM_CCOEFF_NORMED)threshold = 0.8#umpy.where(condition[, x, y])#Return elements, either from x or y, depending on condition.#If only condition is given, return condition.nonzero().loc = np.where( res &gt;= threshold)for pt in zip(*loc[::-1]): cv2.rectangle(img_rgb, pt, (pt[0] + w, pt[1] + h), (0,0,255), 2) cv2.imwrite('res.png',img_rgb) 以下是我以马里奥游戏为例，截取了一帧图像，到其中标记所有金币的结果展示：","link":"/blog/2019/10/27/OpenCV/02图像/七-OpenCV图像处理_图像变换进阶、模板匹配/"},{"title":"(六)TensorFlow_Python调用API","text":"针对于常用的TensorFlow方法进行一个简要的汇总介绍。 1. 变量转换 构造 tf.string_to_number(string_tensor, out_type=None, name=None) tf.to_double(x, name='ToDouble') tf.to_float(x, name='ToFloat') tf.to_bfloat16(x, name='ToBFloat16') tf.to_int32(x, name='ToInt32') tf.to_int64(x, name='ToInt64') tf.cast(x, dtype, name=None) 形状与重塑 tf.shape(input, name=None) tf.size(input, name=None) tf.rank(input, name=None) tf.reshape(tensor, shape, name=None) tf.squeeze(input, squeeze_dims=None, name=None) tf.expand_dims(input, dim, name=None) 切片与插入 tf.slice(input_, begin, size, name=None) tf.split(split_dim, num_split, value, name='split') tf.tile(input, multiples, name=None) tf.pad(input, paddings, name=None) tf.concat(concat_dim, values, name='concat') tf.pack(values, name='pack') tf.unpack(value, num=None, name='unpack') tf.reverse_sequence(input, seq_lengths, seq_dim, name=None) tf.reverse(tensor, dims, name=None) tf.transpose(a, perm=None, name='transpose') tf.gather(params, indices, name=None) tf.dynamic_partition(data, partitions, num_partitions, name=None) tf.dynamic_stitch(indices, data, name=None) 2.构建图 核心图数据结构 class tf.Graph class tf.Operation class tf.Tensor 张量类型 class tf.DType tf.as_dtype(type_value) 公用函数 tf.device(dev) tf.name_scope(name) tf.control_dependencies(control_inputs) tf.convert_to_tensor(value, dtype=None, name=None) tf.get_default_graph() tf.import_graph_def(graph_def, input_map=None, return_elements=None, name=None, op_dict=None) Graph 集合 tf.add_to_collection(name, value) tf.get_collection(key, scope=None) class tf.GraphKeys 定义新的操作 class tf.RegisterGradient tf.NoGradient(op_type) class tf.RegisterShape class tf.TensorShape class tf.Dimension tf.op_scope(values, name, default_name) tf.get_seed(op_seed) 3. 运行图 Session 管理 class tf.Session class tf.InteractiveSession tf.get_default_session() 错误分类 class tf.OpError class tf.errors.CancelledError class tf.errors.UnknownError class tf.errors.InvalidArgumentError class tf.errors.DeadlineExceededError class tf.errors.NotFoundError class tf.errors.AlreadyExistsError class tf.errors.PermissionDeniedError class tf.errors.UnauthenticatedError class tf.errors.ResourceExhaustedError class tf.errors.FailedPreconditionError class tf.errors.AbortedError class tf.errors.OutOfRangeError class tf.errors.UnimplementedError class tf.errors.InternalError class tf.errors.UnavailableError class tf.errors.DataLossError 4. 变量，序列以及随机数 常量张量 tf.zeros(shape, dtype=tf.float32, name=None) tf.zeros_like(tensor, dtype=None, name=None) tf.ones(shape, dtype=tf.float32, name=None) tf.ones_like(tensor, dtype=None, name=None) tf.fill(dims, value, name=None) tf.constant(value, dtype=None, shape=None, name='Const') 序列 tf.linspace(start, stop, num, name=None) tf.range(start, limit, delta=1, name='range') 随机张量 例子: tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) tf.random_uniform(shape, minval=0.0, maxval=1.0, dtype=tf.float32, seed=None, name=None) tf.random_shuffle(value, seed=None, name=None) tf.set_random_seed(seed) 5.控制流 控制流 tf.identity(input, name=None) tf.tuple(tensors, name=None, control_inputs=None) tf.group(*inputs, **kwargs) tf.no_op(name=None) tf.count_up_to(ref, limit, name=None) 逻辑运算符 tf.logical_and(x, y, name=None) tf.logical_not(x, name=None) tf.logical_or(x, y, name=None) tf.logical_xor(x, y, name='LogicalXor') 比较操作 tf.equal(x, y, name=None) tf.not_equal(x, y, name=None) tf.less(x, y, name=None) tf.less_equal(x, y, name=None) tf.greater(x, y, name=None) tf.greater_equal(x, y, name=None) tf.select(condition, t, e, name=None) tf.where(input, name=None) 调试操作 tf.is_finite(x, name=None) tf.is_inf(x, name=None) tf.is_nan(x, name=None) tf.verify_tensor_all_finite(t, msg, name=None) tf.check_numerics(tensor, message, name=None) tf.add_check_numerics_ops() tf.Assert(condition, data, summarize=None, name=None) tf.Print(input_, data, message=None, first_n=None, summarize=None, name=None) 6.图像 编码解码 tf.image.decode_jpeg(contents, channels=None, ratio=None, fancy_upscaling=None, try_recover_truncated=None, acceptable_fraction=None, name=None) tf.image.encode_jpeg(image, format=None, quality=None, progressive=None, optimize_size=None, chroma_downsampling=None, density_unit=None, x_density=None, y_density=None, xmp_metadata=None, name=None) tf.image.decode_png(contents, channels=None, name=None) tf.image.encode_png(image, compression=None, name=None) 调整 tf.image.resize_images(images, new_height, new_width, method=0) tf.image.resize_area(images, size, name=None) tf.image.resize_bicubic(images, size, name=None) tf.image.resize_bilinear(images, size, name=None) tf.image.resize_nearest_neighbor(images, size, name=None) 剪裁 tf.image.resize_image_with_crop_or_pad(image, target_height, target_width) tf.image.pad_to_bounding_box(image, offset_height, offset_width, target_height, target_width) tf.image.crop_to_bounding_box(image, offset_height, offset_width, target_height, target_width) tf.image.random_crop(image, size, seed=None, name=None) tf.image.extract_glimpse(input, size, offsets, centered=None, normalized=None, uniform_noise=None, name=None) 翻转和置换 tf.image.flip_up_down(image) tf.image.random_flip_up_down(image, seed=None) tf.image.flip_left_right(image) tf.image.random_flip_left_right(image, seed=None) tf.image.transpose_image(image) 图像调整 tf.image.adjust_brightness(image, delta, min_value=None, max_value=None) tf.image.random_brightness(image, max_delta, seed=None) tf.image.adjust_contrast(images, contrast_factor, min_value=None, max_value=None) tf.image.random_contrast(image, lower, upper, seed=None) tf.image.per_image_whitening(image) 7.输入与读取 占位符 tf.placeholder(dtype, shape=None, name=None) 读取 class tf.ReaderBase class tf.TextLineReader class tf.WholeFileReader class tf.IdentityReader class tf.TFRecordReader class tf.FixedLengthRecordReader 转换 tf.decode_csv(records, record_defaults, field_delim=None, name=None) tf.decode_raw(bytes, out_type, little_endian=None, name=None) 示例序列缓冲区 tf.parse_example(serialized, names=None, sparse_keys=None, sparse_types=None, dense_keys=None, dense_types=None, dense_defaults=None, dense_shapes=None, name='ParseExample') tf.parse_single_example(serialized, names=None, sparse_keys=None, sparse_types=None, dense_keys=None, dense_types=None, dense_defaults=None, dense_shapes=None, name='ParseSingleExample') 队列 class tf.QueueBase class tf.FIFOQueue class tf.RandomShuffleQueue 文件系统处理 tf.matching_files(pattern, name=None) tf.read_file(filename, name=None) 输入管道 输入管道的开始 tf.train.match_filenames_once(pattern, name=None) tf.train.limit_epochs(tensor, num_epochs=None, name=None) tf.train.range_input_producer(limit, num_epochs=None, shuffle=True, seed=None, capacity=32, name=None) tf.train.slice_input_producer(tensor_list, num_epochs=None, shuffle=True, seed=None, capacity=32, name=None) tf.train.string_input_producer(string_tensor, num_epochs=None, shuffle=True, seed=None, capacity=32, name=None) Batching at the end of an input pipeline tf.train.batch(tensor_list, batch_size, num_threads=1, capacity=32, enqueue_many=False, shapes=None, name=None) tf.train.batch_join(tensor_list_list, batch_size, capacity=32, enqueue_many=False, shapes=None, name=None) tf.train.shuffle_batch(tensor_list, batch_size, capacity, min_after_dequeue, num_threads=1, seed=None, enqueue_many=False, shapes=None, name=None) tf.train.shuffle_batch_join(tensor_list_list, batch_size, capacity, min_after_dequeue, seed=None, enqueue_many=False, shapes=None, name=None) 8.数学相关 算术运算符 tf.add(x, y, name=None) tf.sub(x, y, name=None) tf.mul(x, y, name=None) tf.div(x, y, name=None) tf.mod(x, y, name=None) 基本属性函数 tf.add_n(inputs, name=None) tf.abs(x, name=None) tf.neg(x, name=None) tf.sign(x, name=None) tf.inv(x, name=None) tf.square(x, name=None) tf.round(x, name=None) tf.sqrt(x, name=None) tf.rsqrt(x, name=None) tf.pow(x, y, name=None) tf.exp(x, name=None) tf.log(x, name=None) tf.ceil(x, name=None) tf.floor(x, name=None) tf.maximum(x, y, name=None) tf.minimum(x, y, name=None) tf.cos(x, name=None) tf.sin(x, name=None) 矩阵数学函数 tf.diag(diagonal, name=None) tf.transpose(a, perm=None, name='transpose') tf.matmul(a, b, transpose_a=False, transpose_b=False, a_is_sparse=False, b_is_sparse=False, name=None) tf.batch_matmul(x, y, adj_x=None, adj_y=None, name=None) tf.matrix_determinant(input, name=None) tf.batch_matrix_determinant(input, name=None) tf.matrix_inverse(input, name=None) tf.batch_matrix_inverse(input, name=None) tf.cholesky(input, name=None) tf.batch_cholesky(input, name=None) 复数函数 tf.complex(real, imag, name=None) tf.complex_abs(x, name=None) tf.conj(in_, name=None) tf.imag(in_, name=None) tf.real(in_, name=None) 减值 tf.reduce_sum(input_tensor, reduction_indices=None, keep_dims=False, name=None) tf.reduce_prod(input_tensor, reduction_indices=None, keep_dims=False, name=None) tf.reduce_min(input_tensor, reduction_indices=None, keep_dims=False, name=None) tf.reduce_max(input_tensor, reduction_indices=None, keep_dims=False, name=None) tf.reduce_mean(input_tensor, reduction_indices=None, keep_dims=False, name=None) tf.reduce_all(input_tensor, reduction_indices=None, keep_dims=False, name=None) tf.reduce_any(input_tensor, reduction_indices=None, keep_dims=False, name=None) tf.accumulate_n(inputs, shape=None, tensor_dtype=None, name=None) 分割 tf.segment_sum(data, segment_ids, name=None) tf.segment_prod(data, segment_ids, name=None) tf.segment_min(data, segment_ids, name=None) tf.segment_max(data, segment_ids, name=None) tf.segment_mean(data, segment_ids, name=None) tf.unsorted_segment_sum(data, segment_ids, num_segments, name=None) tf.sparse_segment_sum(data, indices, segment_ids, name=None) tf.sparse_segment_mean(data, indices, segment_ids, name=None) 序列中的比较和索引 tf.argmin(input, dimension, name=None) tf.argmax(input, dimension, name=None) tf.listdiff(x, y, name=None) tf.where(input, name=None) tf.unique(x, name=None) tf.edit_distance(hypothesis, truth, normalize=True, name='edit_distance') tf.invert_permutation(x, name=None) 9.神经网络相关 激活函数 tf.nn.relu(features, name=None) tf.nn.relu6(features, name=None) tf.nn.softplus(features, name=None) tf.nn.dropout(x, keep_prob, noise_shape=None, seed=None, name=None) tf.nn.bias_add(value, bias, name=None) tf.sigmoid(x, name=None) tf.tanh(x, name=None) 卷积 tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, name=None) tf.nn.depthwise_conv2d(input, filter, strides, padding, name=None) tf.nn.separable_conv2d(input, depthwise_filter, pointwise_filter, strides, padding, name=None) 池化 tf.nn.avg_pool(value, ksize, strides, padding, name=None) tf.nn.max_pool(value, ksize, strides, padding, name=None) tf.nn.max_pool_with_argmax(input, ksize, strides, padding, Targmax=None, name=None) 归一化 tf.nn.l2_normalize(x, dim, epsilon=1e-12, name=None) tf.nn.local_response_normalization(input, depth_radius=None, bias=None, alpha=None, beta=None, name=None) tf.nn.moments(x, axes, name=None) 损失函数 tf.nn.l2_loss(t, name=None) 分类 tf.nn.sigmoid_cross_entropy_with_logits(logits, targets, name=None) tf.nn.softmax(logits, name=None) tf.nn.softmax_cross_entropy_with_logits(logits, labels, name=None) 嵌入张量查找值 tf.nn.embedding_lookup(params, ids, name=None) 评估 tf.nn.top_k(input, k, name=None) tf.nn.in_top_k(predictions, targets, k, name=None) 候选采样 采样损失函数 tf.nn.nce_loss(weights, biases, inputs, labels, num_sampled, num_classes, num_true=1, sampled_values=None, remove_accidental_hits=False, name='nce_loss') tf.nn.sampled_softmax_loss(weights, biases, inputs, labels, num_sampled, num_classes, num_true=1, sampled_values=None, remove_accidental_hits=True, name='sampled_softmax_loss') 候选取样器 tf.nn.uniform_candidate_sampler(true_classes, num_true, num_sampled, unique, range_max, seed=None, name=None) tf.nn.log_uniform_candidate_sampler(true_classes, num_true, num_sampled, unique, range_max, seed=None, name=None) tf.nn.learned_unigram_candidate_sampler(true_classes, num_true, num_sampled, unique, range_max, seed=None, name=None) tf.nn.fixed_unigram_candidate_sampler(true_classes, num_true, num_sampled, unique, range_max, vocab_file='', distortion=0.0, num_reserved_ids=0, num_shards=1, shard=0, unigrams=[], seed=None, name=None) 其它候选抽样工具 tf.nn.compute_accidental_hits(true_classes, sampled_candidates, num_true, seed=None, name=None) 10.稀疏张量 稀疏张量表示 class tf.SparseTensor class tf.SparseTensorValue 稀疏到稠密变换 tf.sparse_to_dense(sparse_indices, output_shape, sparse_values, default_value, name=None) tf.sparse_tensor_to_dense(sp_input, default_value, name=None) tf.sparse_to_indicator(sp_input, vocab_size, name=None) 操作 tf.sparse_concat(concat_dim, sp_inputs, name=None) tf.sparse_reorder(sp_input, name=None) tf.sparse_retain(sp_input, to_retain) tf.sparse_fill_empty_rows(sp_input, default_value, name=None) 11.变量 Variables class tf.Variable Variable helper functions tf.all_variables() tf.trainable_variables() tf.initialize_all_variables() tf.initialize_variables(var_list, name='init') tf.assert_variables_initialized(var_list=None) Saving and Restoring Variables class tf.train.Saver tf.train.latest_checkpoint(checkpoint_dir, latest_filename=None) tf.train.get_checkpoint_state(checkpoint_dir, latest_filename=None) tf.train.update_checkpoint_state(save_dir, model_checkpoint_path, all_model_checkpoint_paths=None, latest_filename=None) Sharing Variables tf.get_variable(name, shape=None, dtype=tf.float32, initializer=None, trainable=True, collections=None) tf.get_variable_scope() tf.variable_scope(name_or_scope, reuse=None, initializer=None) tf.constant_initializer(value=0.0) tf.random_normal_initializer(mean=0.0, stddev=1.0, seed=None) tf.truncated_normal_initializer(mean=0.0, stddev=1.0, seed=None) tf.random_uniform_initializer(minval=0.0, maxval=1.0, seed=None) tf.uniform_unit_scaling_initializer(factor=1.0, seed=None) tf.zeros_initializer(shape, dtype=tf.float32) Sparse Variable Updates tf.scatter_update(ref, indices, updates, use_locking=None, name=None) tf.scatter_add(ref, indices, updates, use_locking=None, name=None) tf.scatter_sub(ref, indices, updates, use_locking=None, name=None) tf.sparse_mask(a, mask_indices, name=None) class tf.IndexedSlices 12.训练 Optimizers class tf.train.Optimizer Usage Processing gradients before applying them. Gating Gradients Slots class tf.train.GradientDescentOptimizer class tf.train.AdagradOptimizer class tf.train.MomentumOptimizer class tf.train.AdamOptimizer class tf.train.FtrlOptimizer class tf.train.RMSPropOptimizer Gradient Computation tf.gradients(ys, xs, grad_ys=None, name='gradients', colocate_gradients_with_ops=False, gate_gradients=False, aggregation_method=None) class tf.AggregationMethod tf.stop_gradient(input, name=None) Gradient Clipping tf.clip_by_value(t, clip_value_min, clip_value_max, name=None) tf.clip_by_norm(t, clip_norm, name=None) tf.clip_by_average_norm(t, clip_norm, name=None) tf.clip_by_global_norm(t_list, clip_norm, use_norm=None, name=None) tf.global_norm(t_list, name=None) Decaying the learning rate tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate, staircase=False, name=None) Moving Averages class tf.train.ExponentialMovingAverage Coordinator and QueueRunner class tf.train.Coordinator class tf.train.QueueRunner tf.train.add_queue_runner(qr, collection='queue_runners') tf.train.start_queue_runners(sess=None, coord=None, daemon=True, start=True, collection='queue_runners') Summary Operations tf.scalar_summary(tags, values, collections=None, name=None) tf.image_summary(tag, tensor, max_images=None, collections=None, name=None) tf.histogram_summary(tag, values, collections=None, name=None) tf.nn.zero_fraction(value, name=None) tf.merge_summary(inputs, collections=None, name=None) tf.merge_all_summaries(key='summaries') Adding Summaries to Event Files class tf.train.SummaryWriter tf.train.summary_iterator(path) Training utilities tf.train.global_step(sess, global_step_tensor) tf.train.write_graph(graph_def, logdir, name, as_text=True)","link":"/blog/2019/10/08/TensorFlow/六-TensorFlow_Python调用API/"}],"tags":[{"name":"C++_STL","slug":"C-STL","link":"/blog/tags/C-STL/"},{"name":"Linux","slug":"Linux","link":"/blog/tags/Linux/"},{"name":"MySql","slug":"MySql","link":"/blog/tags/MySql/"},{"name":"MNIST","slug":"MNIST","link":"/blog/tags/MNIST/"},{"name":"TensorBoard","slug":"TensorBoard","link":"/blog/tags/TensorBoard/"},{"name":"TensorFlow","slug":"TensorFlow","link":"/blog/tags/TensorFlow/"},{"name":"LinearRegression","slug":"LinearRegression","link":"/blog/tags/LinearRegression/"},{"name":"CIFAR-10","slug":"CIFAR-10","link":"/blog/tags/CIFAR-10/"},{"name":"Java","slug":"Java","link":"/blog/tags/Java/"},{"name":"WEB","slug":"WEB","link":"/blog/tags/WEB/"},{"name":"Pycharm","slug":"Pycharm","link":"/blog/tags/Pycharm/"},{"name":"Python","slug":"Python","link":"/blog/tags/Python/"},{"name":"AOGNets","slug":"AOGNets","link":"/blog/tags/AOGNets/"},{"name":"设计模式","slug":"设计模式","link":"/blog/tags/设计模式/"},{"name":"hexo搭建","slug":"hexo搭建","link":"/blog/tags/hexo搭建/"},{"name":"Markdown","slug":"Markdown","link":"/blog/tags/Markdown/"},{"name":"CNN","slug":"CNN","link":"/blog/tags/CNN/"},{"name":"OpenCV","slug":"OpenCV","link":"/blog/tags/OpenCV/"}],"categories":[{"name":"C/C++","slug":"C-C","link":"/blog/categories/C-C/"},{"name":"Linux","slug":"Linux","link":"/blog/categories/Linux/"},{"name":"WEB","slug":"WEB","link":"/blog/categories/WEB/"},{"name":"TensorFlow","slug":"TensorFlow","link":"/blog/categories/TensorFlow/"},{"name":"MySql","slug":"WEB/MySql","link":"/blog/categories/WEB/MySql/"},{"name":"JAVA","slug":"WEB/JAVA","link":"/blog/categories/WEB/JAVA/"},{"name":"Python","slug":"Python","link":"/blog/categories/Python/"},{"name":"深度学习","slug":"深度学习","link":"/blog/categories/深度学习/"},{"name":"设计模式","slug":"设计模式","link":"/blog/categories/设计模式/"},{"name":"随笔","slug":"随笔","link":"/blog/categories/随笔/"},{"name":"OpenCV","slug":"OpenCV","link":"/blog/categories/OpenCV/"}]}